<div id="S001" class="NLM_sec NLM_sec-type_intro NLM_sec_level_1">
 <h2 id="_i5" class="section-heading-2">1. Introduction</h2>
 <p>Language acquisition and understanding are still linked to a wide range of challenges in robotics, despite significant progress and achievements in artificial intelligence (AI) through recent advances in machine learning during this decade. These advances have especially been in deep learning, fields related to language (such as machine translation), speech recognition, image recognition, image captioning, distributed semantic representation learning, and parsing. Although several studies demonstrated that an agent without pre-existing linguistic knowledge can learn to understand linguistic commands solely with deep learning and a labeled dataset, problems still abound. The actual linguistic phenomena that future service robots interacting with human users naturally in their daily lives will have to deal with are complex, diverse, dynamic, and highly uncertain. The goal of this study is to expose the scientific and engineering challenges clearly at the intersection of robotics and natural language processing (NLP) that must be solved to develop future service robots.</p>
 <p>There are many reasons why <i>language</i> is still challenging in AI and robotics, as we describe later in this paper. Further exploration of this field should be conducted based on a clear understanding of the difficulties of the challenges. We would argue that the ad-hoc implementation of language skills in robots and an at-random exploration of the language system cannot lead to an appropriate understanding of the language used by humans, or to the creation of robots that can deal with language in the same way as humans. This is because language itself is dynamic, systemic, cognitive, and a social phenomenon.</p>
 <p>The goal of this survey paper is to clarify the frontier, i.e. the challenges, in <i>language and robotics</i> by surveying achievements of the related research communities and linguistic phenomena that have been mostly ignored in robotics to date. Accordingly, we first conduct a background review of this research field and share our ideas on language and robotics, including thoughts on the importance of this research field. This is not only for building future intelligent robots, but also for understanding human intelligence and language phenomena. The definition of language may depend on the relevant academic field. In this paper, we use the term language to represent a natural language that we humans use in our daily lives, e.g. English or Japanese. Although it is sometimes argued that other animals have language-like systems, e.g. the syntactic structure of the songs of certain birds, this study focuses on human language. A language is a type of social sign system, such as gestures or traffic signs. Nonlinguistic social signs can be used as signs for social communication; e.g. nonverbal communication has been studied in social robotics for decades [<span class="ref-lnk lazy-ref"><a data-rid="CIT0001 CIT0002 CIT0003 CIT0004 CIT0005" data-reflink="_i80 _i82" href="#"><span class="off-screen">Citation</span>1–5</a></span>]. However, we have excluded such signs from the scope of this survey and focused on natural language, which has syntax, semantics, and lexicons.</p>
 <p>The remainder of this paper is organized as follows. Section&nbsp;<a href="#S002">2</a> introduces the background of the research field of language and robotics, and points to seven distinct topics in the field. These seven topics are described in Sections&nbsp;<a href="#S003">3</a>–<a href="#S009">9</a>, respectively, i.e. logic probabilistic programming and learning distributed semantics, unsupervised syntactic parsing with grounding phrases and predicates, category and concept formation, metaphorical expressions, affordance and action learning, pragmatics and social language, and resources and criteria for evaluation, i.e. dataset, simulator, and competition. Finally, Section&nbsp;<a href="#S010">10</a> concludes this paper with a discussion and future perspectives.</p>
</div>
<div id="S002" class="NLM_sec NLM_sec_level_1">
 <h2 id="_i6" class="section-heading-2">2. Background</h2>
 <p>This section introduces the background for the research field of language and robotics and highlights seven distinct challenges in the field.</p>
 <div id="S002-S2001" class="NLM_sec NLM_sec_level_2">
  <h3 class="section-heading-3" id="_i7">2.1. Language in robotics</h3>
  <p>Robots interacting with human users using speech signals try to behave correctly based on speech commands from the users in a real-world environment, which is full of uncertainty. To deal with the problems related to language understanding in a real-world environment, robot audition and vision have been studied to improve the robustness of automatic speech recognition and scene understanding for decades&nbsp;[<span class="ref-lnk lazy-ref"><a data-rid="CIT0006 CIT0007 CIT0008 CIT0009 CIT0010 CIT0011 CIT0012 CIT0013" data-reflink="_i80 _i82" href="#"><span class="off-screen">Citation</span>6–13</a></span>]. However, challenges still abound in this field. Several studies have attempted to enable robots to understand the meaning of sentences. However, many applications still use manual rules, which only enable robots to understand a very small proportion of language. They also often have difficulty in dealing with uncertainty in language, e.g. treating speech recognition errors and the ambiguity of expressions.</p>
  <p>The number of studies on language learning by robots has been increasing recently. One representative approach is based on deep learning. Several studies successfully enabled artificial agents to understand simple sentences using neural networks by composing linguistics, visuals, and other information based on supervised learning or reinforcement learning (RL) [<span class="ref-lnk lazy-ref"><a data-rid="CIT0014 CIT0015 CIT0016" data-reflink="_i80 _i82" href="#"><span class="off-screen">Citation</span>14–16</a></span>]. Another representative approach has been taken in the field of symbol emergence in robotics&nbsp;[<span class="ref-lnk lazy-ref"><a data-rid="CIT0017" data-reflink="_i80 _i82" href="#"><span class="off-screen">Citation</span>17</a></span>,<span class="ref-lnk lazy-ref"><a data-rid="CIT0018" data-reflink="_i80 _i82" href="#"><span class="off-screen">Citation</span>18</a></span>]. Many types of unsupervised learning systems that can discover words, categories, and motions from sensor–motor information based on hierarchical Bayesian models have been developed. We must specifically highlight achievements in object category and concept formation by robots with cognitive scientists and linguists, as many parts of category formation discussed in cognitive science, typically nominal category formation, have already been modeled and reproduced in robotics (see Section&nbsp;<a href="#S005">5</a>). Of course, there are many types of categories and concepts. However, that which has been learned is still a very limited set of linguistic phenomena. For example, current robots cannot learn logic and reasoning, use metaphorical expressions, or infer a variety of concepts from abstract and functional words using a bottom-up approach.</p>
  <p>To create robots that can communicate naturally with people in a real-world environment, e.g. offices and houses, we need to develop methods that enable them to process uttered sentences robustly in a real-world environment, despite inevitable uncertainties. We need to recognize that language is not a material object or a set of objective signals, but rather a dynamic symbol system in which the meaning of signs depends on the context and is understood subjectively. Further, language needs to be understood from a social viewpoint. Considering mutual (or shared) belief systems is indispensable to develop a theory of language understanding in communication&nbsp;[<span class="ref-lnk lazy-ref"><a data-rid="CIT0019" data-reflink="_i80 _i82" href="#"><span class="off-screen">Citation</span>19</a></span>].</p>
  <p>Symbol grounding is a long-standing problem in AI and robotics&nbsp;[<span class="ref-lnk lazy-ref"><a data-rid="CIT0020" data-reflink="_i80 _i82" href="#"><span class="off-screen">Citation</span>20</a></span>], even though several researchers pointed out that the definition of the problem itself is somewhat misleading&nbsp;[<span class="ref-lnk lazy-ref"><a data-rid="CIT0017" data-reflink="_i80 _i82" href="#"><span class="off-screen">Citation</span>17</a></span>,<span class="ref-lnk lazy-ref"><a data-rid="CIT0018" data-reflink="_i80 _i82" href="#"><span class="off-screen">Citation</span>18</a></span>,<span class="ref-lnk lazy-ref"><a data-rid="CIT0021" data-reflink="_i80 _i82" href="#"><span class="off-screen">Citation</span>21</a></span>,<span class="ref-lnk lazy-ref"><a data-rid="CIT0022" data-reflink="_i80 _i82" href="#"><span class="off-screen">Citation</span>22</a></span>]. In all cases, it is important for a robot to ground symbols in their sensor–motor information, i.e. the perceptual world.Many studies related to the symbol grounding problem attempted to ground ‘words'. However, the meaning of a sentence is not the sum of the meanings of its words. Robots must ground the meaning of phrases and sentences. For this purpose, unsupervised learning of syntactic parsing with grounding phrases and predicates is also important. The meanings of words and phrases are not only determined by what they represent, but also by their relationship with other words. This is called distributional semantics. Therefore, learning distributional semantics is also crucial.</p>
  <p>As we addressed in this section, creating a robot capable of ultimately communicating naturally with humans in a real-world environment is a great scientific challenge. To make progress in this field, we need to define appropriate tasks, and have an appropriate dataset. Considering the reality of the communication and collaboration to which the language contributes, these cannot be described as one-shot input–output information processing steps; instead, they involve continuous interaction. Therefore, a static dataset prepared by recording a series of interactions is insufficient for the study. However, using actual robots is not cost-efficient. Therefore, having a suitable simulator is important to accelerate such studies. This point is also addressed in the following section.</p>
 </div>
 <div id="S002-S2002" class="NLM_sec NLM_sec_level_2">
  <h3 class="section-heading-3" id="_i8">2.2. Robotics for language</h3>
  <p>The language that we, human beings, use is far more complex and structured than the sign systems that other animals use. The linguistic capability enables us to collaborate with other agents, i.e. leads to multi-agent coordination, and to form social norms and structures. Such magnificent capabilities gave humans the highest position among the species on earth. This language capability can be regarded as the fruit of our evolution and adaptation to the real-world environment. We can argue that language is meaningful in terms of adaptation and competition in a real-world environment. Furthermore, we can argue that the main functions of language are to enable people to communicate with each other and to represent objects, actions, events, emotions, intentions, and phenomena in the real-world, including the physical and social environment, to survive and prosper.</p>
  <p>Therefore, it is crucially important to explore how language can help agents to adapt to the environment and collaborate with others to grasp its central function. However, researchers in the field of classical linguistics have not been able to address these problems, as stereotypical linguists have been focusing on written sentences alone. In studies of linguistics and NLP, real-world sensor–motor information has rarely been involved. However, currently, we can use robots with sensor–motor systems to experience multimodal information and perform real-world tasks. Employing robots will expand the horizon of linguistics. Cognitive linguistics introduced the notion of embodiment, leading to significant progress&nbsp;[<span class="ref-lnk lazy-ref"><a data-rid="CIT0023 CIT0024 CIT0025" data-reflink="_i80 _i82" href="#"><span class="off-screen">Citation</span>23–25</a></span>]. However, so far, actual embodied systems have not been employed as ‘materials and methods’ in the study of language, despite embodiment and real-world uncertainty being crucial for intelligence. We believe that including robots in the study of language will broaden the horizon of cognitive linguistics as well.</p>
  <p>The field of linguistics has mainly been focused on the language spoken or written by adults, i.e. learned and used language. NLP has mainly dealt with correct written sentences. However, humans can not only use but also learn a language. In the developmental process, input data received during the learning process is not written text data, but rather speech signals with multimodal sensor–motor information including haptic, visual, auditory, and motor information. Language learning needs to be performed in a real-world environment that is full of uncertainty. It is unrealistic to assume that an infant can acquire complete linguistic knowledge from speech signals alone. To model language acquisition, we need to deal with real-world information, including at least sensor–motor information. This means that at the minimum, a robot would be required for further study of the language.</p>
  <p>In understanding a language, real-world multimodal information is essential as well. When one says, ‘please take it', to another person while pointing at an object, visual information is used to reduce uncertainty in the interpretation of ‘it’ (exophora). This indicates that many sentences require additional information, i.e. context, for interpretation. In practice, most context cannot be captured by considering written sentences alone. In various situations, the existence of real-world, i.e. embodied, information is essential to language understanding.</p>
  <p>NLP has mainly been handling written text. NLP has led to many achievements, of course, as many linguistic phenomena and problems could be addressed using written text alone. However, many open questions, which cannot be solved solely with written text, remain in NLP. The NLP research community is also expanding in the direction of studies involving multimodal sensor information, e.g. multimodal machine translation, image and video captioning, and visual question answering&nbsp;[<span class="ref-lnk lazy-ref"><a data-rid="CIT0026 CIT0027 CIT0028 CIT0029" data-reflink="_i80 _i82" href="#"><span class="off-screen">Citation</span>26–29</a></span>]. A new academic challenge regarding NLP using sensor–motor information in real-world environments, namely, <i>language and robotics</i>, is now being introduced.</p>
  <p>Having an embodied system is crucial to the modeling of many linguistic phenomena. For example, the meaning of metaphors is based on cross-modal inference. Metaphors cannot be understood without the notion of embodiment. Robotics will be able to provide an appropriate model for metaphors by leveraging its multimodal servomotor information.</p>
  <p>Affordance learning is also crucial for language understanding. The concept of a tool is linked to actions. For example, ‘chair’ cannot be defined without referring to the ‘sit down’ or ‘be seated’ action. Affordance learning has been studied in cognitive robotics in the past decade&nbsp;[<span class="ref-lnk lazy-ref"><a data-rid="CIT0030" data-reflink="_i80 _i82" href="#"><span class="off-screen">Citation</span>30</a></span>]. This demonstrates that there is scope for robotics to contribute to language understanding.</p>
 </div>
 <div id="S002-S2003" class="NLM_sec NLM_sec_level_2">
  <h3 class="section-heading-3" id="_i9">2.3. Scope of survey on frontiers of language and robotics</h3>
  <p>We have pointed out several challenges and important topics in this section. Many challenges still abound at the frontier of language and robotics. To create a robot that can learn and understand language through natural interactions with human participants, as does a human infant, we must tackle the following problems.</p>
  <ul class="NLM_list NLM_list-list_type-bullet">
   <li><p class="inline">Logic probabilistic programming and learning distributed semantics (Section&nbsp;<a href="#S003">3</a>)</p></li>
   <li><p class="inline">Unsupervised syntactic parsing with grounding phrases and predicates (Section&nbsp;<a href="#S004">4</a>)</p></li>
   <li><p class="inline">Category and concept formation (Section&nbsp;<a href="#S005">5</a>)</p></li>
   <li><p class="inline">Metaphor and embodiment (Section&nbsp;<a href="#S006">6</a>)</p></li>
   <li><p class="inline">Affordance and action learning (Section&nbsp;<a href="#S007">7</a>)</p></li>
   <li><p class="inline">Pragmatics and social language (Section&nbsp;<a href="#S008">8</a>)</p></li>
   <li><p class="inline">Dataset, simulator, and competition (Section&nbsp;<a href="#S009">9</a>)</p></li>
  </ul>
  <p>Although these are the topics that must be studied, they do not cover all the problems in language and robotics. We have intentionally chosen the topics that are related to language understanding and acquisition and have been relatively lacking in the context of studies of robotics and NLP to highlight its frontiers. We have excluded the topics that have been intensively studied. For example, we have excluded robot audition and vision studies. In addition, we have excluded nonverbal communication in social robotics as well, because we focus on natural language in this survey. Figure&nbsp;<a href="#F0001">1</a> summarizes the issues in an illustration. We describe their current status and more detailed challenges in the following sections.</p>
  <div class="figure figureViewer" id="F0001">
   <div class="hidden figureViewerArticleInfo">
    <span class="figViewerTitle">Survey on frontiers of language and robotics</span>
    <div class="articleAuthors articleInfoSection">
     <div class="authorsHeading">
      All authors
     </div>
     <div class="authors">
      <a class="entryAuthor" href="/action/doSearch?Contrib=Taniguchi%2C+T"><span class="hlFld-ContribAuthor"></span></a><a href="/author/Taniguchi%2C+T"><span class="NLM_given-names">T.</span> Taniguchi</a>, <a class="entryAuthor" href="/action/doSearch?Contrib=Mochihashi%2C+D"><span class="hlFld-ContribAuthor"></span></a><a href="/author/Mochihashi%2C+D"><span class="NLM_given-names">D.</span> Mochihashi</a>, <a class="entryAuthor" href="/action/doSearch?Contrib=Nagai%2C+T"><span class="hlFld-ContribAuthor"></span></a><a href="/author/Nagai%2C+T"><span class="NLM_given-names">T.</span> Nagai</a>, <a class="entryAuthor" href="/action/doSearch?Contrib=Uchida%2C+S"><span class="hlFld-ContribAuthor"></span></a><a href="/author/Uchida%2C+S"><span class="NLM_given-names">S.</span> Uchida</a>, <a class="entryAuthor" href="/action/doSearch?Contrib=Inoue%2C+N"><span class="hlFld-ContribAuthor"></span></a><a href="/author/Inoue%2C+N"><span class="NLM_given-names">N.</span> Inoue</a>, <a class="entryAuthor" href="/action/doSearch?Contrib=Kobayashi%2C+I"><span class="hlFld-ContribAuthor"></span></a><a href="/author/Kobayashi%2C+I"><span class="NLM_given-names">I.</span> Kobayashi</a>, <a class="entryAuthor" href="/action/doSearch?Contrib=Nakamura%2C+T"><span class="hlFld-ContribAuthor"></span></a><a href="/author/Nakamura%2C+T"><span class="NLM_given-names">T.</span> Nakamura</a>, <a class="entryAuthor" href="/action/doSearch?Contrib=Hagiwara%2C+Y"><span class="hlFld-ContribAuthor"></span></a><a href="/author/Hagiwara%2C+Y"><span class="NLM_given-names">Y.</span> Hagiwara</a>, <a class="entryAuthor" href="/action/doSearch?Contrib=Iwahashi%2C+N"><span class="hlFld-ContribAuthor"></span></a><a href="/author/Iwahashi%2C+N"><span class="NLM_given-names">N.</span> Iwahashi</a> &amp; <a class="entryAuthor" href="/action/doSearch?Contrib=Inamura%2C+T"><span class="hlFld-ContribAuthor"></span></a><a href="/author/Inamura%2C+T"><span class="NLM_given-names">T.</span> Inamura</a>
     </div>
    </div>
    <div class="articleLowerInfo articleInfoSection">
     <div class="articleLowerInfoSection articleInfoDOI">
      <a href="https://doi.org/10.1080/01691864.2019.1632223">https://doi.org/10.1080/01691864.2019.1632223</a>
     </div>
     <div class="articleInfoPublicationDate articleLowerInfoSection border">
      <h6>Published online:</h6>24 June 2019
     </div>
    </div>
   </div>
   <div class="figureThumbnailContainer">
    <div class="figureInfo">
     <div class="short-legend">
      <p class="captionText"><span class="captionLabel">Figure 1. </span> Overview of challenges and relationships between topics described in this survey.</p>
     </div>
    </div><a href="#" class="thumbnail" aria-label="thumbnail image"><img id="F0001image" src="/na101/home/literatum/publisher/tandf/journals/content/tadr20/2019/tadr20.v033.i15-16/01691864.2019.1632223/20210828/images/medium/tadr_a_1632223_f0001_ob.jpg" loading="lazy" height="349" width="500"></a>
    <div class="figureDownloadOptions">
     <a href="#" class="downloadBtn btn btn-sm" role="button">Display full size</a>
    </div>
   </div>
  </div>
  <div class="hidden rs_skip" id="fig-description-F0001">
   <p class="captionText"><span class="captionLabel">Figure 1. </span> Overview of challenges and relationships between topics described in this survey.</p>
  </div>
  <div class="hidden rs_skip" id="figureFootNote-F0001">
   <div class="figureFootNote-F0001"></div>
  </div>
  <p></p>
 </div>
</div>
<div id="S003" class="NLM_sec NLM_sec_level_1">
 <h2 id="_i11" class="section-heading-2">3. Probabilistic logic programming and distributed representations</h2>
 <p>As exemplified by Sherlock Holmes, humans can predict what happens next, or what happened before, by combining observed facts with their knowledge of the world. The ability to draw a conclusion with reasoning, henceforth, <i>inference</i>, is one of the crucial components of future intelligent robots&nbsp;[<span class="ref-lnk lazy-ref"><a data-rid="CIT0031" data-reflink="_i80 _i82" href="#"><span class="off-screen">Citation</span>31</a></span>].</p>
 <p>For example, in Figure&nbsp;<a href="#F0001">1</a>, the robot brings a bottle, responding to the user utterance ‘<i>I'm thirsty</i> ’. In order to achieve this, the robot needs to infer that it should bring the bottle, based on the user utterance and problem-solving knowledge, e.g. ‘a bottle of water can solve thirstiness', and ‘there is a bottle of water in the kitchen'. Even if the robot can recognize the sentence spoken and has knowledge, it may not be capable of doing this if it does not have the ability to derive a conclusion by combining these types of information.</p>
 <p>Conventional studies of logic programming (LP) have been focused on this issue. However, developing methods that enable a robot to learn logic programs and distributed representations on a large-scale knowledge base via sensor–motor experiences in a real-world environment is still a challenge.</p>
 <p>This section discusses the latest advances in reasoning in two areas, LP and distributed representations in NLP, and points to future challenges.</p>
 <div id="S003-S2001" class="NLM_sec NLM_sec_level_2">
  <h3 class="section-heading-3" id="_i12">3.1. Probabilistic logic programming</h3>
  <div id="S003-S2001-S3001" class="NLM_sec NLM_sec_level_3">
   <h4 class="section-heading-4" id="_i13">3.1.1. Logic programming</h4>
   <p>LP is essentially a declarative programming paradigm based on formal logic. LP has its roots in automated theorem proving, where the purpose is to test whether or not a logic program Γ can prove a logical formula, or query, <i>ψ</i>, i.e. <span class="NLM_disp-formula-image inline-formula rs_preserve">
     <noscript>
      <img src="/na101/home/literatum/publisher/tandf/journals/content/tadr20/2019/tadr20.v033.i15-16/01691864.2019.1632223/20210828/images/tadr_a_1632223_ilm0001.gif" alt="">
     </noscript><img src="//:0" alt="" class="mml-formula" data-formula-source="{&quot;type&quot; : &quot;image&quot;, &quot;src&quot; : &quot;/na101/home/literatum/publisher/tandf/journals/content/tadr20/2019/tadr20.v033.i15-16/01691864.2019.1632223/20210828/images/tadr_a_1632223_ilm0001.gif&quot;}"><span class="mml-formula"></span></span><span class="NLM_disp-formula inline-formula rs_preserve"><img src="//:0" alt="" data-formula-source="{&quot;type&quot; : &quot;mathjax&quot;}">
     <math>
      <mi mathvariant="normal">
       Γ
      </mi><mo>
       ⊨
      </mo><mi>
       ψ
      </mi>
     </math></span> or <span class="NLM_disp-formula-image inline-formula rs_preserve">
     <noscript>
      <img src="/na101/home/literatum/publisher/tandf/journals/content/tadr20/2019/tadr20.v033.i15-16/01691864.2019.1632223/20210828/images/tadr_a_1632223_ilm0002.gif" alt="">
     </noscript><img src="//:0" alt="" class="mml-formula" data-formula-source="{&quot;type&quot; : &quot;image&quot;, &quot;src&quot; : &quot;/na101/home/literatum/publisher/tandf/journals/content/tadr20/2019/tadr20.v033.i15-16/01691864.2019.1632223/20210828/images/tadr_a_1632223_ilm0002.gif&quot;}"><span class="mml-formula"></span></span><span class="NLM_disp-formula inline-formula rs_preserve"><img src="//:0" alt="" data-formula-source="{&quot;type&quot; : &quot;mathjax&quot;}">
     <math>
      <mi mathvariant="normal">
       Γ
      </mi><mo>
       ⊭
      </mo><mi>
       ψ
      </mi>
     </math></span>. For computational efficiency, the language used in logic programs is typically restricted to a subset of first-order logic (e.g. Horn clauses&nbsp;[<span class="ref-lnk lazy-ref"><a data-rid="CIT0032" data-reflink="_i80 _i82" href="#"><span class="off-screen">Citation</span>32</a></span>]). From a reasoning perspective, LP serves as an inference engine. Hobbs et&nbsp;al.&nbsp;[<span class="ref-lnk lazy-ref"><a data-rid="CIT0033" data-reflink="_i80 _i82" href="#"><span class="off-screen">Citation</span>33</a></span>] propose an <i>Interpretation as Abduction</i> framework, where natural language understanding is formulated as abductive theorem proving. In this context, a logic program Γ is a commonsense knowledge base (e.g. <span class="NLM_disp-formula-image inline-formula rs_preserve">
     <noscript>
      <img src="/na101/home/literatum/publisher/tandf/journals/content/tadr20/2019/tadr20.v033.i15-16/01691864.2019.1632223/20210828/images/tadr_a_1632223_ilm0003.gif" alt="">
     </noscript><img src="//:0" alt="" class="mml-formula" data-formula-source="{&quot;type&quot; : &quot;image&quot;, &quot;src&quot; : &quot;/na101/home/literatum/publisher/tandf/journals/content/tadr20/2019/tadr20.v033.i15-16/01691864.2019.1632223/20210828/images/tadr_a_1632223_ilm0003.gif&quot;}"><span class="mml-formula"></span></span><span class="NLM_disp-formula inline-formula rs_preserve"><img src="//:0" alt="" data-formula-source="{&quot;type&quot; : &quot;mathjax&quot;}">
     <math>
      <mo fence="false">
       {
      </mo><mrow>
       <mi mathvariant="normal">
        b
       </mi>
       <mi mathvariant="normal">
        i
       </mi>
       <mi mathvariant="normal">
        r
       </mi>
       <mi mathvariant="normal">
        d
       </mi>
      </mrow><mo>
       (
      </mo><mrow>
       <mi mathvariant="normal">
        T
       </mi>
       <mi mathvariant="normal">
        w
       </mi>
       <mi mathvariant="normal">
        e
       </mi>
       <mi mathvariant="normal">
        e
       </mi>
       <mi mathvariant="normal">
        t
       </mi>
       <mi mathvariant="normal">
        y
       </mi>
      </mrow><mo>
       )
      </mo><mo>
       ,
      </mo><mtext>
       &nbsp;
      </mtext><mi mathvariant="normal">
       ∀
      </mi><mtext>
       &nbsp;
      </mtext><mi>
       x
      </mi><mrow>
       <mi mathvariant="normal">
        b
       </mi>
       <mi mathvariant="normal">
        i
       </mi>
       <mi mathvariant="normal">
        r
       </mi>
       <mi mathvariant="normal">
        d
       </mi>
      </mrow><mo>
       (
      </mo><mi>
       x
      </mi><mo>
       )
      </mo><mo>
       →
      </mo><mrow>
       <mi mathvariant="normal">
        f
       </mi>
       <mi mathvariant="normal">
        l
       </mi>
       <mi mathvariant="normal">
        y
       </mi>
      </mrow><mo>
       (
      </mo><mi>
       x
      </mi><mo>
       )
      </mo><mo fence="false">
       }
      </mo>
     </math></span>) and a query <i>ψ</i> will be a question that is of interest (e.g. <span class="NLM_disp-formula-image inline-formula rs_preserve">
     <noscript>
      <img src="/na101/home/literatum/publisher/tandf/journals/content/tadr20/2019/tadr20.v033.i15-16/01691864.2019.1632223/20210828/images/tadr_a_1632223_ilm0004.gif" alt="">
     </noscript><img src="//:0" alt="" class="mml-formula" data-formula-source="{&quot;type&quot; : &quot;image&quot;, &quot;src&quot; : &quot;/na101/home/literatum/publisher/tandf/journals/content/tadr20/2019/tadr20.v033.i15-16/01691864.2019.1632223/20210828/images/tadr_a_1632223_ilm0004.gif&quot;}"><span class="mml-formula"></span></span><span class="NLM_disp-formula inline-formula rs_preserve"><img src="//:0" alt="" data-formula-source="{&quot;type&quot; : &quot;mathjax&quot;}">
     <math>
      <mrow>
       <mi mathvariant="normal">
        f
       </mi>
       <mi mathvariant="normal">
        l
       </mi>
       <mi mathvariant="normal">
        y
       </mi>
      </mrow><mo>
       (
      </mo><mrow>
       <mi mathvariant="normal">
        T
       </mi>
       <mi mathvariant="normal">
        w
       </mi>
       <mi mathvariant="normal">
        e
       </mi>
       <mi mathvariant="normal">
        e
       </mi>
       <mi mathvariant="normal">
        t
       </mi>
       <mi mathvariant="normal">
        y
       </mi>
      </mrow><mo>
       )
      </mo>
     </math></span>).</p>
   <p>Typically, reasoning involves a wide variety of inferences (e.g. coreference resolution, word sense disambiguation, etc.), where the inferences are dependent on each other. Thus, it is difficult to define the types of inferences that should precede in advance algorithmically. Although there is a wide variety of approaches to implementing reasoning, ranging from conventional, feature-based machine learning classifiers such as logistic regression to modern deep learning techniques, LP provides an elegant solution to this issue, by virtue of its declarative nature. In declarative programming, all that is required to solve a problem is to provide general knowledge around problem-solving; procedures on how to actually solve this specific problem are not needed. For example, for Sudoku, we would write rules such as ‘for all <span class="NLM_disp-formula-image inline-formula rs_preserve">
     <noscript>
      <img src="/na101/home/literatum/publisher/tandf/journals/content/tadr20/2019/tadr20.v033.i15-16/01691864.2019.1632223/20210828/images/tadr_a_1632223_ilm0005.gif" alt="">
     </noscript><img src="//:0" alt="" class="mml-formula" data-formula-source="{&quot;type&quot; : &quot;image&quot;, &quot;src&quot; : &quot;/na101/home/literatum/publisher/tandf/journals/content/tadr20/2019/tadr20.v033.i15-16/01691864.2019.1632223/20210828/images/tadr_a_1632223_ilm0005.gif&quot;}"><span class="mml-formula"></span></span><span class="NLM_disp-formula inline-formula rs_preserve"><img src="//:0" alt="" data-formula-source="{&quot;type&quot; : &quot;mathjax&quot;}">
     <math>
      <mi>
       i
      </mi><mo>
       ,
      </mo><mi>
       j
      </mi><mo>
       :
      </mo><mn>
       1
      </mn><mo>
       ≤
      </mo><msub>
       <mi>
        c
       </mi>
       <mrow>
        <mi>
         i
        </mi>
        <mo>
         ,
        </mo>
        <mi>
         j
        </mi>
       </mrow>
      </msub><mo>
       ≤
      </mo><mn>
       9
      </mn>
     </math></span>', where <span class="NLM_disp-formula-image inline-formula rs_preserve">
     <noscript>
      <img src="/na101/home/literatum/publisher/tandf/journals/content/tadr20/2019/tadr20.v033.i15-16/01691864.2019.1632223/20210828/images/tadr_a_1632223_ilm0006.gif" alt="">
     </noscript><img src="//:0" alt="" class="mml-formula" data-formula-source="{&quot;type&quot; : &quot;image&quot;, &quot;src&quot; : &quot;/na101/home/literatum/publisher/tandf/journals/content/tadr20/2019/tadr20.v033.i15-16/01691864.2019.1632223/20210828/images/tadr_a_1632223_ilm0006.gif&quot;}"><span class="mml-formula"></span></span><span class="NLM_disp-formula inline-formula rs_preserve"><img src="//:0" alt="" data-formula-source="{&quot;type&quot; : &quot;mathjax&quot;}">
     <math>
      <msub>
       <mi>
        c
       </mi>
       <mrow>
        <mi>
         i
        </mi>
        <mo>
         ,
        </mo>
        <mi>
         j
        </mi>
       </mrow>
      </msub>
     </math></span> represents a cell value at the <i>i</i>th row and <i>j</i>th column, and then simply run the inference engine. In the literature, a number of LP formalisms have been proposed such as Prolog, Answer Set Programming&nbsp;[<span class="ref-lnk lazy-ref"><a data-rid="CIT0034" data-reflink="_i80 _i82" href="#"><span class="off-screen">Citation</span>34</a></span>], where the expressivity of logic programs and their logical semantics, etc., are different from each other.</p>
  </div>
  <div id="S003-S2001-S3002" class="NLM_sec NLM_sec_level_3">
   <h4 class="section-heading-4" id="_i14">3.1.2. Reasoning with uncertainty</h4>
   <p>Conventional LP cannot represent uncertainty in knowledge. For example, a rule such as ‘If it rains, John will be absent from school with the probability of 60%’ is not representable in LP. To solve this problem, <i>probabilistic logic programming</i> (PLP), a probabilistic extension of LP, has been developed. A wide variety of formalisms has been proposed, such as PRISM&nbsp;[<span class="ref-lnk lazy-ref"><a data-rid="CIT0035" data-reflink="_i80 _i82" href="#"><span class="off-screen">Citation</span>35</a></span>], stochastic logic programming&nbsp;[<span class="ref-lnk lazy-ref"><a data-rid="CIT0036" data-reflink="_i80 _i82" href="#"><span class="off-screen">Citation</span>36</a></span>], and ProbLog&nbsp;[<span class="ref-lnk lazy-ref"><a data-rid="CIT0037" data-reflink="_i80 _i82" href="#"><span class="off-screen">Citation</span>37</a></span>]. In the field of statistical relational learning, certain logic-based formalisms have also been proposed such as Markov logic&nbsp;[<span class="ref-lnk lazy-ref"><a data-rid="CIT0038" data-reflink="_i80 _i82" href="#"><span class="off-screen">Citation</span>38</a></span>] and probabilistic soft logic&nbsp;[<span class="ref-lnk lazy-ref"><a data-rid="CIT0039" data-reflink="_i80 _i82" href="#"><span class="off-screen">Citation</span>39</a></span>]. Most of the popular PLP formalisms to date are based on distribution semantics (DS), proposed by Sato&nbsp;[<span class="ref-lnk lazy-ref"><a data-rid="CIT0035" data-reflink="_i80 _i82" href="#"><span class="off-screen">Citation</span>35</a></span>].</p>
   <p>We briefly provide an overview of DS. DS introduces a probabilistic semantics for logic programs. The idea is as follows. We assume that a logic program Π consists of facts <i>F</i> and rules <i>R</i> (i.e. <span class="NLM_disp-formula-image inline-formula rs_preserve">
     <noscript>
      <img src="/na101/home/literatum/publisher/tandf/journals/content/tadr20/2019/tadr20.v033.i15-16/01691864.2019.1632223/20210828/images/tadr_a_1632223_ilm0007.gif" alt="">
     </noscript><img src="//:0" alt="" class="mml-formula" data-formula-source="{&quot;type&quot; : &quot;image&quot;, &quot;src&quot; : &quot;/na101/home/literatum/publisher/tandf/journals/content/tadr20/2019/tadr20.v033.i15-16/01691864.2019.1632223/20210828/images/tadr_a_1632223_ilm0007.gif&quot;}"><span class="mml-formula"></span></span><span class="NLM_disp-formula inline-formula rs_preserve"><img src="//:0" alt="" data-formula-source="{&quot;type&quot; : &quot;mathjax&quot;}">
     <math>
      <mi mathvariant="normal">
       Π
      </mi><mo>
       =
      </mo><mi>
       F
      </mi><mo>
       ∪
      </mo><mi>
       R
      </mi>
     </math></span>). Consider a probabilistically sampled subset of facts <span class="NLM_disp-formula-image inline-formula rs_preserve">
     <noscript>
      <img src="/na101/home/literatum/publisher/tandf/journals/content/tadr20/2019/tadr20.v033.i15-16/01691864.2019.1632223/20210828/images/tadr_a_1632223_ilm0008.gif" alt="">
     </noscript><img src="//:0" alt="" class="mml-formula" data-formula-source="{&quot;type&quot; : &quot;image&quot;, &quot;src&quot; : &quot;/na101/home/literatum/publisher/tandf/journals/content/tadr20/2019/tadr20.v033.i15-16/01691864.2019.1632223/20210828/images/tadr_a_1632223_ilm0008.gif&quot;}"><span class="mml-formula"></span></span><span class="NLM_disp-formula inline-formula rs_preserve"><img src="//:0" alt="" data-formula-source="{&quot;type&quot; : &quot;mathjax&quot;}">
     <math>
      <msup>
       <mi>
        F
       </mi>
       <mo>
        ′
       </mo>
      </msup><mo>
       ⊆
      </mo><mi>
       F
      </mi>
     </math></span>, according to some probability distribution, and a logic program <span class="NLM_disp-formula-image inline-formula rs_preserve">
     <noscript>
      <img src="/na101/home/literatum/publisher/tandf/journals/content/tadr20/2019/tadr20.v033.i15-16/01691864.2019.1632223/20210828/images/tadr_a_1632223_ilm0009.gif" alt="">
     </noscript><img src="//:0" alt="" class="mml-formula" data-formula-source="{&quot;type&quot; : &quot;image&quot;, &quot;src&quot; : &quot;/na101/home/literatum/publisher/tandf/journals/content/tadr20/2019/tadr20.v033.i15-16/01691864.2019.1632223/20210828/images/tadr_a_1632223_ilm0009.gif&quot;}"><span class="mml-formula"></span></span><span class="NLM_disp-formula inline-formula rs_preserve"><img src="//:0" alt="" data-formula-source="{&quot;type&quot; : &quot;mathjax&quot;}">
     <math>
      <msup>
       <mi>
        F
       </mi>
       <mo>
        ′
       </mo>
      </msup><mo>
       ∪
      </mo><mi>
       R
      </mi>
     </math></span>. We then derive a logical consequence, e.g. in terms of a minimal model. After we repeat the sampling many times, we obtain a set of logical consequences (or interchangeably, interpretation or truth assignment) from the sampled program in terms of some LP semantics, e.g. minimal model semantics. In DS, the probability mass is distributed over these logical consequences. Let <span class="NLM_disp-formula-image inline-formula rs_preserve">
     <noscript>
      <img src="/na101/home/literatum/publisher/tandf/journals/content/tadr20/2019/tadr20.v033.i15-16/01691864.2019.1632223/20210828/images/tadr_a_1632223_ilm0010.gif" alt="">
     </noscript><img src="//:0" alt="" class="mml-formula" data-formula-source="{&quot;type&quot; : &quot;image&quot;, &quot;src&quot; : &quot;/na101/home/literatum/publisher/tandf/journals/content/tadr20/2019/tadr20.v033.i15-16/01691864.2019.1632223/20210828/images/tadr_a_1632223_ilm0010.gif&quot;}"><span class="mml-formula"></span></span><span class="NLM_disp-formula inline-formula rs_preserve"><img src="//:0" alt="" data-formula-source="{&quot;type&quot; : &quot;mathjax&quot;}">
     <math>
      <mrow>
       <mi mathvariant="script">
        I
       </mi>
      </mrow>
     </math></span> be a set of all such logical consequences, namely <span class="NLM_disp-formula-image inline-formula rs_preserve">
     <noscript>
      <img src="/na101/home/literatum/publisher/tandf/journals/content/tadr20/2019/tadr20.v033.i15-16/01691864.2019.1632223/20210828/images/tadr_a_1632223_ilm0011.gif" alt="">
     </noscript><img src="//:0" alt="" class="mml-formula" data-formula-source="{&quot;type&quot; : &quot;image&quot;, &quot;src&quot; : &quot;/na101/home/literatum/publisher/tandf/journals/content/tadr20/2019/tadr20.v033.i15-16/01691864.2019.1632223/20210828/images/tadr_a_1632223_ilm0011.gif&quot;}"><span class="mml-formula"></span></span><span class="NLM_disp-formula inline-formula rs_preserve"><img src="//:0" alt="" data-formula-source="{&quot;type&quot; : &quot;mathjax&quot;}">
     <math>
      <mrow>
       <mi mathvariant="script">
        I
       </mi>
      </mrow><mo>
       =
      </mo><mo fence="false">
       {
      </mo><mi>
       I
      </mi><mo>
       ∣
      </mo><msup>
       <mi>
        F
       </mi>
       <mo>
        ′
       </mo>
      </msup><mo>
       ⊆
      </mo><mi>
       F
      </mi><mo>
       ,
      </mo><msup>
       <mi>
        F
       </mi>
       <mo>
        ′
       </mo>
      </msup><mo>
       ∪
      </mo><mi>
       R
      </mi><mo>
       ⊨
      </mo><mi>
       I
      </mi><mo fence="false">
       }
      </mo>
     </math></span>. In DS, <span class="NLM_disp-formula-image inline-formula rs_preserve">
     <noscript>
      <img src="/na101/home/literatum/publisher/tandf/journals/content/tadr20/2019/tadr20.v033.i15-16/01691864.2019.1632223/20210828/images/tadr_a_1632223_ilm0012.gif" alt="">
     </noscript><img src="//:0" alt="" class="mml-formula" data-formula-source="{&quot;type&quot; : &quot;image&quot;, &quot;src&quot; : &quot;/na101/home/literatum/publisher/tandf/journals/content/tadr20/2019/tadr20.v033.i15-16/01691864.2019.1632223/20210828/images/tadr_a_1632223_ilm0012.gif&quot;}"><span class="mml-formula"></span></span><span class="NLM_disp-formula inline-formula rs_preserve"><img src="//:0" alt="" data-formula-source="{&quot;type&quot; : &quot;mathjax&quot;}">
     <math>
      <munder>
       <mo>
        ∑
       </mo>
       <mrow>
        <mi>
         I
        </mi>
        <mo>
         ∈
        </mo>
        <mrow>
         <mi mathvariant="script">
          I
         </mi>
        </mrow>
       </mrow>
      </munder><mi>
       P
      </mi><mo>
       (
      </mo><mi>
       I
      </mi><mo>
       )
      </mo>
     </math></span> is set to be 1. The probability function of <i>I</i> can be designed arbitrarily.</p>
   <p>One instantiation of DS is ProbLog&nbsp;[<span class="ref-lnk lazy-ref"><a data-rid="CIT0037" data-reflink="_i80 _i82" href="#"><span class="off-screen">Citation</span>37</a></span>]. The basic syntax of ProbLog resembles that of Prolog. As an LP semantics, ProbLog employs a well-founded model&nbsp;[<span class="ref-lnk lazy-ref"><a data-rid="CIT0040" data-reflink="_i80 _i82" href="#"><span class="off-screen">Citation</span>40</a></span>] and assumes that the program given is a locally stratified normal logic programs; under a locally stratified LP, a well-founded model coincides with the minimal model and the stable model. One important extension is <i>probabilistic fact</i> denoted by <sans-serif>
     p::f
    </sans-serif>, which means that fact <sans-serif>
     f
    </sans-serif> is selected with probability <i>p</i> and not selected with probability 1−<i>p</i>. A ProbLog program consists of three components: (i) facts <i>F</i>, (ii) probabilistic facts <span class="NLM_disp-formula-image inline-formula rs_preserve">
     <noscript>
      <img src="/na101/home/literatum/publisher/tandf/journals/content/tadr20/2019/tadr20.v033.i15-16/01691864.2019.1632223/20210828/images/tadr_a_1632223_ilm0013.gif" alt="">
     </noscript><img src="//:0" alt="" class="mml-formula" data-formula-source="{&quot;type&quot; : &quot;image&quot;, &quot;src&quot; : &quot;/na101/home/literatum/publisher/tandf/journals/content/tadr20/2019/tadr20.v033.i15-16/01691864.2019.1632223/20210828/images/tadr_a_1632223_ilm0013.gif&quot;}"><span class="mml-formula"></span></span><span class="NLM_disp-formula inline-formula rs_preserve"><img src="//:0" alt="" data-formula-source="{&quot;type&quot; : &quot;mathjax&quot;}">
     <math>
      <msub>
       <mi>
        F
       </mi>
       <mi>
        P
       </mi>
      </msub>
     </math></span>, and (iii) rules <i>R</i>. Assuming probabilistic independence over the probabilistic facts, the probability of interpretation <i>I</i> is defined as follows: <span class="NLM_disp-formula-image disp-formula rs_preserve">
     <noscript>
      <img src="/na101/home/literatum/publisher/tandf/journals/content/tadr20/2019/tadr20.v033.i15-16/01691864.2019.1632223/20210828/images/tadr_a_1632223_m0001.gif" alt="">
     </noscript><img src="//:0" alt="" class="mml-formula" data-formula-source="{&quot;type&quot; : &quot;image&quot;, &quot;src&quot; : &quot;/na101/home/literatum/publisher/tandf/journals/content/tadr20/2019/tadr20.v033.i15-16/01691864.2019.1632223/20210828/images/tadr_a_1632223_m0001.gif&quot;}"><span class="mml-formula"><span class="disp_formula_label_div"><span id="M0001" class="disp-formula-label">(1) </span></span></span></span><span class="NLM_disp-formula disp-formula rs_preserve"><img src="//:0" alt="" data-formula-source="{&quot;type&quot; : &quot;mathjax&quot;}">
     <math>
      <mi>
       P
      </mi><mo>
       (
      </mo><mi>
       I
      </mi><mo>
       )
      </mo><mo>
       =
      </mo><munder>
       <mo>
        ∑
       </mo>
       <mrow>
        <msup>
         <mi>
          F
         </mi>
         <mo>
          ′
         </mo>
        </msup>
        <mo>
         ∈
        </mo>
        <mrow>
         <mi mathvariant="script">
          F
         </mi>
        </mrow>
        <mo>
         (
        </mo>
        <mi>
         I
        </mi>
        <mo>
         )
        </mo>
       </mrow>
      </munder><mi>
       P
      </mi><mo>
       (
      </mo><msup>
       <mi>
        F
       </mi>
       <mo>
        ′
       </mo>
      </msup><mo>
       )
      </mo><mo>
       =
      </mo><munder>
       <mo>
        ∑
       </mo>
       <mrow>
        <msup>
         <mi>
          F
         </mi>
         <mo>
          ′
         </mo>
        </msup>
        <mo>
         ∈
        </mo>
        <mrow>
         <mi mathvariant="script">
          F
         </mi>
        </mrow>
        <mo>
         (
        </mo>
        <mi>
         I
        </mi>
        <mo>
         )
        </mo>
       </mrow>
      </munder><munder>
       <mo>
        ∏
       </mo>
       <mrow>
        <mrow>
         <mi mathvariant="sans-serif">
          f
         </mi>
        </mrow>
        <mo>
         ∈
        </mo>
        <msup>
         <mi mathvariant="sans-serif">
          F
         </mi>
         <mo>
          ′
         </mo>
        </msup>
       </mrow>
      </munder><mi>
       P
      </mi><mo>
       (
      </mo><mrow>
       <mi mathvariant="sans-serif">
        f
       </mi>
      </mrow><mo mathvariant="sans-serif">
       )
      </mo><munder>
       <mo>
        ∏
       </mo>
       <mrow>
        <mrow>
         <mi mathvariant="sans-serif">
          f
         </mi>
        </mrow>
        <mo>
         ∈
        </mo>
        <msub>
         <mi mathvariant="sans-serif">
          F
         </mi>
         <mi mathvariant="sans-serif">
          P
         </mi>
        </msub>
        <mo>
         ∖
        </mo>
        <msup>
         <mi mathvariant="sans-serif">
          F
         </mi>
         <mo>
          ′
         </mo>
        </msup>
       </mrow>
      </munder><mo mathvariant="sans-serif">
       (
      </mo><mn mathvariant="sans-serif">
       1
      </mn><mo mathvariant="sans-serif">
       −
      </mo><mi mathvariant="sans-serif">
       P
      </mi><mo mathvariant="sans-serif">
       (
      </mo><mrow>
       <mi mathvariant="sans-serif">
        f
       </mi>
      </mrow><mo mathvariant="sans-serif">
       )
      </mo><mo mathvariant="sans-serif">
       )
      </mo><mo mathvariant="sans-serif">
       ,
      </mo>
     </math><span class="mathjaxLabel"><span class="disp_formula_label_div"><span id="M0001" class="disp-formula-label">(1) </span></span></span></span> where <span class="NLM_disp-formula-image inline-formula rs_preserve">
     <noscript>
      <img src="/na101/home/literatum/publisher/tandf/journals/content/tadr20/2019/tadr20.v033.i15-16/01691864.2019.1632223/20210828/images/tadr_a_1632223_ilm0014.gif" alt="">
     </noscript><img src="//:0" alt="" class="mml-formula" data-formula-source="{&quot;type&quot; : &quot;image&quot;, &quot;src&quot; : &quot;/na101/home/literatum/publisher/tandf/journals/content/tadr20/2019/tadr20.v033.i15-16/01691864.2019.1632223/20210828/images/tadr_a_1632223_ilm0014.gif&quot;}"><span class="mml-formula"></span></span><span class="NLM_disp-formula inline-formula rs_preserve"><img src="//:0" alt="" data-formula-source="{&quot;type&quot; : &quot;mathjax&quot;}">
     <math>
      <mrow>
       <mi mathvariant="script">
        F
       </mi>
      </mrow><mo>
       (
      </mo><mi>
       I
      </mi><mo>
       )
      </mo>
     </math></span> is a family of sets of probabilistic facts that has <i>I</i> as a logical consequence, that is, <span class="NLM_disp-formula-image inline-formula rs_preserve">
     <noscript>
      <img src="/na101/home/literatum/publisher/tandf/journals/content/tadr20/2019/tadr20.v033.i15-16/01691864.2019.1632223/20210828/images/tadr_a_1632223_ilm0015.gif" alt="">
     </noscript><img src="//:0" alt="" class="mml-formula" data-formula-source="{&quot;type&quot; : &quot;image&quot;, &quot;src&quot; : &quot;/na101/home/literatum/publisher/tandf/journals/content/tadr20/2019/tadr20.v033.i15-16/01691864.2019.1632223/20210828/images/tadr_a_1632223_ilm0015.gif&quot;}"><span class="mml-formula"></span></span><span class="NLM_disp-formula inline-formula rs_preserve"><img src="//:0" alt="" data-formula-source="{&quot;type&quot; : &quot;mathjax&quot;}">
     <math>
      <mrow>
       <mi mathvariant="script">
        F
       </mi>
      </mrow><mo>
       (
      </mo><mi>
       I
      </mi><mo>
       )
      </mo><mo>
       =
      </mo><mo fence="false">
       {
      </mo><msup>
       <mi>
        F
       </mi>
       <mo>
        ′
       </mo>
      </msup><mo>
       ⊆
      </mo><msub>
       <mi>
        F
       </mi>
       <mi>
        P
       </mi>
      </msub><mo>
       ∣
      </mo><mi>
       F
      </mi><mo>
       ∪
      </mo><msup>
       <mi>
        F
       </mi>
       <mo>
        ′
       </mo>
      </msup><mo>
       ∪
      </mo><mi>
       R
      </mi><mo>
       ⊨
      </mo><mi>
       I
      </mi><mo fence="false">
       }
      </mo>
     </math></span>. ProbLog also has several strong, built-in mechanisms to perform efficient inference, e.g. computing a marginal probability and a maximum a posteriori (MAP) inference, and learn its probabilistic parameters&nbsp;[<span class="ref-lnk lazy-ref"><a data-rid="CIT0041" data-reflink="_i80 _i82" href="#"><span class="off-screen">Citation</span>41</a></span>].</p>
  </div>
 </div>
 <div id="S003-S2002" class="NLM_sec NLM_sec_level_2">
  <h3 class="section-heading-3" id="_i16">3.2. Reasoning with distributed representations</h3>
  <p>Representing the meaning of words, phrases, or even sentences using low-dimensional dense vectors is shown to be effective in a wide range of NLP tasks, such as machine translation, textual entailment, and question answering&nbsp;[<span class="ref-lnk lazy-ref"><a data-rid="CIT0042 CIT0043 CIT0044 CIT0045" data-reflink="_i80 _i82" href="#"><span class="off-screen">Citation</span>42–45</a></span>, etc.]. Such representations are called <i>distributed representations</i>. The benefit of distributed representations is that they allow us to estimate the proximity of meaning based on vector similarity. In the field of reasoning, researchers started to leverage distributed representations in the reasoning process. This section discusses two recent advances in reasoning leveraging distributed representations: automated theorem proving and knowledge base embedding.</p>
  <div id="S003-S2002-S3001" class="NLM_sec NLM_sec_level_3">
   <h4 class="section-heading-4" id="_i17">3.2.1. PLP with distributed representations</h4>
   <p>One weakness of PLP is that a symbol used in logic programs is assumed to represent a unique concept. Consider the logic program <span class="NLM_disp-formula-image inline-formula rs_preserve">
     <noscript>
      <img src="/na101/home/literatum/publisher/tandf/journals/content/tadr20/2019/tadr20.v033.i15-16/01691864.2019.1632223/20210828/images/tadr_a_1632223_ilm0016.gif" alt="">
     </noscript><img src="//:0" alt="" class="mml-formula" data-formula-source="{&quot;type&quot; : &quot;image&quot;, &quot;src&quot; : &quot;/na101/home/literatum/publisher/tandf/journals/content/tadr20/2019/tadr20.v033.i15-16/01691864.2019.1632223/20210828/images/tadr_a_1632223_ilm0016.gif&quot;}"><span class="mml-formula"></span></span><span class="NLM_disp-formula inline-formula rs_preserve"><img src="//:0" alt="" data-formula-source="{&quot;type&quot; : &quot;mathjax&quot;}">
     <math>
      <mi mathvariant="normal">
       Γ
      </mi><mo>
       =
      </mo>
     </math></span> {<sans-serif>
     grandfather
    </sans-serif> <span class="NLM_disp-formula-image inline-formula rs_preserve">
     <noscript>
      <img src="/na101/home/literatum/publisher/tandf/journals/content/tadr20/2019/tadr20.v033.i15-16/01691864.2019.1632223/20210828/images/tadr_a_1632223_ilm0017.gif" alt="">
     </noscript><img src="//:0" alt="" class="mml-formula" data-formula-source="{&quot;type&quot; : &quot;image&quot;, &quot;src&quot; : &quot;/na101/home/literatum/publisher/tandf/journals/content/tadr20/2019/tadr20.v033.i15-16/01691864.2019.1632223/20210828/images/tadr_a_1632223_ilm0017.gif&quot;}"><span class="mml-formula"></span></span><span class="NLM_disp-formula inline-formula rs_preserve"><img src="//:0" alt="" data-formula-source="{&quot;type&quot; : &quot;mathjax&quot;}">
     <math>
      <mo>
       →
      </mo>
     </math></span> <sans-serif>
     happy
    </sans-serif>}. Under the above assumption, PLP does allow us to deduce <sans-serif>
     happy
    </sans-serif> given that <sans-serif>
     grandfather
    </sans-serif> holds; however, given that <sans-serif>
     grandpa
    </sans-serif> holds, it does <i>not</i> allow us to deduce <sans-serif>
     happy
    </sans-serif>, regardless of the conceptual similarity between these two symbols. If we could exhaustively enumerate ontological axioms in the world, i.e. <sans-serif>
     grandpa
    </sans-serif> <span class="NLM_disp-formula-image inline-formula rs_preserve">
     <noscript>
      <img src="/na101/home/literatum/publisher/tandf/journals/content/tadr20/2019/tadr20.v033.i15-16/01691864.2019.1632223/20210828/images/tadr_a_1632223_ilm0018.gif" alt="">
     </noscript><img src="//:0" alt="" class="mml-formula" data-formula-source="{&quot;type&quot; : &quot;image&quot;, &quot;src&quot; : &quot;/na101/home/literatum/publisher/tandf/journals/content/tadr20/2019/tadr20.v033.i15-16/01691864.2019.1632223/20210828/images/tadr_a_1632223_ilm0018.gif&quot;}"><span class="mml-formula"></span></span><span class="NLM_disp-formula inline-formula rs_preserve"><img src="//:0" alt="" data-formula-source="{&quot;type&quot; : &quot;mathjax&quot;}">
     <math>
      <mo>
       ↔
      </mo>
     </math></span> <sans-serif>
     grandfather
    </sans-serif> in this case, then this would not be a problem. However, this assumption is impractical from a knowledge engineering perspective.</p>
   <p>To overcome this weakness, researchers embed PLP, or other logic-based formalisms, into a continuous space using advances in distributed representations&nbsp;[<span class="ref-lnk lazy-ref"><a data-rid="CIT0046 CIT0047 CIT0048 CIT0049 CIT0050 CIT0051 CIT0052" data-reflink="_i80 _i82" href="#"><span class="off-screen">Citation</span>46–52</a></span>, etc.], which is an active topic in both the NLP and machine learning research communities. For instance, Rocktaschel and Riedel&nbsp;[<span class="ref-lnk lazy-ref"><a data-rid="CIT0052" data-reflink="_i80 _i82" href="#"><span class="off-screen">Citation</span>52</a></span>] propose a Prolog-style theorem prover, i.e. selective linear definite clause (SLD) derivation, in a continuous space. Given a goal (or query), SLD derivation subsequently attempts to prove the goal (or subgoal) by unifying it with the head of a rule in the knowledge base. However, as mentioned previously, the unification of a goal <sans-serif>
     grandfatherOf(John, Bob)
    </sans-serif> and head <sans-serif>
     grandpaOf(John, Bob)
    </sans-serif> fails even when these are semantically similar.</p>
   <p>To solve this problem, Rocktaschel and Riedel&nbsp;[<span class="ref-lnk lazy-ref"><a data-rid="CIT0052" data-reflink="_i80 _i82" href="#"><span class="off-screen">Citation</span>52</a></span>] employ soft unification based on the similarity of predicates and constants instead of hard pattern matching. The proposed theorem prover returns a score representing how successful the proof is, instead of whether the proof is successful or not. Specifically, predicates and constants occurring in a knowledge base are embedded into a continuous space by assigning a low-dimensional dense vector to them. When proving a goal, the similarity between a goal and a rule head in the knowledge base is calculated. Because unification is always successful, the proof process is performed up to depth <i>d</i>. The vectors representing predicates and constants are tuned such that a query that can be proven by the knowledge base receives a higher score.</p>
  </div>
  <div id="S003-S2002-S3002" class="NLM_sec NLM_sec_level_3">
   <h4 class="section-heading-4" id="_i18">3.2.2. Embedding knowledge into continuous space</h4>
   <p>Embedding a knowledge base into a continuous space has received much attention in recent years&nbsp;[<span class="ref-lnk lazy-ref"><a data-rid="CIT0053 CIT0054 CIT0055 CIT0056" data-reflink="_i80 _i82" href="#"><span class="off-screen">Citation</span>53–56</a></span>]. The most basic and simple form of knowledge embedding methods is <i>TransE</i>&nbsp;[<span class="ref-lnk lazy-ref"><a data-rid="CIT0057" data-reflink="_i80 _i82" href="#"><span class="off-screen">Citation</span>57</a></span>].</p>
   <p>We assume that a knowledge base contains a set of triplets (<i>h</i>,<i>r</i>,<i>t</i>), where <i>h</i>,<i>t</i> represent an entity and <i>r</i> represents a relation between the entities (e.g. (Tokyo, is_capital_of, Japan)). TransE represents each entity and relation as a point in a <i>n</i>-dimensional continuous space. The core idea of TransE is that for each triplet <span class="NLM_disp-formula-image inline-formula rs_preserve">
     <noscript>
      <img src="/na101/home/literatum/publisher/tandf/journals/content/tadr20/2019/tadr20.v033.i15-16/01691864.2019.1632223/20210828/images/tadr_a_1632223_ilm0019.gif" alt="">
     </noscript><img src="//:0" alt="" class="mml-formula" data-formula-source="{&quot;type&quot; : &quot;image&quot;, &quot;src&quot; : &quot;/na101/home/literatum/publisher/tandf/journals/content/tadr20/2019/tadr20.v033.i15-16/01691864.2019.1632223/20210828/images/tadr_a_1632223_ilm0019.gif&quot;}"><span class="mml-formula"></span></span><span class="NLM_disp-formula inline-formula rs_preserve"><img src="//:0" alt="" data-formula-source="{&quot;type&quot; : &quot;mathjax&quot;}">
     <math>
      <mo>
       (
      </mo><mi>
       h
      </mi><mo>
       ,
      </mo><mi>
       r
      </mi><mo>
       ,
      </mo><mi>
       t
      </mi><mo>
       )
      </mo>
     </math></span> in a knowledge base, the corresponding vector representations <span class="NLM_disp-formula-image inline-formula rs_preserve">
     <noscript>
      <img src="/na101/home/literatum/publisher/tandf/journals/content/tadr20/2019/tadr20.v033.i15-16/01691864.2019.1632223/20210828/images/tadr_a_1632223_ilm0020.gif" alt="">
     </noscript><img src="//:0" alt="" class="mml-formula" data-formula-source="{&quot;type&quot; : &quot;image&quot;, &quot;src&quot; : &quot;/na101/home/literatum/publisher/tandf/journals/content/tadr20/2019/tadr20.v033.i15-16/01691864.2019.1632223/20210828/images/tadr_a_1632223_ilm0020.gif&quot;}"><span class="mml-formula"></span></span><span class="NLM_disp-formula inline-formula rs_preserve"><img src="//:0" alt="" data-formula-source="{&quot;type&quot; : &quot;mathjax&quot;}">
     <math>
      <mrow>
       <mi mathvariant="bold">
        h
       </mi>
       <mo mathvariant="bold">
        ,
       </mo>
       <mi mathvariant="bold">
        r
       </mi>
       <mo mathvariant="bold">
        ,
       </mo>
       <mi mathvariant="bold">
        t
       </mi>
      </mrow>
     </math></span> can be learned by minimizing the following loss function: <span class="NLM_disp-formula-image disp-formula rs_preserve">
     <noscript>
      <img src="/na101/home/literatum/publisher/tandf/journals/content/tadr20/2019/tadr20.v033.i15-16/01691864.2019.1632223/20210828/images/tadr_a_1632223_m0002.gif" alt="">
     </noscript><img src="//:0" alt="" class="mml-formula" data-formula-source="{&quot;type&quot; : &quot;image&quot;, &quot;src&quot; : &quot;/na101/home/literatum/publisher/tandf/journals/content/tadr20/2019/tadr20.v033.i15-16/01691864.2019.1632223/20210828/images/tadr_a_1632223_m0002.gif&quot;}"><span class="mml-formula"><span class="disp_formula_label_div"><span id="M0002" class="disp-formula-label">(2) </span></span></span></span><span class="NLM_disp-formula disp-formula rs_preserve"><img src="//:0" alt="" data-formula-source="{&quot;type&quot; : &quot;mathjax&quot;}">
     <math>
      <mi>
       l
      </mi><mo>
       (
      </mo><mi>
       K
      </mi><mo>
       )
      </mo><mo>
       =
      </mo><munder>
       <mo>
        ∑
       </mo>
       <mrow>
        <mo>
         (
        </mo>
        <mi>
         h
        </mi>
        <mo>
         ,
        </mo>
        <mi>
         r
        </mi>
        <mo>
         ,
        </mo>
        <mi>
         t
        </mi>
        <mo>
         )
        </mo>
        <mo>
         ∈
        </mo>
        <msup>
         <mi>
          K
         </mi>
         <mo>
          +
         </mo>
        </msup>
       </mrow>
      </munder><munder>
       <mo>
        ∑
       </mo>
       <mrow>
        <mo>
         (
        </mo>
        <msup>
         <mi>
          h
         </mi>
         <mo>
          ′
         </mo>
        </msup>
        <mo>
         ,
        </mo>
        <msup>
         <mi>
          r
         </mi>
         <mo>
          ′
         </mo>
        </msup>
        <mo>
         ,
        </mo>
        <msup>
         <mi>
          t
         </mi>
         <mo>
          ′
         </mo>
        </msup>
        <mo>
         )
        </mo>
        <mo>
         ∈
        </mo>
        <msup>
         <mi>
          K
         </mi>
         <mo>
          −
         </mo>
        </msup>
       </mrow>
      </munder><mo>
       [
      </mo><mi>
       γ
      </mi><mo>
       +
      </mo><mi>
       f
      </mi><mo>
       (
      </mo><mi>
       h
      </mi><mo>
       ,
      </mo><mi>
       r
      </mi><mo>
       ,
      </mo><mi>
       t
      </mi><mo>
       )
      </mo><mo>
       −
      </mo><mi>
       f
      </mi><mo>
       (
      </mo><msup>
       <mi>
        h
       </mi>
       <mo>
        ′
       </mo>
      </msup><mo>
       ,
      </mo><msup>
       <mi>
        r
       </mi>
       <mo>
        ′
       </mo>
      </msup><mo>
       ,
      </mo><msup>
       <mi>
        t
       </mi>
       <mo>
        ′
       </mo>
      </msup><mo>
       )
      </mo><msub>
       <mo>
        ]
       </mo>
       <mo>
        +
       </mo>
      </msub><mo>
       ,
      </mo>
     </math><span class="mathjaxLabel"><span class="disp_formula_label_div"><span id="M0002" class="disp-formula-label">(2) </span></span></span></span> where <i>γ</i> is a margin, <span class="NLM_disp-formula-image inline-formula rs_preserve">
     <noscript>
      <img src="/na101/home/literatum/publisher/tandf/journals/content/tadr20/2019/tadr20.v033.i15-16/01691864.2019.1632223/20210828/images/tadr_a_1632223_ilm0021.gif" alt="">
     </noscript><img src="//:0" alt="" class="mml-formula" data-formula-source="{&quot;type&quot; : &quot;image&quot;, &quot;src&quot; : &quot;/na101/home/literatum/publisher/tandf/journals/content/tadr20/2019/tadr20.v033.i15-16/01691864.2019.1632223/20210828/images/tadr_a_1632223_ilm0021.gif&quot;}"><span class="mml-formula"></span></span><span class="NLM_disp-formula inline-formula rs_preserve"><img src="//:0" alt="" data-formula-source="{&quot;type&quot; : &quot;mathjax&quot;}">
     <math>
      <msup>
       <mi>
        K
       </mi>
       <mo>
        +
       </mo>
      </msup><mo>
       ,
      </mo><msup>
       <mi>
        K
       </mi>
       <mo>
        −
       </mo>
      </msup>
     </math></span> represent a set of triplets contained, or not contained, in a knowledge base <i>K</i>, respectively, and <span class="NLM_disp-formula-image inline-formula rs_preserve">
     <noscript>
      <img src="/na101/home/literatum/publisher/tandf/journals/content/tadr20/2019/tadr20.v033.i15-16/01691864.2019.1632223/20210828/images/tadr_a_1632223_ilm0022.gif" alt="">
     </noscript><img src="//:0" alt="" class="mml-formula" data-formula-source="{&quot;type&quot; : &quot;image&quot;, &quot;src&quot; : &quot;/na101/home/literatum/publisher/tandf/journals/content/tadr20/2019/tadr20.v033.i15-16/01691864.2019.1632223/20210828/images/tadr_a_1632223_ilm0022.gif&quot;}"><span class="mml-formula"></span></span><span class="NLM_disp-formula inline-formula rs_preserve"><img src="//:0" alt="" data-formula-source="{&quot;type&quot; : &quot;mathjax&quot;}">
     <math>
      <mo>
       [
      </mo><mo>
       ⋅
      </mo><msub>
       <mo>
        ]
       </mo>
       <mo>
        +
       </mo>
      </msub><mo>
       =
      </mo><mo movablelimits="true">
       max
      </mo><mo>
       (
      </mo><mn>
       0
      </mn><mo>
       ,
      </mo><mo>
       ⋅
      </mo><mo>
       )
      </mo>
     </math></span>. <span class="NLM_disp-formula-image inline-formula rs_preserve">
     <noscript>
      <img src="/na101/home/literatum/publisher/tandf/journals/content/tadr20/2019/tadr20.v033.i15-16/01691864.2019.1632223/20210828/images/tadr_a_1632223_ilm0023.gif" alt="">
     </noscript><img src="//:0" alt="" class="mml-formula" data-formula-source="{&quot;type&quot; : &quot;image&quot;, &quot;src&quot; : &quot;/na101/home/literatum/publisher/tandf/journals/content/tadr20/2019/tadr20.v033.i15-16/01691864.2019.1632223/20210828/images/tadr_a_1632223_ilm0023.gif&quot;}"><span class="mml-formula"></span></span><span class="NLM_disp-formula inline-formula rs_preserve"><img src="//:0" alt="" data-formula-source="{&quot;type&quot; : &quot;mathjax&quot;}">
     <math>
      <mi>
       f
      </mi><mo>
       (
      </mo><mi>
       h
      </mi><mo>
       ,
      </mo><mi>
       r
      </mi><mo>
       ,
      </mo><mi>
       t
      </mi><mo>
       )
      </mo><mo>
       =
      </mo><mo>
       −
      </mo><mo>
       ∥
      </mo><mrow>
       <mi mathvariant="bold">
        h
       </mi>
      </mrow><mo>
       +
      </mo><mrow>
       <mi mathvariant="bold">
        r
       </mi>
      </mrow><mo>
       −
      </mo><mrow>
       <mi mathvariant="bold">
        t
       </mi>
      </mrow><msub>
       <mo>
        ∥
       </mo>
       <mn>
        2
       </mn>
      </msub>
     </math></span> (or L1 norm), represents the quality of triplet <span class="NLM_disp-formula-image inline-formula rs_preserve">
     <noscript>
      <img src="/na101/home/literatum/publisher/tandf/journals/content/tadr20/2019/tadr20.v033.i15-16/01691864.2019.1632223/20210828/images/tadr_a_1632223_ilm0024.gif" alt="">
     </noscript><img src="//:0" alt="" class="mml-formula" data-formula-source="{&quot;type&quot; : &quot;image&quot;, &quot;src&quot; : &quot;/na101/home/literatum/publisher/tandf/journals/content/tadr20/2019/tadr20.v033.i15-16/01691864.2019.1632223/20210828/images/tadr_a_1632223_ilm0024.gif&quot;}"><span class="mml-formula"></span></span><span class="NLM_disp-formula inline-formula rs_preserve"><img src="//:0" alt="" data-formula-source="{&quot;type&quot; : &quot;mathjax&quot;}">
     <math>
      <mo>
       (
      </mo><mi>
       h
      </mi><mo>
       ,
      </mo><mi>
       r
      </mi><mo>
       ,
      </mo><mi>
       t
      </mi><mo>
       )
      </mo>
     </math></span> based on the corresponding vectors. During training, for a triplet <span class="NLM_disp-formula-image inline-formula rs_preserve">
     <noscript>
      <img src="/na101/home/literatum/publisher/tandf/journals/content/tadr20/2019/tadr20.v033.i15-16/01691864.2019.1632223/20210828/images/tadr_a_1632223_ilm0025.gif" alt="">
     </noscript><img src="//:0" alt="" class="mml-formula" data-formula-source="{&quot;type&quot; : &quot;image&quot;, &quot;src&quot; : &quot;/na101/home/literatum/publisher/tandf/journals/content/tadr20/2019/tadr20.v033.i15-16/01691864.2019.1632223/20210828/images/tadr_a_1632223_ilm0025.gif&quot;}"><span class="mml-formula"></span></span><span class="NLM_disp-formula inline-formula rs_preserve"><img src="//:0" alt="" data-formula-source="{&quot;type&quot; : &quot;mathjax&quot;}">
     <math>
      <mo>
       (
      </mo><mi>
       h
      </mi><mo>
       ,
      </mo><mi>
       r
      </mi><mo>
       ,
      </mo><mi>
       t
      </mi><mo>
       )
      </mo>
     </math></span> contained in a knowledge base, <span class="NLM_disp-formula-image inline-formula rs_preserve">
     <noscript>
      <img src="/na101/home/literatum/publisher/tandf/journals/content/tadr20/2019/tadr20.v033.i15-16/01691864.2019.1632223/20210828/images/tadr_a_1632223_ilm0026.gif" alt="">
     </noscript><img src="//:0" alt="" class="mml-formula" data-formula-source="{&quot;type&quot; : &quot;image&quot;, &quot;src&quot; : &quot;/na101/home/literatum/publisher/tandf/journals/content/tadr20/2019/tadr20.v033.i15-16/01691864.2019.1632223/20210828/images/tadr_a_1632223_ilm0026.gif&quot;}"><span class="mml-formula"></span></span><span class="NLM_disp-formula inline-formula rs_preserve"><img src="//:0" alt="" data-formula-source="{&quot;type&quot; : &quot;mathjax&quot;}">
     <math>
      <mrow>
       <mi mathvariant="bold">
        h
       </mi>
      </mrow><mo>
       +
      </mo><mrow>
       <mi mathvariant="bold">
        r
       </mi>
      </mrow>
     </math></span> becomes closer to <span class="NLM_disp-formula-image inline-formula rs_preserve">
     <noscript>
      <img src="/na101/home/literatum/publisher/tandf/journals/content/tadr20/2019/tadr20.v033.i15-16/01691864.2019.1632223/20210828/images/tadr_a_1632223_ilm0027.gif" alt="">
     </noscript><img src="//:0" alt="" class="mml-formula" data-formula-source="{&quot;type&quot; : &quot;image&quot;, &quot;src&quot; : &quot;/na101/home/literatum/publisher/tandf/journals/content/tadr20/2019/tadr20.v033.i15-16/01691864.2019.1632223/20210828/images/tadr_a_1632223_ilm0027.gif&quot;}"><span class="mml-formula"></span></span><span class="NLM_disp-formula inline-formula rs_preserve"><img src="//:0" alt="" data-formula-source="{&quot;type&quot; : &quot;mathjax&quot;}">
     <math>
      <mrow>
       <mi mathvariant="bold">
        t
       </mi>
      </mrow>
     </math></span>.</p>
   <p>These learned distributed representations can be used for automated question answering, for instance. For example, consider a question ‘<i>Where was Obama born?</i>'. Let the distributed representation of <i>Obama</i> and <i>is_born_in</i> be <span class="NLM_disp-formula-image inline-formula rs_preserve">
     <noscript>
      <img src="/na101/home/literatum/publisher/tandf/journals/content/tadr20/2019/tadr20.v033.i15-16/01691864.2019.1632223/20210828/images/tadr_a_1632223_ilm0028.gif" alt="">
     </noscript><img src="//:0" alt="" class="mml-formula" data-formula-source="{&quot;type&quot; : &quot;image&quot;, &quot;src&quot; : &quot;/na101/home/literatum/publisher/tandf/journals/content/tadr20/2019/tadr20.v033.i15-16/01691864.2019.1632223/20210828/images/tadr_a_1632223_ilm0028.gif&quot;}"><span class="mml-formula"></span></span><span class="NLM_disp-formula inline-formula rs_preserve"><img src="//:0" alt="" data-formula-source="{&quot;type&quot; : &quot;mathjax&quot;}">
     <math>
      <mrow>
       <mi mathvariant="bold">
        o
       </mi>
       <mi mathvariant="bold">
        b
       </mi>
       <mi mathvariant="bold">
        a
       </mi>
       <mi mathvariant="bold">
        m
       </mi>
       <mi mathvariant="bold">
        a
       </mi>
      </mrow><mo>
       ,
      </mo><mrow>
       <mi mathvariant="bold">
        i
       </mi>
       <mi mathvariant="bold">
        s
       </mi>
       <mi mathvariant="bold">
        _
       </mi>
       <mi mathvariant="bold">
        b
       </mi>
       <mi mathvariant="bold">
        o
       </mi>
       <mi mathvariant="bold">
        r
       </mi>
       <mi mathvariant="bold">
        n
       </mi>
       <mi mathvariant="bold">
        _
       </mi>
       <mi mathvariant="bold">
        i
       </mi>
       <mi mathvariant="bold">
        n
       </mi>
      </mrow>
     </math></span>. Let the distributed representation of an arbitrary entity <i>t</i> be <span class="NLM_disp-formula-image inline-formula rs_preserve">
     <noscript>
      <img src="/na101/home/literatum/publisher/tandf/journals/content/tadr20/2019/tadr20.v033.i15-16/01691864.2019.1632223/20210828/images/tadr_a_1632223_ilm0029.gif" alt="">
     </noscript><img src="//:0" alt="" class="mml-formula" data-formula-source="{&quot;type&quot; : &quot;image&quot;, &quot;src&quot; : &quot;/na101/home/literatum/publisher/tandf/journals/content/tadr20/2019/tadr20.v033.i15-16/01691864.2019.1632223/20210828/images/tadr_a_1632223_ilm0029.gif&quot;}"><span class="mml-formula"></span></span><span class="NLM_disp-formula inline-formula rs_preserve"><img src="//:0" alt="" data-formula-source="{&quot;type&quot; : &quot;mathjax&quot;}">
     <math>
      <mrow>
       <mi mathvariant="bold">
        t
       </mi>
      </mrow>
     </math></span>. To answer the question, we need to find an entity <i>t</i> that maximizes <span class="NLM_disp-formula-image inline-formula rs_preserve">
     <noscript>
      <img src="/na101/home/literatum/publisher/tandf/journals/content/tadr20/2019/tadr20.v033.i15-16/01691864.2019.1632223/20210828/images/tadr_a_1632223_ilm0030.gif" alt="">
     </noscript><img src="//:0" alt="" class="mml-formula" data-formula-source="{&quot;type&quot; : &quot;image&quot;, &quot;src&quot; : &quot;/na101/home/literatum/publisher/tandf/journals/content/tadr20/2019/tadr20.v033.i15-16/01691864.2019.1632223/20210828/images/tadr_a_1632223_ilm0030.gif&quot;}"><span class="mml-formula"></span></span><span class="NLM_disp-formula inline-formula rs_preserve"><img src="//:0" alt="" data-formula-source="{&quot;type&quot; : &quot;mathjax&quot;}">
     <math>
      <mi>
       f
      </mi><mo>
       (
      </mo><mrow>
       <mi mathvariant="bold">
        o
       </mi>
       <mi mathvariant="bold">
        b
       </mi>
       <mi mathvariant="bold">
        a
       </mi>
       <mi mathvariant="bold">
        m
       </mi>
       <mi mathvariant="bold">
        a
       </mi>
      </mrow><mo>
       ,
      </mo><mrow>
       <mi mathvariant="bold">
        i
       </mi>
       <mi mathvariant="bold">
        s
       </mi>
       <mi mathvariant="bold">
        _
       </mi>
       <mi mathvariant="bold">
        b
       </mi>
       <mi mathvariant="bold">
        o
       </mi>
       <mi mathvariant="bold">
        r
       </mi>
       <mi mathvariant="bold">
        n
       </mi>
       <mi mathvariant="bold">
        _
       </mi>
       <mi mathvariant="bold">
        i
       </mi>
       <mi mathvariant="bold">
        n
       </mi>
      </mrow><mo>
       ,
      </mo><mrow>
       <mi mathvariant="bold">
        t
       </mi>
      </mrow><mo>
       )
      </mo>
     </math></span>. Compared to simple pattern matching, the advantage here is flexibility—the system can answer the question in a situation where (Obama, is_born_in, ·) does not exist in the knowledge base, but similarly related instances such as (Obama, is_given_birth_in, ·) do.</p>
  </div>
 </div>
 <div id="S003-S2003" class="NLM_sec NLM_sec_level_2">
  <h3 class="section-heading-3" id="_i20">3.3. Challenges in PLP and distributed representations</h3>
  <p>Despite recent advances in the community, many obstacles remain to integrate logical inference within robots in the real world.</p>
  <p>First, in the work we have seen so far, the model of the world is not grounded in the real world. Second, the vocabulary set, i.e. predicates and terms, used for representing observations and a knowledge base is predefined by the user. Third, the knowledge acquisition bottleneck is still present. The use of distributed representations partially solves this issue, i.e. through ontological knowledge; however, a separate mechanism is required for other types of knowledge, e.g. relations between events such as causal relations. Therefore, the following challenges remain as open questions:</p>
  <ul class="NLM_list NLM_list-list_type-bullet">
   <li><p class="inline">Developing a method for logical inference where the model of the world is grounded on continuous and constantly changing real-world sensory inputs.</p></li>
   <li><p class="inline">Developing a mechanism to associate new concepts emerging from inputs with existing predicates, i.e. ‘packing’ similar concepts from robots' perceptions and ‘labeling’ them.</p></li>
   <li><p class="inline">Enhancing purely symbolic logical inference with causal inference in the physical world, e.g. using a physics simulator.</p></li>
  </ul>
  <p></p>
 </div>
</div>
<div id="S004" class="NLM_sec NLM_sec_level_1">
 <h2 id="_i21" class="section-heading-2">4. Unsupervised syntactic parsing with grounding phrases and predicates</h2>
 <p>To conduct the logical inferences described earlier, <i>syntactic parsing</i> should be perfected in advance to be suitable for real-world communication. For example, the robot in Figure&nbsp;<a href="#F0001">1</a> is inferring the latent syntactic structure of the sentence given, and understands it needs to bring ‘the bottle’, not ‘the kitchen'. Syntactic parsing is indispensable for semantic parsing, semantic role identification, and other semantics-driven tasks in NLP. Syntactic parsing can essentially be categorized as follows, in the current practice of NLP&nbsp;[<span class="ref-lnk lazy-ref"><a data-rid="CIT0058" data-reflink="_i80 _i82" href="#"><span class="off-screen">Citation</span>58</a></span>]: (a) dependency parsing, (b) constituent parsing, such as context-free grammars (CFG), tree adjoining grammars (TAG), and (c) combinatory categorial grammars (CCG).</p>
 <p>Dependency parsing has become widespread by virtue of its simplicity and universality, and downstream tasks are often designed assuming dependencies. However, as it can be converted into a specific form of CFG&nbsp;[<span class="ref-lnk lazy-ref"><a data-rid="CIT0059" data-reflink="_i80 _i82" href="#"><span class="off-screen">Citation</span>59</a></span>], constituent parsing such as CFG is still a key issue. Furthermore, because CCG can also handle information that cannot be dealt with using other formalisms (such as <i>λ</i>-calculus expressions), CCG has attracted much attention of late and much research has emerged around CCG, following&nbsp;[<span class="ref-lnk lazy-ref"><a data-rid="CIT0060" data-reflink="_i80 _i82" href="#"><span class="off-screen">Citation</span>60</a></span>].</p>
 <div id="S004-S2001" class="NLM_sec NLM_sec_level_2">
  <h3 class="section-heading-3" id="_i22">4.1. Unsupervised and supervised parsing</h3>
  <p>Learning a grammar is straightforward if equipped with a corpus of annotated ground truth trees, i.e. data labels for training a syntactic parser. However, in the realm of robotics, we need the unsupervised learning of grammars to be flexible and to fit the utterances of users. Indeed, aside from developmental considerations, users often speak in a way that cannot be handled by a predefined grammar, which is usually based on written text prepared in a different environment. Such colloquial expressions are especially characteristic of robotics. Unsupervised models are also necessary for semi-supervised learning as a prerequisite of using existing grammars&nbsp;[<span class="ref-lnk lazy-ref"><a data-rid="CIT0061" data-reflink="_i80 _i82" href="#"><span class="off-screen">Citation</span>61</a></span>,<span class="ref-lnk lazy-ref"><a data-rid="CIT0062" data-reflink="_i80 _i82" href="#"><span class="off-screen">Citation</span>62</a></span>]. Below, we describe the current status of the aforementioned three approaches to parsing.</p>
  <div id="S004-S2001-S3001" class="NLM_sec NLM_sec_level_3">
   <h4 class="section-heading-4" id="_i23">4.1.1. Dependency parsing</h4>
   <p>The most basic model of unsupervised dependency parsing was first introduced by Klein and Manning&nbsp;[<span class="ref-lnk lazy-ref"><a data-rid="CIT0063" data-reflink="_i80 _i82" href="#"><span class="off-screen">Citation</span>63</a></span>] and was referred to as the dependency model with valence<span class="ref-lnk fn-ref-lnk lazy-ref"><a data-rid="EN0001" href="#" data-reflink="fn"><span class="off-screen">Footnote</span><sup>1</sup></a></span> (DMV). DMV is a statistical generative model that yields a sentence by iteratively generating a dependent word in a random direction from the source word in a probabilistic fashion. DMV has been improved markedly through many studies since [<span class="ref-lnk lazy-ref"><a data-rid="CIT0064 CIT0065 CIT0066" data-reflink="_i80 _i82" href="#"><span class="off-screen">Citation</span>64–66</a></span>]. For example, Headden et&nbsp;al. [<span class="ref-lnk lazy-ref"><a data-rid="CIT0064" data-reflink="_i80 _i82" href="#"><span class="off-screen">Citation</span>64</a></span>] introduced enhanced smoothing and lexicalization based on the Bayesian treatment of equivalent probabilistic CFG. Spitkovsky et&nbsp;al.&nbsp;[<span class="ref-lnk lazy-ref"><a data-rid="CIT0065" data-reflink="_i80 _i82" href="#"><span class="off-screen">Citation</span>65</a></span>] controlled the data fed to the inference algorithm from straightforward to complex, mimicking a baby learning a language, and yielded better accuracy than a simple batch inference. Jiang et&nbsp;al. [<span class="ref-lnk lazy-ref"><a data-rid="CIT0066" data-reflink="_i80 _i82" href="#"><span class="off-screen">Citation</span>66</a></span>] recently leveraged neural networks to deal with possible correlations in grammar rules, defining the state of the art as an extension of a basic DMV.</p>
   <p>However, dependency parsing has limitations: the most prominent issue is that large syntactic structures such as relative clauses or compositional sentences cannot be recognized. For example, ‘that…’ in ‘it is true <i>that…</i>’ contains a sentence, but dependency parsing just attaches ‘that’ to the head of the sentence and cannot recognize the fact that the term ‘that’ is not a pronoun here but introduces a relative clause. For this purpose, constituent parsing is a better alternative, as described below.</p>
  </div>
  <div id="S004-S2001-S3002" class="NLM_sec NLM_sec_level_3">
   <h4 class="section-heading-4" id="_i24">4.1.2. Constituent parsing</h4>
   <p>Constituent parsing is a general term referring to a model that assigns hierarchical phrase structures to a sentence. The most basic of these is CFG and its probabilistic extension, probabilistic CFG (PCFG) [<span class="ref-lnk lazy-ref"><a data-rid="CIT0067" data-reflink="_i80 _i82" href="#"><span class="off-screen">Citation</span>67</a></span>]. For example, PCFG decomposes a sentence (S) into a noun (N) and a verb phrase (VP), VP in turn decomposes into a verb (V) and N, and finally N and V are substituted with actual words such as ‘she plays music'. This approach has a long history in the field of NLP, and many extensions and inference methods have been proposed&nbsp;[<span class="ref-lnk lazy-ref"><a data-rid="CIT0058" data-reflink="_i80 _i82" href="#"><span class="off-screen">Citation</span>58</a></span>,<span class="ref-lnk lazy-ref"><a data-rid="CIT0067" data-reflink="_i80 _i82" href="#"><span class="off-screen">Citation</span>67</a></span>].</p>
   <p>However, unsupervised learning of PCFG is a notoriously difficult problem, because we usually only need to find few valid parses of a sentence within <span class="NLM_disp-formula-image inline-formula rs_preserve">
     <noscript>
      <img src="/na101/home/literatum/publisher/tandf/journals/content/tadr20/2019/tadr20.v033.i15-16/01691864.2019.1632223/20210828/images/tadr_a_1632223_ilm0031.gif" alt="">
     </noscript><img src="//:0" alt="" class="mml-formula" data-formula-source="{&quot;type&quot; : &quot;image&quot;, &quot;src&quot; : &quot;/na101/home/literatum/publisher/tandf/journals/content/tadr20/2019/tadr20.v033.i15-16/01691864.2019.1632223/20210828/images/tadr_a_1632223_ilm0031.gif&quot;}"><span class="mml-formula"></span></span><span class="NLM_disp-formula inline-formula rs_preserve"><img src="//:0" alt="" data-formula-source="{&quot;type&quot; : &quot;mathjax&quot;}">
     <math>
      <mi>
       O
      </mi><mo>
       (
      </mo><msup>
       <mi>
        N
       </mi>
       <mn>
        3
       </mn>
      </msup><msup>
       <mi>
        K
       </mi>
       <mn>
        3
       </mn>
      </msup><mo>
       )
      </mo>
     </math></span> possibilities, where <i>N</i> is the length of the sentence and <i>K</i> is the number of nonterminal symbols, i.e. syntactic categories. Therefore, inference in unsupervised PCFG induction is quite prone to being trapped by local maxima, and thus has been avoided for a long time. Johnson et&nbsp;al.&nbsp;[<span class="ref-lnk lazy-ref"><a data-rid="CIT0068" data-reflink="_i80 _i82" href="#"><span class="off-screen">Citation</span>68</a></span>] recently proposed a novel MCMC sampling scheme for this problem that avoids local maxima by using a Bayesian inference on PCFG induction. For simplicity, these studies on PCFG parsing assumed part-of-speech (POS) tags, i.e. preterminals, as input. Pate and Johnson [<span class="ref-lnk lazy-ref"><a data-rid="CIT0069" data-reflink="_i80 _i82" href="#"><span class="off-screen">Citation</span>69</a></span>] showed that using a word itself instead of a POS improves parsing accuracy; Levy et&nbsp;al. [<span class="ref-lnk lazy-ref"><a data-rid="CIT0070" data-reflink="_i80 _i82" href="#"><span class="off-screen">Citation</span>70</a></span>] employed a sequential Monte Carlo method for the online inference of grammars, which resembles actual situations found in robotics research.</p>
  </div>
  <div id="S004-S2001-S3003" class="NLM_sec NLM_sec_level_3">
   <h4 class="section-heading-4" id="_i25">4.1.3. CCGs</h4>
   <p>Although CFG recognizes phrase structures, it is still limited, because these structures only have a symbolic meaning. For example, a rule S <span class="NLM_disp-formula-image inline-formula rs_preserve">
     <noscript>
      <img src="/na101/home/literatum/publisher/tandf/journals/content/tadr20/2019/tadr20.v033.i15-16/01691864.2019.1632223/20210828/images/tadr_a_1632223_ilm0032.gif" alt="">
     </noscript><img src="//:0" alt="" class="mml-formula" data-formula-source="{&quot;type&quot; : &quot;image&quot;, &quot;src&quot; : &quot;/na101/home/literatum/publisher/tandf/journals/content/tadr20/2019/tadr20.v033.i15-16/01691864.2019.1632223/20210828/images/tadr_a_1632223_ilm0032.gif&quot;}"><span class="mml-formula"></span></span><span class="NLM_disp-formula inline-formula rs_preserve"><img src="//:0" alt="" data-formula-source="{&quot;type&quot; : &quot;mathjax&quot;}">
     <math>
      <mo>
       →
      </mo>
     </math></span> NP VP merely states that the symbol ‘NP’ can be decomposed into a pair of symbols ‘NP’ and ‘VP', which has nothing to do with the fact that the actual words it governs are nouns or verbs. Therefore, the CFG analysis of sentence inevitably becomes a type of hierarchical POS tagging, i.e. a combinatorial process to yield preterminals such as N or V.</p>
   <p>CCG [<span class="ref-lnk lazy-ref"><a data-rid="CIT0060" data-reflink="_i80 _i82" href="#"><span class="off-screen">Citation</span>60</a></span>] is a formalism that does not suffer from this issue: all phrase structures in CCG are functions and derived from the bottom. For example, because VPs require a noun phrase (NP) to be an S, a VP is actually denoted by <span class="NLM_disp-formula-image inline-formula rs_preserve">
     <noscript>
      <img src="/na101/home/literatum/publisher/tandf/journals/content/tadr20/2019/tadr20.v033.i15-16/01691864.2019.1632223/20210828/images/tadr_a_1632223_ilm0033.gif" alt="">
     </noscript><img src="//:0" alt="" class="mml-formula" data-formula-source="{&quot;type&quot; : &quot;image&quot;, &quot;src&quot; : &quot;/na101/home/literatum/publisher/tandf/journals/content/tadr20/2019/tadr20.v033.i15-16/01691864.2019.1632223/20210828/images/tadr_a_1632223_ilm0033.gif&quot;}"><span class="mml-formula"></span></span><span class="NLM_disp-formula inline-formula rs_preserve"><img src="//:0" alt="" data-formula-source="{&quot;type&quot; : &quot;mathjax&quot;}">
     <math>
      <mrow>
       <mi mathvariant="normal">
        S
       </mi>
      </mrow><mspace width="thinmathspace"></mspace><mrow>
       <mo>
        |
       </mo>
      </mrow><mspace width="thinmathspace"></mspace><mrow>
       <mi mathvariant="normal">
        N
       </mi>
       <mi mathvariant="normal">
        P
       </mi>
      </mrow>
     </math></span><span class="ref-lnk fn-ref-lnk lazy-ref"><a data-rid="EN0002" href="#" data-reflink="fn"><span class="off-screen">Footnote</span><sup>2</sup></a></span> instead of a distinct, and meaningless, symbol VP. An NP is also an artifact that possibly takes a determiner (DT) to function as an N, thus denoted by <span class="NLM_disp-formula-image inline-formula rs_preserve">
     <noscript>
      <img src="/na101/home/literatum/publisher/tandf/journals/content/tadr20/2019/tadr20.v033.i15-16/01691864.2019.1632223/20210828/images/tadr_a_1632223_ilm0034.gif" alt="">
     </noscript><img src="//:0" alt="" class="mml-formula" data-formula-source="{&quot;type&quot; : &quot;image&quot;, &quot;src&quot; : &quot;/na101/home/literatum/publisher/tandf/journals/content/tadr20/2019/tadr20.v033.i15-16/01691864.2019.1632223/20210828/images/tadr_a_1632223_ilm0034.gif&quot;}"><span class="mml-formula"></span></span><span class="NLM_disp-formula inline-formula rs_preserve"><img src="//:0" alt="" data-formula-source="{&quot;type&quot; : &quot;mathjax&quot;}">
     <math>
      <mrow>
       <mi mathvariant="normal">
        N
       </mi>
      </mrow><mspace width="thinmathspace"></mspace><mrow>
       <mo>
        |
       </mo>
      </mrow><mspace width="thinmathspace"></mspace><mrow>
       <mi mathvariant="normal">
        D
       </mi>
       <mi mathvariant="normal">
        T
       </mi>
      </mrow>
     </math></span>; therefore a VP is finally denoted as <span class="NLM_disp-formula-image inline-formula rs_preserve">
     <noscript>
      <img src="/na101/home/literatum/publisher/tandf/journals/content/tadr20/2019/tadr20.v033.i15-16/01691864.2019.1632223/20210828/images/tadr_a_1632223_ilm0035.gif" alt="">
     </noscript><img src="//:0" alt="" class="mml-formula" data-formula-source="{&quot;type&quot; : &quot;image&quot;, &quot;src&quot; : &quot;/na101/home/literatum/publisher/tandf/journals/content/tadr20/2019/tadr20.v033.i15-16/01691864.2019.1632223/20210828/images/tadr_a_1632223_ilm0035.gif&quot;}"><span class="mml-formula"></span></span><span class="NLM_disp-formula inline-formula rs_preserve"><img src="//:0" alt="" data-formula-source="{&quot;type&quot; : &quot;mathjax&quot;}">
     <math>
      <mrow>
       <mi mathvariant="normal">
        S
       </mi>
      </mrow><mspace width="thinmathspace"></mspace><mrow>
       <mo>
        |
       </mo>
      </mrow><mspace width="thinmathspace"></mspace><mo>
       (
      </mo><mrow>
       <mi mathvariant="normal">
        N
       </mi>
      </mrow><mspace width="thinmathspace"></mspace><mrow>
       <mo>
        |
       </mo>
      </mrow><mspace width="thinmathspace"></mspace><mrow>
       <mi mathvariant="normal">
        D
       </mi>
       <mi mathvariant="normal">
        T
       </mi>
      </mrow><mo>
       )
      </mo>
     </math></span>. CCG was introduced to NLP around 2002&nbsp;[<span class="ref-lnk lazy-ref"><a data-rid="CIT0071" data-reflink="_i80 _i82" href="#"><span class="off-screen">Citation</span>71</a></span>], and supervised learning was readily available. By contrast, the unsupervised learning of CCG was only introduced in 2013 [<span class="ref-lnk lazy-ref"><a data-rid="CIT0072" data-reflink="_i80 _i82" href="#"><span class="off-screen">Citation</span>72</a></span>] using a framework of hierarchical Dirichlet processes (HDP)&nbsp;[<span class="ref-lnk lazy-ref"><a data-rid="CIT0073" data-reflink="_i80 _i82" href="#"><span class="off-screen">Citation</span>73</a></span>]. From a statistical perspective, it has a clear advantage over an unsupervised PCFG also using HDP&nbsp;[<span class="ref-lnk lazy-ref"><a data-rid="CIT0074" data-reflink="_i80 _i82" href="#"><span class="off-screen">Citation</span>74</a></span>], in that it simply utilizes an infinite-dimensional vector of probabilities as opposed to a matrix of infinite × infinite dimensions. Martínez-Gómez et&nbsp;al. [<span class="ref-lnk lazy-ref"><a data-rid="CIT0075" data-reflink="_i80 _i82" href="#"><span class="off-screen">Citation</span>75</a></span>] recently introduced <span class="monospace">ccg2lambda</span>, which combines CCG parsing with a <i>λ</i>-calculus to enable an inference on textual entailment. It also has the advantage of handling ambiguities by virtue of a statistical formulation using logistic regression.</p>
  </div>
 </div>
 <div id="S004-S2002" class="NLM_sec NLM_sec_level_2">
  <h3 class="section-heading-3" id="_i26">4.2. Semantic parsing and grounding</h3>
  <p>Once these syntactic analyses of the sentence are available, we can associate them with external information. This process is sometimes called ‘grounding’ and is also studied in the field of NLP [<span class="ref-lnk lazy-ref"><a data-rid="CIT0076" data-reflink="_i80 _i82" href="#"><span class="off-screen">Citation</span>76</a></span>]. The term ‘grounding’ is related to the symbol grounding problem&nbsp;[<span class="ref-lnk lazy-ref"><a data-rid="CIT0020" data-reflink="_i80 _i82" href="#"><span class="off-screen">Citation</span>20</a></span>]. However, the symbol grounding problem does not concern the interpretation of symbols, i.e. semiosis, or language understanding&nbsp;[<span class="ref-lnk lazy-ref"><a data-rid="CIT0018" data-reflink="_i80 _i82" href="#"><span class="off-screen">Citation</span>18</a></span>]. The ‘grounding’ here could be rephrased as ‘language understanding using sensory–motor information'. Semantic parsing and ‘grounding’ by robots, i.e. sensory–motor systems, that can associate syntactic structure with dynamic information, is important for language acquisition and understanding in a real-world environment.</p>
  <p>In a discrete case, Poon [<span class="ref-lnk lazy-ref"><a data-rid="CIT0077" data-reflink="_i80 _i82" href="#"><span class="off-screen">Citation</span>77</a></span>] leverages a travel planning database called ATIS and associates nodes and edges in a syntactic tree with the database. This is essentially a nested hidden Markov model (HMM), based on unsupervised semantic parsing&nbsp;[<span class="ref-lnk lazy-ref"><a data-rid="CIT0078" data-reflink="_i80 _i82" href="#"><span class="off-screen">Citation</span>78</a></span>] that automatically clusters each predicate in a tree by maximizing the likelihood of a sentence computed by a Markov logic network. It has the clear advantage of abstracting away various possible linguistic expressions with respect to the database; however, the database must be given in advance and usually has a narrow scope. Because of its discrete nature, this approach cannot discern subtle differences in linguistic expression and adjust the actual behavior of the robots accordingly.</p>
  <p>In a continuous case, there is an abundance of research connecting linguistic expressions with images [<span class="ref-lnk lazy-ref"><a data-rid="CIT0079 CIT0080 CIT0081 CIT0082" data-reflink="_i80 _i82" href="#"><span class="off-screen">Citation</span>79–82</a></span>]. As an example more closely inspired by robotics, [<span class="ref-lnk lazy-ref"><a data-rid="CIT0083" data-reflink="_i80 _i82" href="#"><span class="off-screen">Citation</span>83</a></span>] and its extension [<span class="ref-lnk lazy-ref"><a data-rid="CIT0084" data-reflink="_i80 _i82" href="#"><span class="off-screen">Citation</span>84</a></span>] aim to discriminate which predicates are applicable to a given object, such as ‘lift’ and ‘move', but not ‘sing', for a box. To solve this problem, the former employs consensus decoding and the latter uses a mixed observability Markov decision process (MDP) leveraging sensory information. Although these works connect linguistic expression with sensory information, this content, such as imagery, is usually static and the objective is discriminative. Moreover, the candidate predicates are known in advance, and thus the approach does not cover broad linguistic expressions in general.</p>
  <p>Note that we are not insisting on ‘grounding’ any word or phrase on the sensory–motor information provided by robots, i.e. external information. Sensory–motor information provides the cognitive system of a robot with observations, e.g. visual, auditory, and haptic information. However, many words representing abstract concepts cannot be directly ‘grounded’ on such sensory–motor information. For example, we cannot determine a proper probabilistic distribution of sensory–motor information for ‘in', ‘freedom’, or ‘the'. Even a verb can be considered as an abstract concept. ‘Through’ can represent different trajectories or a controller depending on target objects. Even though ‘in’ is abstract, ‘in front of the door’ seems concrete and more conducive to an association with sensory–motor information. Semantic parsing with real-world information and finding a way to handle abstract concepts is an important challenge.</p>
 </div>
 <div id="S004-S2003" class="NLM_sec NLM_sec_level_2">
  <h3 class="section-heading-3" id="_i27">4.3. Challenges in unsupervised syntactic parsing with grounding phrases and predicates</h3>
  <p>Studies involving the unsupervised learning of syntactic parsing in robotics are still in the preliminary stage. Attamimi et&nbsp;al. developed an integrative robotics system that can learn word meaning and grammar in an unsupervised manner&nbsp;[<span class="ref-lnk lazy-ref"><a data-rid="CIT0085" data-reflink="_i80 _i82" href="#"><span class="off-screen">Citation</span>85</a></span>]. However, this approach relies on HMM, which does not have a hierarchical structure, for modeling grammar. Aly et&nbsp;al. introduced an unsupervised CCG to enable a robot to find the syntactic role of each word in a situated human–robot interaction&nbsp;[<span class="ref-lnk lazy-ref"><a data-rid="CIT0086" data-reflink="_i80 _i82" href="#"><span class="off-screen">Citation</span>86</a></span>].</p>
  <p>In connection with the topics described in this section, we identified the following challenges:</p>
  <p></p>
  <ul class="NLM_list NLM_list-list_type-bullet">
   <li><p class="inline">Enabling robust unsupervised parsing of colloquial or nonstandard sentences with the help of multimodal information obtained from robots.</p></li>
   <li><p class="inline">Associating syntactic structures and substructures (such as those in CCG) with sensor–motor information for grounded language interpretation and generation.</p></li>
   <li><p class="inline">Developing a machine learning algorithm to associate the predicates in semantic parsing with a distributed meaning representation in robots, organized based on sensor–motor information.</p></li>
  </ul>
  <p></p>
 </div>
</div>
<div id="S005" class="NLM_sec NLM_sec_level_1">
 <h2 id="_i28" class="section-heading-2">5. Category and concept formation</h2>
 <p>The ability to categorize and to conceptualize is essential to living creatures. A creature may not to survive if it is not capable of distinguishing beneficial items from harmful ones including, with regard to food, whether it is edible or not. Humans have categorized objects, actions, events, emotions, intentions, and phenomena. In addition, we label these using language. Thus, it is reasonable to say that language reflects the way we think and perceive the world around us, and considering linguistic categories is an effective way to comprehend the human ability to categorize and the way robots should operate in this respect. To understand human language, a robot needs to be able to categorize objects and events, and to handle concepts. In Figure&nbsp;<a href="#F0001">1</a>, the robot grasps the concepts of ‘chair', ‘bottle’, and ‘ball', and can understand utterances from the user. Of course, the robot needs to understand ‘grasp', ‘bring’, ‘thirsty’, and ‘joy’ as well. During this decade, unsupervised categorization and concept formation have been studied in robotics&nbsp;[<span class="ref-lnk lazy-ref"><a data-rid="CIT0017" data-reflink="_i80 _i82" href="#"><span class="off-screen">Citation</span>17</a></span>]. This section describes the foundation of category and concept formation, the current state of robotics, and future challenges.</p>
 <div id="S005-S2001" class="NLM_sec NLM_sec_level_2">
  <h3 class="section-heading-3" id="_i29">5.1. Linguistic categories</h3>
  <div id="S005-S2001-S3001" class="NLM_sec NLM_sec_level_3">
   <h4 class="section-heading-4" id="_i30">5.1.1. Similarities and differences</h4>
   <p>A linguistic category represents ‘the conceptualization of a collection of similar experiences thatare meaningful and relevant to us&nbsp;[<span class="ref-lnk lazy-ref"><a data-rid="CIT0087" data-reflink="_i80 _i82" href="#"><span class="off-screen">Citation</span>87</a></span>]'. The existence of such categories proves that humans have the ability to find similarities between objects. For instance, whether a car is manufactured by Toyota, Honda, or Renault, we can categorize it as a ‘car’ as long as it fits the conceptualized form of a ‘car'. However, if a vehicle accommodates a group of people, then we would think of it as a ‘bus'. This leads to the following questions: Can a robot categorize ‘similar’ items into a group? Can a robot draw a line between linguistic categories such as a ‘car’ and a ‘bus?’</p>
   <p>It should be noted here that the lines between categories are by no means definitive. While traditional semantics presupposes binary features (see [<span class="ref-lnk lazy-ref"><a data-rid="CIT0088" data-reflink="_i80 _i82" href="#"><span class="off-screen">Citation</span>88</a></span>] for an overview), current trends in cognitive semantics consider a prototype with both central and peripheral members in each category (cf. [<span class="ref-lnk lazy-ref"><a data-rid="CIT0089" data-reflink="_i80 _i82" href="#"><span class="off-screen">Citation</span>89</a></span>]).</p>
   <p>We should also point out that categories are related to language [<span class="ref-lnk lazy-ref"><a data-rid="CIT0090" data-reflink="_i80 _i82" href="#"><span class="off-screen">Citation</span>90</a></span>]. English speaking people distinguish a ‘bush', i.e. short tree, from a ‘tree’ in a daily context, but Japanese people usually do not. Conversely, there is only one word for ‘flatfish’ in English, whereas Japanese people delineate between two types of flatfish, namely ‘karei’ and ‘hirame'. Separate cultures and communities may consider distinct categories and concepts. This implies that there may even be separate categories for robots, as they have a completely different sensory system from human beings.</p>
  </div>
  <div id="S005-S2001-S3002" class="NLM_sec NLM_sec_level_3">
   <h4 class="section-heading-4" id="_i31">5.1.2. Taxonomy and partonomy</h4>
   <p>Linguistic categories often exhibit hierarchical relations [<span class="ref-lnk lazy-ref"><a data-rid="CIT0087" data-reflink="_i80 _i82" href="#"><span class="off-screen">Citation</span>87</a></span>]. This type of lexical relation is called taxonomy. For example, ‘cucumbers', ‘cabbages’, and ‘onions’ are considered members of the ‘vegetable’ category. Similarly, ‘dogs', ‘cats’, and ‘horses’ are grouped in the category of ‘animals', These superordinate terms are usually abstract notions; no specific entity is labeled as a ‘vegetable’ or an ‘animal'. This leads to another question: Can a robot form abstract categories on top of concrete groups?</p>
   <p>Another type of lexical relation is called partonomy, where one word denotes a part of another. Winston et&nbsp;al. [<span class="ref-lnk lazy-ref"><a data-rid="CIT0091" data-reflink="_i80 _i82" href="#"><span class="off-screen">Citation</span>91</a></span>] proposes six types of partonomic relations, including a component and integral object, e.g. a ‘cup’ and a ‘handle'; a member and a collection, e.g. a ‘tree’ and a ‘forest'; and a material and an object, e.g. ‘steel’ and a ‘bike'. This naturally leads to the following question: Can a robot identify components, or ingredients, of an object?</p>
  </div>
  <div id="S005-S2001-S3003" class="NLM_sec NLM_sec_level_3">
   <h4 class="section-heading-4" id="_i32">5.1.3. Semantic network via frame</h4>
   <p>Words have semantic relations in relation to a frame or scene [<span class="ref-lnk lazy-ref"><a data-rid="CIT0092" data-reflink="_i80 _i82" href="#"><span class="off-screen">Citation</span>92</a></span>,<span class="ref-lnk lazy-ref"><a data-rid="CIT0093" data-reflink="_i80 _i82" href="#"><span class="off-screen">Citation</span>93</a></span>]. For example, a ‘menu', ‘dish’, ‘knife’, and ‘fork’ are linked through a designated ‘restaurant’ frame, and a ‘plane', ‘train’, ‘hotel’, and ‘camera’ are linked through a ‘travel’ frame. The important distinction from other types of linguistic categories is that words can be categorized based on human activity and social customs. The link between ‘hotel’ and ‘camera’ is by no means linguistic but rather it is situational and even subjective. A question then arises as to whether a robot can understand these semantic networks.</p>
   <p>Note that a word can belong to several frames; ‘knife’ can be viewed as a member of the restaurant frame when used for eating, whereas it can also be a weapon and linked to words such as ‘sword’ and ‘arrow’ when used in the context of fighting.</p>
  </div>
  <div id="S005-S2001-S3004" class="NLM_sec NLM_sec_level_3">
   <h4 class="section-heading-4" id="_i33">5.1.4. Abstract concepts and ad-hoc categories</h4>
   <p>In many studies discussing concepts and categories, we tend to exhibit a nominal bias, i.e. we tend to think of nouns. Many concepts and categories corresponding to nouns, e.g. objects, places, and movements, are observable and a statistical categorization method can be a constructive model with a categorization and conceptualization capability. However, forming abstract concepts, e.g. ‘in', a preposition, ‘use', a verb, and ‘democracy', a conceptual word, requires other mechanisms and is important in many senses&nbsp;[<span class="ref-lnk lazy-ref"><a data-rid="CIT0094" data-reflink="_i80 _i82" href="#"><span class="off-screen">Citation</span>94</a></span>]. In daily language, we tend to use an abundance of abstract words. Therefore, enabling a robot to grasp abstract concepts is also important&nbsp;[<span class="ref-lnk lazy-ref"><a data-rid="CIT0095" data-reflink="_i80 _i82" href="#"><span class="off-screen">Citation</span>95</a></span>,<span class="ref-lnk lazy-ref"><a data-rid="CIT0096" data-reflink="_i80 _i82" href="#"><span class="off-screen">Citation</span>96</a></span>].</p>
   <p>Moreover, many categories are not static but dynamic. People can form so-called ad-hoc categories instantly based on the situations they face&nbsp;[<span class="ref-lnk lazy-ref"><a data-rid="CIT0097" data-reflink="_i80 _i82" href="#"><span class="off-screen">Citation</span>97</a></span>]. From this viewpoint, categorization may even be considered as a type of inference. How to model the learning capability for ad-hoc categories in the cognitive system of a robot is another question.</p>
  </div>
 </div>
 <div id="S005-S2002" class="NLM_sec NLM_sec_level_2">
  <h3 class="section-heading-3" id="_i34">5.2. Multimodal categorization</h3>
  <div id="S005-S2002-S3001" class="NLM_sec NLM_sec_level_3">
   <h4 class="section-heading-4" id="_i35">5.2.1. Concept formation by robots</h4>
   <p>To enable robots to implement concept formation, several studies assume that concepts are internal representations related to words or phrases that enable robots to infer categories into which they may classify sensory information. Regarding categorization, image classification using deep neural networks has been widely studied [<span class="ref-lnk lazy-ref"><a data-rid="CIT0098 CIT0099 CIT0100 CIT0101 CIT0102" data-reflink="_i80 _i82" href="#"><span class="off-screen">Citation</span>98–102</a></span>]. These studies use a large amount of labeled data and achieve very accurate object recognition. However, humans do not use such labeled data and it is important that the concepts are formed in an unsupervised manner. Studies on unsupervised image classification have also been conducted [<span class="ref-lnk lazy-ref"><a data-rid="CIT0103 CIT0104 CIT0105 CIT0106 CIT0107 CIT0108" data-reflink="_i80 _i82" href="#"><span class="off-screen">Citation</span>103–108</a></span>]. However, the importance of multimodality in concept formation has been recognized [<span class="ref-lnk lazy-ref"><a data-rid="CIT0109" data-reflink="_i80 _i82" href="#"><span class="off-screen">Citation</span>109</a></span>] and the difficulty in forming human-like concepts using single modality has been acknowledged. In studies using multimodal information, methods to learn relationships among modalities by using nonnegative matrix factorization and neural networks have been proposed [<span class="ref-lnk lazy-ref"><a data-rid="CIT0110 CIT0111 CIT0112 CIT0113 CIT0114 CIT0115 CIT0116 CIT0117" data-reflink="_i80 _i82" href="#"><span class="off-screen">Citation</span>110–117</a></span>]. In these studies, the learned latent space can be interpreted as representing concepts.</p>
   <p>Several methods have been proposed to classify multimodal information into categories in an unsupervised manner using stochastic models. Methods based on latent Dirichlet allocation (LDA) [<span class="ref-lnk lazy-ref"><a data-rid="CIT0118" data-reflink="_i80 _i82" href="#"><span class="off-screen">Citation</span>118</a></span>], initially proposed for unsupervised document classification and LDA, were extended to a multimodal LDA (MLDA) [<span class="ref-lnk lazy-ref"><a data-rid="CIT0119" data-reflink="_i80 _i82" href="#"><span class="off-screen">Citation</span>119</a></span>] for the classification of multimodal information. Here, category <i>z</i> is learned by classifying multimodal information <span class="NLM_disp-formula-image inline-formula rs_preserve">
     <noscript>
      <img src="/na101/home/literatum/publisher/tandf/journals/content/tadr20/2019/tadr20.v033.i15-16/01691864.2019.1632223/20210828/images/tadr_a_1632223_ilm0036.gif" alt="">
     </noscript><img src="//:0" alt="" class="mml-formula" data-formula-source="{&quot;type&quot; : &quot;image&quot;, &quot;src&quot; : &quot;/na101/home/literatum/publisher/tandf/journals/content/tadr20/2019/tadr20.v033.i15-16/01691864.2019.1632223/20210828/images/tadr_a_1632223_ilm0036.gif&quot;}"><span class="mml-formula"></span></span><span class="NLM_disp-formula inline-formula rs_preserve"><img src="//:0" alt="" data-formula-source="{&quot;type&quot; : &quot;mathjax&quot;}">
     <math>
      <msub>
       <mi mathvariant="bold-italic">
        w
       </mi>
       <mn>
        1
       </mn>
      </msub><mo>
       ,
      </mo><msub>
       <mi mathvariant="bold-italic">
        w
       </mi>
       <mn>
        2
       </mn>
      </msub><mo>
       ,
      </mo><mo>
       …
      </mo>
     </math></span> acquired through the sensors of a robot. The concepts are represented in a continuous space linked to a probability distribution <span class="NLM_disp-formula-image inline-formula rs_preserve">
     <noscript>
      <img src="/na101/home/literatum/publisher/tandf/journals/content/tadr20/2019/tadr20.v033.i15-16/01691864.2019.1632223/20210828/images/tadr_a_1632223_ilm0037.gif" alt="">
     </noscript><img src="//:0" alt="" class="mml-formula" data-formula-source="{&quot;type&quot; : &quot;image&quot;, &quot;src&quot; : &quot;/na101/home/literatum/publisher/tandf/journals/content/tadr20/2019/tadr20.v033.i15-16/01691864.2019.1632223/20210828/images/tadr_a_1632223_ilm0037.gif&quot;}"><span class="mml-formula"></span></span><span class="NLM_disp-formula inline-formula rs_preserve"><img src="//:0" alt="" data-formula-source="{&quot;type&quot; : &quot;mathjax&quot;}">
     <math>
      <mi>
       P
      </mi><mo>
       (
      </mo><mi>
       z
      </mi><mrow>
       <mo>
        |
       </mo>
      </mrow><msub>
       <mi mathvariant="bold-italic">
        w
       </mi>
       <mn>
        1
       </mn>
      </msub><mo>
       ,
      </mo><msub>
       <mi mathvariant="bold-italic">
        w
       </mi>
       <mn>
        2
       </mn>
      </msub><mo>
       ,
      </mo><mo>
       …
      </mo><mo>
       )
      </mo>
     </math></span>, and concept formation is equivalent to learning the parameters of this distribution. Using MLDA, the robot classified visual, haptic, and auditory information obtained by observing (Figure&nbsp;<a href="#F0002">2</a>(b)), grasping (Figure&nbsp;<a href="#F0002">2</a>(c)), and shaking (Figure&nbsp;<a href="#F0002">2</a>(c)) the objects shown in Figure&nbsp;<a href="#F0002">2</a>(a), and was able to form the basic-level categories shown in Figure&nbsp;<a href="#F0002">2</a>(a). The probability distribution <span class="NLM_disp-formula-image inline-formula rs_preserve">
     <noscript>
      <img src="/na101/home/literatum/publisher/tandf/journals/content/tadr20/2019/tadr20.v033.i15-16/01691864.2019.1632223/20210828/images/tadr_a_1632223_ilm0038.gif" alt="">
     </noscript><img src="//:0" alt="" class="mml-formula" data-formula-source="{&quot;type&quot; : &quot;image&quot;, &quot;src&quot; : &quot;/na101/home/literatum/publisher/tandf/journals/content/tadr20/2019/tadr20.v033.i15-16/01691864.2019.1632223/20210828/images/tadr_a_1632223_ilm0038.gif&quot;}"><span class="mml-formula"></span></span><span class="NLM_disp-formula inline-formula rs_preserve"><img src="//:0" alt="" data-formula-source="{&quot;type&quot; : &quot;mathjax&quot;}">
     <math>
      <mi>
       P
      </mi><mo>
       (
      </mo><msub>
       <mi mathvariant="bold-italic">
        w
       </mi>
       <mn>
        1
       </mn>
      </msub><mo>
       ,
      </mo><msub>
       <mi mathvariant="bold-italic">
        w
       </mi>
       <mn>
        2
       </mn>
      </msub><mo>
       ,
      </mo><mo>
       …
      </mo><mspace width="thinmathspace"></mspace><mrow>
       <mo>
        |
       </mo>
      </mrow><mspace width="thinmathspace"></mspace><mi>
       z
      </mi><mo>
       )
      </mo>
     </math></span> can be computed using learned parameters and the multimodal information that maximizes this probability is considered to represent the prototype of the category: <span class="NLM_disp-formula-image inline-formula rs_preserve">
     <noscript>
      <img src="/na101/home/literatum/publisher/tandf/journals/content/tadr20/2019/tadr20.v033.i15-16/01691864.2019.1632223/20210828/images/tadr_a_1632223_ilm0039.gif" alt="">
     </noscript><img src="//:0" alt="" class="mml-formula" data-formula-source="{&quot;type&quot; : &quot;image&quot;, &quot;src&quot; : &quot;/na101/home/literatum/publisher/tandf/journals/content/tadr20/2019/tadr20.v033.i15-16/01691864.2019.1632223/20210828/images/tadr_a_1632223_ilm0039.gif&quot;}"><span class="mml-formula"></span></span><span class="NLM_disp-formula inline-formula rs_preserve"><img src="//:0" alt="" data-formula-source="{&quot;type&quot; : &quot;mathjax&quot;}">
     <math>
      <msub>
       <mrow>
        <mover>
         <mi mathvariant="bold-italic">
          w
         </mi>
         <mo>
          ¯
         </mo>
        </mover>
       </mrow>
       <mn>
        1
       </mn>
      </msub><mo>
       ,
      </mo><msub>
       <mrow>
        <mover>
         <mi mathvariant="bold-italic">
          w
         </mi>
         <mo>
          ¯
         </mo>
        </mover>
       </mrow>
       <mn>
        2
       </mn>
      </msub><mo>
       ,
      </mo><mo>
       …
      </mo><mo>
       =
      </mo><mrow>
       <mi mathvariant="normal">
        a
       </mi>
       <mi mathvariant="normal">
        r
       </mi>
       <mi mathvariant="normal">
        g
       </mi>
       <mi mathvariant="normal">
        m
       </mi>
       <mi mathvariant="normal">
        a
       </mi>
       <mi mathvariant="normal">
        x
       </mi>
      </mrow><mi>
       p
      </mi><mo>
       (
      </mo><msub>
       <mi mathvariant="bold-italic">
        w
       </mi>
       <mn>
        1
       </mn>
      </msub><mo>
       ,
      </mo><msub>
       <mi mathvariant="bold-italic">
        w
       </mi>
       <mn>
        2
       </mn>
      </msub><mo>
       ,
      </mo><mo>
       …
      </mo><mspace width="thinmathspace"></mspace><mrow>
       <mo>
        |
       </mo>
      </mrow><mspace width="thinmathspace"></mspace><mi>
       z
      </mi><mo>
       )
      </mo>
     </math></span> Because this probability distribution is continuous, this concept model can represent not only a central member but also a peripheral member.</p>
   <div class="figure figureViewer" id="F0002">
    <div class="hidden figureViewerArticleInfo">
     <span class="figViewerTitle">Survey on frontiers of language and robotics</span>
     <div class="articleAuthors articleInfoSection">
      <div class="authorsHeading">
       All authors
      </div>
      <div class="authors">
       <a class="entryAuthor" href="/action/doSearch?Contrib=Taniguchi%2C+T"><span class="hlFld-ContribAuthor"></span></a><a href="/author/Taniguchi%2C+T"><span class="NLM_given-names">T.</span> Taniguchi</a>, <a class="entryAuthor" href="/action/doSearch?Contrib=Mochihashi%2C+D"><span class="hlFld-ContribAuthor"></span></a><a href="/author/Mochihashi%2C+D"><span class="NLM_given-names">D.</span> Mochihashi</a>, <a class="entryAuthor" href="/action/doSearch?Contrib=Nagai%2C+T"><span class="hlFld-ContribAuthor"></span></a><a href="/author/Nagai%2C+T"><span class="NLM_given-names">T.</span> Nagai</a>, <a class="entryAuthor" href="/action/doSearch?Contrib=Uchida%2C+S"><span class="hlFld-ContribAuthor"></span></a><a href="/author/Uchida%2C+S"><span class="NLM_given-names">S.</span> Uchida</a>, <a class="entryAuthor" href="/action/doSearch?Contrib=Inoue%2C+N"><span class="hlFld-ContribAuthor"></span></a><a href="/author/Inoue%2C+N"><span class="NLM_given-names">N.</span> Inoue</a>, <a class="entryAuthor" href="/action/doSearch?Contrib=Kobayashi%2C+I"><span class="hlFld-ContribAuthor"></span></a><a href="/author/Kobayashi%2C+I"><span class="NLM_given-names">I.</span> Kobayashi</a>, <a class="entryAuthor" href="/action/doSearch?Contrib=Nakamura%2C+T"><span class="hlFld-ContribAuthor"></span></a><a href="/author/Nakamura%2C+T"><span class="NLM_given-names">T.</span> Nakamura</a>, <a class="entryAuthor" href="/action/doSearch?Contrib=Hagiwara%2C+Y"><span class="hlFld-ContribAuthor"></span></a><a href="/author/Hagiwara%2C+Y"><span class="NLM_given-names">Y.</span> Hagiwara</a>, <a class="entryAuthor" href="/action/doSearch?Contrib=Iwahashi%2C+N"><span class="hlFld-ContribAuthor"></span></a><a href="/author/Iwahashi%2C+N"><span class="NLM_given-names">N.</span> Iwahashi</a> &amp; <a class="entryAuthor" href="/action/doSearch?Contrib=Inamura%2C+T"><span class="hlFld-ContribAuthor"></span></a><a href="/author/Inamura%2C+T"><span class="NLM_given-names">T.</span> Inamura</a>
      </div>
     </div>
     <div class="articleLowerInfo articleInfoSection">
      <div class="articleLowerInfoSection articleInfoDOI">
       <a href="https://doi.org/10.1080/01691864.2019.1632223">https://doi.org/10.1080/01691864.2019.1632223</a>
      </div>
      <div class="articleInfoPublicationDate articleLowerInfoSection border">
       <h6>Published online:</h6>24 June 2019
      </div>
     </div>
    </div>
    <div class="figureThumbnailContainer">
     <div class="figureInfo">
      <div class="short-legend">
       <p class="captionText"><span class="captionLabel">Figure 2. </span> Object concept formation in a robot: (a) objects used in the experiment, and obtaining visual, haptic, and auditory information by (b) observing, (c) grasping, and (d) shaking objects.</p>
      </div>
     </div><a href="#" class="thumbnail" aria-label="thumbnail image"><img id="F0002image" src="/na101/home/literatum/publisher/tandf/journals/content/tadr20/2019/tadr20.v033.i15-16/01691864.2019.1632223/20210828/images/medium/tadr_a_1632223_f0002_oc.jpg" loading="lazy" height="304" width="500"></a>
     <div class="figureDownloadOptions">
      <a href="#" class="downloadBtn btn btn-sm" role="button">Display full size</a>
     </div>
    </div>
   </div>
   <div class="hidden rs_skip" id="fig-description-F0002">
    <p class="captionText"><span class="captionLabel">Figure 2. </span> Object concept formation in a robot: (a) objects used in the experiment, and obtaining visual, haptic, and auditory information by (b) observing, (c) grasping, and (d) shaking objects.</p>
   </div>
   <div class="hidden rs_skip" id="figureFootNote-F0002">
    <div class="figureFootNote-F0002"></div>
   </div>
   <p></p>
   <p>Other nominal concept formation methods have also been developed. For example, locational, or spatial, concept formation methods have been proposed Taniguchi et&nbsp;al.&nbsp;[<span class="ref-lnk lazy-ref"><a data-rid="CIT0120" data-reflink="_i80 _i82" href="#"><span class="off-screen">Citation</span>120</a></span>,<span class="ref-lnk lazy-ref"><a data-rid="CIT0121" data-reflink="_i80 _i82" href="#"><span class="off-screen">Citation</span>121</a></span>].</p>
  </div>
  <div id="S005-S2002-S3002" class="NLM_sec NLM_sec_level_3">
   <h4 class="section-heading-4" id="_i37">5.2.2. Word meaning acquisition</h4>
   <p>The benefit of using multimodal information is that it makes it possible to infer abstract information from observations. For example, in neural network studies, relationships between modalities are learned, and they can therefore be inferred, to an extent, from other information. In MLDA, this inference is also possible by computing <span class="NLM_disp-formula-image inline-formula rs_preserve">
     <noscript>
      <img src="/na101/home/literatum/publisher/tandf/journals/content/tadr20/2019/tadr20.v033.i15-16/01691864.2019.1632223/20210828/images/tadr_a_1632223_ilm0040.gif" alt="">
     </noscript><img src="//:0" alt="" class="mml-formula" data-formula-source="{&quot;type&quot; : &quot;image&quot;, &quot;src&quot; : &quot;/na101/home/literatum/publisher/tandf/journals/content/tadr20/2019/tadr20.v033.i15-16/01691864.2019.1632223/20210828/images/tadr_a_1632223_ilm0040.gif&quot;}"><span class="mml-formula"></span></span><span class="NLM_disp-formula inline-formula rs_preserve"><img src="//:0" alt="" data-formula-source="{&quot;type&quot; : &quot;mathjax&quot;}">
     <math>
      <mi>
       P
      </mi><mo>
       (
      </mo><msub>
       <mi mathvariant="bold-italic">
        w
       </mi>
       <mn>
        1
       </mn>
      </msub><mrow>
       <mo>
        |
       </mo>
      </mrow><msub>
       <mi mathvariant="bold-italic">
        w
       </mi>
       <mn>
        2
       </mn>
      </msub><mo>
       ,
      </mo><msub>
       <mi mathvariant="bold-italic">
        w
       </mi>
       <mn>
        3
       </mn>
      </msub><mo>
       ,
      </mo><mo>
       …
      </mo><mo>
       )
      </mo>
     </math></span>, which is the probability that modal information <span class="NLM_disp-formula-image inline-formula rs_preserve">
     <noscript>
      <img src="/na101/home/literatum/publisher/tandf/journals/content/tadr20/2019/tadr20.v033.i15-16/01691864.2019.1632223/20210828/images/tadr_a_1632223_ilm0041.gif" alt="">
     </noscript><img src="//:0" alt="" class="mml-formula" data-formula-source="{&quot;type&quot; : &quot;image&quot;, &quot;src&quot; : &quot;/na101/home/literatum/publisher/tandf/journals/content/tadr20/2019/tadr20.v033.i15-16/01691864.2019.1632223/20210828/images/tadr_a_1632223_ilm0041.gif&quot;}"><span class="mml-formula"></span></span><span class="NLM_disp-formula inline-formula rs_preserve"><img src="//:0" alt="" data-formula-source="{&quot;type&quot; : &quot;mathjax&quot;}">
     <math>
      <msub>
       <mi mathvariant="bold-italic">
        w
       </mi>
       <mn>
        1
       </mn>
      </msub>
     </math></span> is derived from other information.</p>
   <p>Moreover, considering <span class="NLM_disp-formula-image inline-formula rs_preserve">
     <noscript>
      <img src="/na101/home/literatum/publisher/tandf/journals/content/tadr20/2019/tadr20.v033.i15-16/01691864.2019.1632223/20210828/images/tadr_a_1632223_ilm0042.gif" alt="">
     </noscript><img src="//:0" alt="" class="mml-formula" data-formula-source="{&quot;type&quot; : &quot;image&quot;, &quot;src&quot; : &quot;/na101/home/literatum/publisher/tandf/journals/content/tadr20/2019/tadr20.v033.i15-16/01691864.2019.1632223/20210828/images/tadr_a_1632223_ilm0042.gif&quot;}"><span class="mml-formula"></span></span><span class="NLM_disp-formula inline-formula rs_preserve"><img src="//:0" alt="" data-formula-source="{&quot;type&quot; : &quot;mathjax&quot;}">
     <math>
      <msub>
       <mi mathvariant="bold-italic">
        w
       </mi>
       <mn>
        1
       </mn>
      </msub>
     </math></span> as words that represent object features and are taught by humans, the robot can acquire word meanings [<span class="ref-lnk lazy-ref"><a data-rid="CIT0122" data-reflink="_i80 _i82" href="#"><span class="off-screen">Citation</span>122</a></span>]. The robot can recall the multimodal information <span class="NLM_disp-formula-image inline-formula rs_preserve">
     <noscript>
      <img src="/na101/home/literatum/publisher/tandf/journals/content/tadr20/2019/tadr20.v033.i15-16/01691864.2019.1632223/20210828/images/tadr_a_1632223_ilm0043.gif" alt="">
     </noscript><img src="//:0" alt="" class="mml-formula" data-formula-source="{&quot;type&quot; : &quot;image&quot;, &quot;src&quot; : &quot;/na101/home/literatum/publisher/tandf/journals/content/tadr20/2019/tadr20.v033.i15-16/01691864.2019.1632223/20210828/images/tadr_a_1632223_ilm0043.gif&quot;}"><span class="mml-formula"></span></span><span class="NLM_disp-formula inline-formula rs_preserve"><img src="//:0" alt="" data-formula-source="{&quot;type&quot; : &quot;mathjax&quot;}">
     <math>
      <msub>
       <mi mathvariant="bold-italic">
        w
       </mi>
       <mn>
        2
       </mn>
      </msub><mo>
       ,
      </mo><msub>
       <mi mathvariant="bold-italic">
        w
       </mi>
       <mn>
        3
       </mn>
      </msub><mo>
       ,
      </mo><mo>
       …
      </mo>
     </math></span> that can be represented by the word. The robot is considered to have understood word meanings through its own body. It has also been suggested that humans understand word meanings through their bodies [<span class="ref-lnk lazy-ref"><a data-rid="CIT0123" data-reflink="_i80 _i82" href="#"><span class="off-screen">Citation</span>123</a></span>,<span class="ref-lnk lazy-ref"><a data-rid="CIT0124" data-reflink="_i80 _i82" href="#"><span class="off-screen">Citation</span>124</a></span>] and MLDA partially implements this capability in robots.</p>
   <p>Furthermore, a stochastic model [<span class="ref-lnk lazy-ref"><a data-rid="CIT0125" data-reflink="_i80 _i82" href="#"><span class="off-screen">Citation</span>125</a></span>] enabling the robot to learn the parameters of a language model in the speech recognizer was proposed by introducing a nested Pitman–Yor language model [<span class="ref-lnk lazy-ref"><a data-rid="CIT0126" data-reflink="_i80 _i82" href="#"><span class="off-screen">Citation</span>126</a></span>] into MLDA. Using this model, the robot can form an object concept and learn speech recognition similarly to infants by using multimodal information obtained from objects and teaching utterances given by humans. Moreover, by connecting the recognized words and concepts, the robot can also acquire word meanings. The robot on which this model is installed (Figure&nbsp;<a href="#F0003">3</a>(b)) obtained multimodal information from the objects shown in Figure&nbsp;<a href="#F0003">3</a>(a); meanwhile, a human user taught object features to the robot. Finally, the robot was able to recognize unseen objects with an 86% accuracy and teaching utterances with a 72% accuracy.</p>
   <div class="figure figureViewer" id="F0003">
    <div class="hidden figureViewerArticleInfo">
     <span class="figViewerTitle">Survey on frontiers of language and robotics</span>
     <div class="articleAuthors articleInfoSection">
      <div class="authorsHeading">
       All authors
      </div>
      <div class="authors">
       <a class="entryAuthor" href="/action/doSearch?Contrib=Taniguchi%2C+T"><span class="hlFld-ContribAuthor"></span></a><a href="/author/Taniguchi%2C+T"><span class="NLM_given-names">T.</span> Taniguchi</a>, <a class="entryAuthor" href="/action/doSearch?Contrib=Mochihashi%2C+D"><span class="hlFld-ContribAuthor"></span></a><a href="/author/Mochihashi%2C+D"><span class="NLM_given-names">D.</span> Mochihashi</a>, <a class="entryAuthor" href="/action/doSearch?Contrib=Nagai%2C+T"><span class="hlFld-ContribAuthor"></span></a><a href="/author/Nagai%2C+T"><span class="NLM_given-names">T.</span> Nagai</a>, <a class="entryAuthor" href="/action/doSearch?Contrib=Uchida%2C+S"><span class="hlFld-ContribAuthor"></span></a><a href="/author/Uchida%2C+S"><span class="NLM_given-names">S.</span> Uchida</a>, <a class="entryAuthor" href="/action/doSearch?Contrib=Inoue%2C+N"><span class="hlFld-ContribAuthor"></span></a><a href="/author/Inoue%2C+N"><span class="NLM_given-names">N.</span> Inoue</a>, <a class="entryAuthor" href="/action/doSearch?Contrib=Kobayashi%2C+I"><span class="hlFld-ContribAuthor"></span></a><a href="/author/Kobayashi%2C+I"><span class="NLM_given-names">I.</span> Kobayashi</a>, <a class="entryAuthor" href="/action/doSearch?Contrib=Nakamura%2C+T"><span class="hlFld-ContribAuthor"></span></a><a href="/author/Nakamura%2C+T"><span class="NLM_given-names">T.</span> Nakamura</a>, <a class="entryAuthor" href="/action/doSearch?Contrib=Hagiwara%2C+Y"><span class="hlFld-ContribAuthor"></span></a><a href="/author/Hagiwara%2C+Y"><span class="NLM_given-names">Y.</span> Hagiwara</a>, <a class="entryAuthor" href="/action/doSearch?Contrib=Iwahashi%2C+N"><span class="hlFld-ContribAuthor"></span></a><a href="/author/Iwahashi%2C+N"><span class="NLM_given-names">N.</span> Iwahashi</a> &amp; <a class="entryAuthor" href="/action/doSearch?Contrib=Inamura%2C+T"><span class="hlFld-ContribAuthor"></span></a><a href="/author/Inamura%2C+T"><span class="NLM_given-names">T.</span> Inamura</a>
      </div>
     </div>
     <div class="articleLowerInfo articleInfoSection">
      <div class="articleLowerInfoSection articleInfoDOI">
       <a href="https://doi.org/10.1080/01691864.2019.1632223">https://doi.org/10.1080/01691864.2019.1632223</a>
      </div>
      <div class="articleInfoPublicationDate articleLowerInfoSection border">
       <h6>Published online:</h6>24 June 2019
      </div>
     </div>
    </div>
    <div class="figureThumbnailContainer">
     <div class="figureInfo">
      <div class="short-legend">
       <p class="captionText"><span class="captionLabel">Figure 3. </span> Learning concepts and language model in a robot: (a) objects and (b) robot used in the experiment.</p>
      </div>
     </div><a href="#" class="thumbnail" aria-label="thumbnail image"><img id="F0003image" src="/na101/home/literatum/publisher/tandf/journals/content/tadr20/2019/tadr20.v033.i15-16/01691864.2019.1632223/20210828/images/medium/tadr_a_1632223_f0003_oc.jpg" loading="lazy" height="283" width="500"></a>
     <div class="figureDownloadOptions">
      <a href="#" class="downloadBtn btn btn-sm" role="button">Display full size</a>
     </div>
    </div>
   </div>
   <div class="hidden rs_skip" id="fig-description-F0003">
    <p class="captionText"><span class="captionLabel">Figure 3. </span> Learning concepts and language model in a robot: (a) objects and (b) robot used in the experiment.</p>
   </div>
   <div class="hidden rs_skip" id="figureFootNote-F0003">
    <div class="figureFootNote-F0003"></div>
   </div>
   <p></p>
  </div>
  <div id="S005-S2002-S3003" class="NLM_sec NLM_sec_level_3">
   <h4 class="section-heading-4" id="_i39">5.2.3. Hierarchical concept formation</h4>
   <p>Concepts have a hierarchical structure and hence studies on the hierarchical classification of images using labeled data have been conducted [<span class="ref-lnk lazy-ref"><a data-rid="CIT0127" data-reflink="_i80 _i82" href="#"><span class="off-screen">Citation</span>127</a></span>,<span class="ref-lnk lazy-ref"><a data-rid="CIT0128" data-reflink="_i80 _i82" href="#"><span class="off-screen">Citation</span>128</a></span>]. However, as we mentioned earlier, learning concepts using multimodal information in an unsupervised manner is important. To implement such a hierarchical concept, a nested Chinese restaurant process&nbsp;[<span class="ref-lnk lazy-ref"><a data-rid="CIT0129" data-reflink="_i80 _i82" href="#"><span class="off-screen">Citation</span>129</a></span>] was introduced into MLDA&nbsp;[<span class="ref-lnk lazy-ref"><a data-rid="CIT0130" data-reflink="_i80 _i82" href="#"><span class="off-screen">Citation</span>130</a></span>]. Using multimodal information obtained by observing, grasping, and shaking objects as well as the studies discussed in the previous section, concepts were classified into categories and hierarchical relationships were estimated. As a result, the hierarchical structure shown in Figure&nbsp;<a href="#F0004">4</a> was estimated by the robot, and one can see that hierarchical relationships based on feature similarities are captured. Applying this model to localization in the concept formation problem, Hagiwara et&nbsp;al. proposed a hierarchical spatial formation method&nbsp;[<span class="ref-lnk lazy-ref"><a data-rid="CIT0131" data-reflink="_i80 _i82" href="#"><span class="off-screen">Citation</span>131</a></span>].</p>
   <div class="figure figureViewer" id="F0004">
    <div class="hidden figureViewerArticleInfo">
     <span class="figViewerTitle">Survey on frontiers of language and robotics</span>
     <div class="articleAuthors articleInfoSection">
      <div class="authorsHeading">
       All authors
      </div>
      <div class="authors">
       <a class="entryAuthor" href="/action/doSearch?Contrib=Taniguchi%2C+T"><span class="hlFld-ContribAuthor"></span></a><a href="/author/Taniguchi%2C+T"><span class="NLM_given-names">T.</span> Taniguchi</a>, <a class="entryAuthor" href="/action/doSearch?Contrib=Mochihashi%2C+D"><span class="hlFld-ContribAuthor"></span></a><a href="/author/Mochihashi%2C+D"><span class="NLM_given-names">D.</span> Mochihashi</a>, <a class="entryAuthor" href="/action/doSearch?Contrib=Nagai%2C+T"><span class="hlFld-ContribAuthor"></span></a><a href="/author/Nagai%2C+T"><span class="NLM_given-names">T.</span> Nagai</a>, <a class="entryAuthor" href="/action/doSearch?Contrib=Uchida%2C+S"><span class="hlFld-ContribAuthor"></span></a><a href="/author/Uchida%2C+S"><span class="NLM_given-names">S.</span> Uchida</a>, <a class="entryAuthor" href="/action/doSearch?Contrib=Inoue%2C+N"><span class="hlFld-ContribAuthor"></span></a><a href="/author/Inoue%2C+N"><span class="NLM_given-names">N.</span> Inoue</a>, <a class="entryAuthor" href="/action/doSearch?Contrib=Kobayashi%2C+I"><span class="hlFld-ContribAuthor"></span></a><a href="/author/Kobayashi%2C+I"><span class="NLM_given-names">I.</span> Kobayashi</a>, <a class="entryAuthor" href="/action/doSearch?Contrib=Nakamura%2C+T"><span class="hlFld-ContribAuthor"></span></a><a href="/author/Nakamura%2C+T"><span class="NLM_given-names">T.</span> Nakamura</a>, <a class="entryAuthor" href="/action/doSearch?Contrib=Hagiwara%2C+Y"><span class="hlFld-ContribAuthor"></span></a><a href="/author/Hagiwara%2C+Y"><span class="NLM_given-names">Y.</span> Hagiwara</a>, <a class="entryAuthor" href="/action/doSearch?Contrib=Iwahashi%2C+N"><span class="hlFld-ContribAuthor"></span></a><a href="/author/Iwahashi%2C+N"><span class="NLM_given-names">N.</span> Iwahashi</a> &amp; <a class="entryAuthor" href="/action/doSearch?Contrib=Inamura%2C+T"><span class="hlFld-ContribAuthor"></span></a><a href="/author/Inamura%2C+T"><span class="NLM_given-names">T.</span> Inamura</a>
      </div>
     </div>
     <div class="articleLowerInfo articleInfoSection">
      <div class="articleLowerInfoSection articleInfoDOI">
       <a href="https://doi.org/10.1080/01691864.2019.1632223">https://doi.org/10.1080/01691864.2019.1632223</a>
      </div>
      <div class="articleInfoPublicationDate articleLowerInfoSection border">
       <h6>Published online:</h6>24 June 2019
      </div>
     </div>
    </div>
    <div class="figureThumbnailContainer">
     <div class="figureInfo">
      <div class="short-legend">
       <p class="captionText"><span class="captionLabel">Figure 4. </span> Hierarchical concepts formed by robot.</p>
      </div>
     </div><a href="#" class="thumbnail" aria-label="thumbnail image"><img id="F0004image" src="/na101/home/literatum/publisher/tandf/journals/content/tadr20/2019/tadr20.v033.i15-16/01691864.2019.1632223/20210828/images/medium/tadr_a_1632223_f0004_oc.jpg" loading="lazy" height="222" width="500"></a>
     <div class="figureDownloadOptions">
      <a href="#" class="downloadBtn btn btn-sm" role="button">Display full size</a>
     </div>
    </div>
   </div>
   <div class="hidden rs_skip" id="fig-description-F0004">
    <p class="captionText"><span class="captionLabel">Figure 4. </span> Hierarchical concepts formed by robot.</p>
   </div>
   <div class="hidden rs_skip" id="figureFootNote-F0004">
    <div class="figureFootNote-F0004"></div>
   </div>
   <p></p>
   <p>Moreover, studies to detect parts of objects [<span class="ref-lnk lazy-ref"><a data-rid="CIT0132" data-reflink="_i80 _i82" href="#"><span class="off-screen">Citation</span>132</a></span>,<span class="ref-lnk lazy-ref"><a data-rid="CIT0133" data-reflink="_i80 _i82" href="#"><span class="off-screen">Citation</span>133</a></span>] and faces [<span class="ref-lnk lazy-ref"><a data-rid="CIT0134 CIT0135 CIT0136" data-reflink="_i80 _i82" href="#"><span class="off-screen">Citation</span>134–136</a></span>] are conducted regarding partonomy. However, in these studies, supervised learning based on visual information is utilized, and, currently, unsupervised learning based on multimodal information has not yet been implemented. A future challenge for robots is to learn partonomy using multimodal information in an unsupervised manner.</p>
  </div>
  <div id="S005-S2002-S3004" class="NLM_sec NLM_sec_level_3">
   <h4 class="section-heading-4" id="_i41">5.2.4. Integrated concept</h4>
   <p>Machine learning methods for forming various concepts and, furthermore, for learning relationships between them has been proposed. Moreover, as the transition in these concepts can be viewed as grammar, the proposed method enables robots to learn grammar using a bottom-up approach [<span class="ref-lnk lazy-ref"><a data-rid="CIT0085" data-reflink="_i80 _i82" href="#"><span class="off-screen">Citation</span>85</a></span>]. The multimodal information obtained from scenes where individuals manipulate objects is classified by MLDAs and the individual, object, motion, and localization concepts are formed. Furthermore, another MLDA is placed on top of these MLDAs to learn the relationships between concepts. Using this model, for example, the meaning of motion can change based on simultaneous observations of objects. We consider that a type of semantic network has been implemented. However, this is a very limited, and inflexible network with a structure that changes depending on the context, as explained in Section&nbsp;<a href="#S005-S2001-S3003">5.1.3</a>. Concepts change depending on the context, situation, and purpose, as seen in the ad-hoc category&nbsp;[<span class="ref-lnk lazy-ref"><a data-rid="CIT0097" data-reflink="_i80 _i82" href="#"><span class="off-screen">Citation</span>97</a></span>]. We need to change our perspective on concepts and no longer consider them as static objects, but rather as dynamic processes.</p>
  </div>
 </div>
 <div id="S005-S2003" class="NLM_sec NLM_sec_level_2">
  <h3 class="section-heading-3" id="_i42">5.3. Challenges in category and concept formation</h3>
  <p>So far, we have shown that significant progress in category and concept formation is being made in current robotics. However, previous studies have mainly focused on nominal categories and concepts, e.g. objects, location, and movement, which are concrete and observable. Therefore, some of the remaining challenges are as follows:</p>
  <p></p>
  <ul class="NLM_list NLM_list-list_type-bullet">
   <li><p class="inline">Inventing a mechanism for representing abstract concepts including emotions, e.g. anger, happiness, and sadness, and social concepts, e.g. democracy, freedom, and society.</p></li>
   <li><p class="inline">Inventing unsupervised machine learning methods to represent verbs, e.g. grasp, throw, and kick, and functional words, e.g. in, on, and over, from sensor–motor information.</p></li>
   <li><p class="inline">Identifying and implementing the process of categorical extension, especially for polysemous words, e.g. gift as a present and gift as a talent.</p></li>
   <li><p class="inline">Enabling a robot to spontaneously form ad-hoc categories to achieve a given goal within a certain context&nbsp;[<span class="ref-lnk lazy-ref"><a data-rid="CIT0097" data-reflink="_i80 _i82" href="#"><span class="off-screen">Citation</span>97</a></span>].</p></li>
   <li><p class="inline">Inventing a model for the distributed representation of concepts that can be used for logical inference, as discussed in Section&nbsp;<a href="#S003">3</a>.</p></li>
  </ul>
  <p></p>
 </div>
</div>
<div id="S006" class="NLM_sec NLM_sec_level_1">
 <h2 id="_i43" class="section-heading-2">6. Metaphorical expressions</h2>
 <p>Daily conversations, which a future service robot is expected to face, are full of metaphorical expressions. In Figure&nbsp;<a href="#F0001">1</a>, ‘I'm filled with joy’ embeds a metaphor in which emotion is compared to a liquid. Even though metaphors play a crucial role in semantics, very few related studies have been performed in robotics.</p>
 <div id="S006-S2001" class="NLM_sec NLM_sec_level_2">
  <h3 class="section-heading-3" id="_i44">6.1. Metaphor as a cognitive process</h3>
  <p>Before the advent of cognitive linguistics, metaphors were only viewed as linguistic ornaments, outside of the main scope of linguistic studies. However, since Lakoff and Johnson [<span class="ref-lnk lazy-ref"><a data-rid="CIT0137" data-reflink="_i80 _i82" href="#"><span class="off-screen">Citation</span>137</a></span>], metaphors have been regarded as one of the important linguistic phenomena that reflect our way of thinking. They claim that, by analyzing language, we can find conceptual metaphors that reside at the core of the human conceptual system.</p>
  <p>Conceptual metaphors are pervasive in language. As such, we are rarely conscious of their existence, but metaphors are the instruments that enable us to form abstract notions. Using metaphors, we understand and experience abstract concepts in the context of others. For example, when people are happy, they might say ‘I'm filled with joy.’ This sentence seems natural enough and may not sound metaphorical, but the subject for the verb ‘fill’ in the literal sense is usually liquid, whereas ‘joy’ is not. Here, we clearly understand ‘joy', as the target domain, which is invisible and intangible, in terms of a ‘liquid', as the source domain, and this expression is underpinned by a conceptual metaphor ‘emotions are liquids.’ The existence and pervasiveness of this metaphor is proven by its effectiveness in expressions such as ‘she is overflowing with love’ and ‘my anger is welling up.’ Thus, human beings are capable of metaphorical understanding, among other basic cognitive abilities. It should also be noted that the conventionality of this metaphor has led to the categorical extension of the verb ‘fill’ and dictionaries have an entry for the meaning of ‘be filled with emotions.’ If robots are to simulate the human cognitive process, then metaphors may play a vital role not only in terms of the cognitive ability to understand abstract notions, but also as a device for extending linguistic categories (see Section&nbsp;<a href="#S005-S2001">5.1</a>). It seems, however, that almost no attempts have been made in this respect in the field of robotics, and hence the mechanism supporting metaphorical understanding and extension requires further research.</p>
 </div>
 <div id="S006-S2002" class="NLM_sec NLM_sec_level_2">
  <h3 class="section-heading-3" id="_i45">6.2. Metaphor and embodied experiences</h3>
  <p>The process of metaphorical understanding involves two domains. One is the target domain, which is usually an abstract notion. The other is the source domain, which is concrete and a concept that we can observe or experience. The properties of the source domain are mapped onto the target domain and linguistic expressions accordingly. Fauconnier and Turner [<span class="ref-lnk lazy-ref"><a data-rid="CIT0138" data-reflink="_i80 _i82" href="#"><span class="off-screen">Citation</span>138</a></span>] provides an actual model of conceptual blending. Some researchers in the field of cognitive linguistics and cognitive science argue that bodily experiences are embedded in the source domain [<span class="ref-lnk lazy-ref"><a data-rid="CIT0023 CIT0024 CIT0025" data-reflink="_i80 _i82" href="#"><span class="off-screen">Citation</span>23–25</a></span>] claims that ‘abstract thoughts grow out of concrete embodied experiences, typically sensory–motor experiences.’ For example, in the ‘purposes (target domain) are destinations (source domain)’ metaphor that can be found in a sentence such as ‘he'll ultimately be successful, but he isn't there yet', the underlying sensory–motor experience, according to Feldman [<span class="ref-lnk lazy-ref"><a data-rid="CIT0025" data-reflink="_i80 _i82" href="#"><span class="off-screen">Citation</span>25</a></span>], is ‘reaching a destination', and thus we can easily understand the metaphorical expression based on our own physical experiences. In addition, Grady [<span class="ref-lnk lazy-ref"><a data-rid="CIT0139" data-reflink="_i80 _i82" href="#"><span class="off-screen">Citation</span>139</a></span>] claims that seemingly abstract metaphors can be decomposed into more basic elements identified there as primary metaphors, such as ‘more is up’ and ‘affection is warmth', with an experiential basis or experiential co-occurrences.</p>
  <p>These findings have significant implications in robotics. If metaphors are grounded in embodied experiences, the body of a robot itself is an important interface for understanding the world. In other words, having a body is a considerable advantage for forming concepts as a robot. It also opens a way to connect concrete and abstract notions. Because metaphors may be used as devices for understanding abstract notions, robots may be able to understand abstract meaning based on embodied experiences, just as humans do. However, again, little research has been conducted on the topic, and hence whether a human-like body is advantageous to understanding abstract notions remains an open question.</p>
 </div>
 <div id="S006-S2003" class="NLM_sec NLM_sec_level_2">
  <h3 class="section-heading-3" id="_i46">6.3. Metaphor resources and universality</h3>
  <p>Several linguistic endeavors have aimed at identifying the various types of metaphors used in the human language. This effort ended up as an attempt to understand how we perceive and conceptualize the world, and identify the types of bodily experiences embedded using the inventory of basic primary metaphors. This included constructing dictionaries of metaphors [<span class="ref-lnk lazy-ref"><a data-rid="CIT0140 CIT0141 CIT0142" data-reflink="_i80 _i82" href="#"><span class="off-screen">Citation</span>140–142</a></span>]. Among them, Seto et&nbsp;al. [<span class="ref-lnk lazy-ref"><a data-rid="CIT0143" data-reflink="_i80 _i82" href="#"><span class="off-screen">Citation</span>143</a></span>] is particularly noteworthy in that it tries to uncover the polysemy of English words in terms of metaphors and metonymies. This has the potential to reveal how meaning emerges from very basic building blocks and thus provide a useful model for categorical extension. MetaNet [<span class="ref-lnk lazy-ref"><a data-rid="CIT0144" data-reflink="_i80 _i82" href="#"><span class="off-screen">Citation</span>144</a></span>] is an online resource for metaphors based on frame semantics [<span class="ref-lnk lazy-ref"><a data-rid="CIT0145" data-reflink="_i80 _i82" href="#"><span class="off-screen">Citation</span>145</a></span>]. The aim of the MetaNet project is to systematically analyze metaphors in a computational way; it now provides more than 600 conceptual metaphors, e.g. anger as a fire, happy as being up, and machines as people, with links to FrameNet frames [<span class="ref-lnk lazy-ref"><a data-rid="CIT0144" data-reflink="_i80 _i82" href="#"><span class="off-screen">Citation</span>144</a></span>]. There have also been some attempts to identify metaphors using linguistic resources [<span class="ref-lnk lazy-ref"><a data-rid="CIT0146" data-reflink="_i80 _i82" href="#"><span class="off-screen">Citation</span>146</a></span>], but the task was challenging, as metaphors are deeply interweaved into the language.</p>
  <p>Comparing resources across several languages (cf. [<span class="ref-lnk lazy-ref"><a data-rid="CIT0147" data-reflink="_i80 _i82" href="#"><span class="off-screen">Citation</span>147</a></span>]), it appears that many metaphors are surprisingly universal. Meanwhile, naturally, differences should be considered [<span class="ref-lnk lazy-ref"><a data-rid="CIT0148" data-reflink="_i80 _i82" href="#"><span class="off-screen">Citation</span>148</a></span>,<span class="ref-lnk lazy-ref"><a data-rid="CIT0149" data-reflink="_i80 _i82" href="#"><span class="off-screen">Citation</span>149</a></span>]. The universality of metaphors can be attributed to the universality of human bodily experiences, but robots may form completely different metaphors because of their physical differences. This may imply that having the body type of a human is one of the necessary conditions for robots to have a cognitive system similar to that of a human.</p>
 </div>
 <div id="S006-S2004" class="NLM_sec NLM_sec_level_2">
  <h3 class="section-heading-3" id="_i47">6.4. Challenges in metaphorical expressions</h3>
  <p>As pointed out above, there have been few endeavors in the field of robotics centered on metaphors and embodiment, despite their importance and the potential benefits. This could be due to the difficulty in implementing metaphorical thinking and the fact that robotics has not advanced enough to cope with metaphors. Nevertheless, we believe it is worth listing some of the challenges here, and we summarize them as follows:</p>
  <p></p>
  <ul class="NLM_list NLM_list-list_type-bullet">
   <li><p class="inline">Clarifying the computational process of categorical extension through metaphors, e.g. from ‘being physically drained’ to ‘emotionally drained.’</p></li>
   <li><p class="inline">Inventing a computational way to understand abstract concepts based on bodily experience, e.g. using an experience tied to ‘reaching somewhere’ to understand the concept of ‘accomplishing something’ and thus, expanding the meaning of ‘reach.’</p></li>
   <li><p class="inline">Inventing a computational mechanism for understanding creative uses of metaphors, e.g. ‘streaming is killing cable’ to mean ‘streaming is displacing cable.’</p></li>
  </ul>
  <p></p>
  <p>Metaphorical expressions are also one of the actively researched topics in cognitive linguistics and developmental psychology. Hence, new insights may come from related fields and interdisciplinary cooperation may be a key to unlocking new possibilities in the field.</p>
 </div>
</div>
<div id="S007" class="NLM_sec NLM_sec_level_1">
 <h2 id="_i48" class="section-heading-2">7. Affordance and action learning</h2>
 <p>Language and actions are deeply related to each other. We can talk about ourselves and the actions of others using sentences, understand language instructions, and act accordingly. From a neuroscientific perspective, the motor and auditory areas are connected to each other, enabling, for example, the autonomous activation of the corresponding motor signals when auditory stimuli tied to a specific word are provided&nbsp;[<span class="ref-lnk lazy-ref"><a data-rid="CIT0150" data-reflink="_i80 _i82" href="#"><span class="off-screen">Citation</span>150</a></span>,<span class="ref-lnk lazy-ref"><a data-rid="CIT0151" data-reflink="_i80 _i82" href="#"><span class="off-screen">Citation</span>151</a></span>]. This type of phenomenon may be useful to the understanding of the meaning of verbs. Furthermore, to understand the language, constraints imposed by the physical body and the concept of possible actions, which is related to the idea of affordance, play an important role&nbsp;[<span class="ref-lnk lazy-ref"><a data-rid="CIT0030" data-reflink="_i80 _i82" href="#"><span class="off-screen">Citation</span>30</a></span>]. Moreover, grammar learning is based on temporal information, namely the order of words, and it seems to be deeply related to an action-planning ability&nbsp;[<span class="ref-lnk lazy-ref"><a data-rid="CIT0152" data-reflink="_i80 _i82" href="#"><span class="off-screen">Citation</span>152</a></span>,<span class="ref-lnk lazy-ref"><a data-rid="CIT0153" data-reflink="_i80 _i82" href="#"><span class="off-screen">Citation</span>153</a></span>]. This section provides an overview of the research area of affordance and action learning, which are key to the understanding and generation of language based on embodiment.</p>
 <div id="S007-S2001" class="NLM_sec NLM_sec_level_2">
  <h3 class="section-heading-3" id="_i49">7.1. Affordance learning</h3>
  <div id="S007-S2001-S3001" class="NLM_sec NLM_sec_level_3">
   <h4 class="section-heading-4" id="_i50">7.1.1. Affordance and functions of objects</h4>
   <p>The concept of an object is not only determined by multimodal information that can be directly observed via sensor systems, such as perception, but also by the functions of an object. Multimodal categorization, which was introduced in Section&nbsp;5, is insufficient to explain object concept formation. Therefore, the functions of an object are important attributes of an object concept. The same can be said about robots&nbsp;[<span class="ref-lnk lazy-ref"><a data-rid="CIT0154 CIT0155 CIT0156" data-reflink="_i80 _i82" href="#"><span class="off-screen">Citation</span>154–156</a></span>]. For example, considering the identification of a chair, what is important to us is the ability to sit down on it, whereas its appearance is not essential. Of course, we do have knowledge of the appearance of chairs and their usual location; however, whether we can sit on them is ultimately the decisive factor. In particular, it is considered that a function of a tool is perceived by an action–effect relationship. Such action-oriented perception learning is often referred to as affordance learning in cognitive robotics [<span class="ref-lnk lazy-ref"><a data-rid="CIT0154 CIT0155 CIT0156" data-reflink="_i80 _i82" href="#"><span class="off-screen">Citation</span>154–156</a></span>].</p>
   <p>Affordance is a concept proposed by Gibson, a psychologist who promoted ecological psychology [<span class="ref-lnk lazy-ref"><a data-rid="CIT0157" data-reflink="_i80 _i82" href="#"><span class="off-screen">Citation</span>157</a></span>]. According to this concept, the meaning of the environment is not held in the human brain but instead exists as a set of possible actions by the human body. In other words, the meaning is naturally defined by the environment and the body facing the environment.</p>
   <p>By contrast, for an object (chair) recognition using a deep neural network, the situation is completely different. For identification using a neural network learning from a large number of chair images, appearance is essential. Of course, functionality could correlate with visual information, but this does not necessarily hold. Affordance is an important concept in the quest for human intelligence, and a perfect example of the fact that intelligence and the body cannot be separated. Hence, affordance is a key concept in intelligent robotics.</p>
  </div>
  <div id="S007-S2001-S3002" class="NLM_sec NLM_sec_level_3">
   <h4 class="section-heading-4" id="_i51">7.1.2. Affordance learning</h4>
   <p>In studies on affordance in robotics, researchers often focus on ways to improve the performance of a robot using the concept of affordance. The other important area of interest regarding affordance in robotics is the issue of how to make robots learn, i.e. acquire, affordance. Here, we introduce several studies from the perspective of learning affordance.</p>
   <p>The first work centered on the usage of affordance in navigation and obstacle avoidance for autonomous mobile robots. Sahin et&nbsp;al. proposed a model of affordance that links the motion of the robot to changes in the visual sensor [<span class="ref-lnk lazy-ref"><a data-rid="CIT0158" data-reflink="_i80 _i82" href="#"><span class="off-screen">Citation</span>158</a></span>]. With this model, the robot can take actions to avoid obstacles naturally.</p>
   <p>The second study contributed a learning tool to be used by robots. Stoytchev proposed a method that enables a robot to learn affordance in connection with a tool by gripping and moving a T-shaped tool in order to pick a target item [<span class="ref-lnk lazy-ref"><a data-rid="CIT0159" data-reflink="_i80 _i82" href="#"><span class="off-screen">Citation</span>159</a></span>,<span class="ref-lnk lazy-ref"><a data-rid="CIT0160" data-reflink="_i80 _i82" href="#"><span class="off-screen">Citation</span>160</a></span>]. This allowed robots to pick items with a high probability using the T-shaped tool.</p>
   <p>The third work provided an example of learning more complex tools. Nakamura et&nbsp;al. defined the change in the object affected by tools such as scissors and staples as a function of the tool [<span class="ref-lnk lazy-ref"><a data-rid="CIT0161" data-reflink="_i80 _i82" href="#"><span class="off-screen">Citation</span>161</a></span>]. Then, a method to learn the relationship between the local features of the tool and an action was proposed using Bayesian networks. Here, the robot could associate, for instance, the functionality of a tool with certain visual features and potential action. It should be noted that this affordance learning is deeply related to the multimodal categorization described in Section&nbsp;<a href="#S005">5</a>.</p>
   <p>Many of these studies demonstrate that robots can acquire affordance in practice and that robots make better decisions using affordance.</p>
  </div>
 </div>
 <div id="S007-S2002" class="NLM_sec NLM_sec_level_2">
  <h3 class="section-heading-3" id="_i52">7.2. Action learning</h3>
  <p>Action learning in robotics is often referred to as learning from demonstration (LfD) or programming by demonstration (PbD), which focuses on the issue of programming a robot motion [<span class="ref-lnk lazy-ref"><a data-rid="CIT0162 CIT0163 CIT0164" data-reflink="_i80 _i82" href="#"><span class="off-screen">Citation</span>162–164</a></span>]. Action leaning is important in language and robotics, because it enables robots to understand the meaning of verbs by acting in the real world. In other words, there is no point in using a robot in the first place unless it can learn actions and form concepts in the actual, physical world. A robot capable of learning the mapping between language and actions, leading to the essential understanding of verbs, becomes very useful in practice, as language instructions can then be used to direct work.</p>
  <div id="S007-S2002-S3001" class="NLM_sec NLM_sec_level_3">
   <h4 class="section-heading-4" id="_i53">7.2.1. Imitation learning</h4>
   <p>There are various types of action learning. In general, they can be divided roughly into two types according to the existence of supervision. In the presence of supervision, learning is sometimes referred to as imitation learning. In robotics, imitation learning often refers to the regeneration of the trajectories of the instructor, also known as LfD. In this case, the problem reduces to the modeling of the demonstrations, i.e. trajectories, of the instructor.</p>
   <p>It should be noted that mimicking the trajectories of an expert is not essential for action learning. An important aspect of an action is its function, and one must be able to reproduce this function inherently. However, considering imitation learning by children, it is initially difficult to notice and to imitate the functional aspects of the actions. Instead, by imitating the trajectory, children eventually uncover its functional underpinnings through their own actions. In other words, it appears that it is also key to simulate trajectories initially in imitation learning. The problem currently revolves around the concept of a unit of action. In other words, we must segment continuous actions into meaningful units. From the perspective of mapping to language, segmented actions lead to discrete symbols, which facilitate the connection between the action and a word. To categorize the segmented unit actions, a Gaussian process hidden semi-Markov model (GP-HSMM) was proposed in [<span class="ref-lnk lazy-ref"><a data-rid="CIT0165" data-reflink="_i80 _i82" href="#"><span class="off-screen">Citation</span>165</a></span>], with the ability to segment time series using the idea of state transitions in a hidden semi-Markov model (HSMM).</p>
   <p>The segmentation of continuous signals is also important in speech signal segmentation. Taniguchi et&nbsp;al. proposed the concept of a double articulation analyzer (DAA) in order to carry out the segmentation and categorization simultaneously based on the double articulation property of speech signals&nbsp;[<span class="ref-lnk lazy-ref"><a data-rid="CIT0166" data-reflink="_i80 _i82" href="#"><span class="off-screen">Citation</span>166</a></span>]. The DAA was implemented based on a hierarchical Bayesian formulation. The segmentation of actions, i.e. trajectories, and speech signals is highly significant in the relationship between actions and language&nbsp;[<span class="ref-lnk lazy-ref"><a data-rid="CIT0167" data-reflink="_i80 _i82" href="#"><span class="off-screen">Citation</span>167</a></span>]. Indeed, each segmented word, i.e. verb, may be connected to a corresponding discrete action, leading to a word acquisition process that is extremely easy to understand.</p>
   <p>In order to act based on the learned discrete actions, robots need to have a decision-making process. This is usually a matter of policy learning, and RL can be used for this. Learned discrete actions are selected based on the learned policy.</p>
   <p>There are other types of imitation learning. In fact, Schaal discussed imitation learning from the perspective of efficient motor learning, the connection between action and perception, and modular motor control in the form of movement primitives [<span class="ref-lnk lazy-ref"><a data-rid="CIT0168" data-reflink="_i80 _i82" href="#"><span class="off-screen">Citation</span>168</a></span>]. In [<span class="ref-lnk lazy-ref"><a data-rid="CIT0169" data-reflink="_i80 _i82" href="#"><span class="off-screen">Citation</span>169</a></span>], a recurrent neural network with parametric bias (RNNPB) model was used to enable the identification and imitation of motions by robots. These were pioneering works on sensor–motor learning based on imitation; however, language was not involved.</p>
   <p>Recent developments in deep learning technologies have opened a new direction in imitation learning. More specifically, generative adversarial imitation learning (GAIL) [<span class="ref-lnk lazy-ref"><a data-rid="CIT0170" data-reflink="_i80 _i82" href="#"><span class="off-screen">Citation</span>170</a></span>] has been proposed based on generative adversarial networks (GAN). This approach is made possible by the fact that the output of the discriminator network can be seen as a reward for the action imitated.</p>
   <p>Although language is not involved in GAIL, the idea has huge potential for action and language learning.</p>
   <p>From a language learning perspective, Hatori et&nbsp;al. [<span class="ref-lnk lazy-ref"><a data-rid="CIT0014" data-reflink="_i80 _i82" href="#"><span class="off-screen">Citation</span>14</a></span>] succeeded in building an interactive system in which the user can use unconstrained spoken language instructions to pick up a common object using an NLP technology based on deep learning. Actions are not learned but predefined in the study. However, the challenge will be to combine action learning and spoken language instruction understanding for picking tasks.</p>
  </div>
  <div id="S007-S2002-S3002" class="NLM_sec NLM_sec_level_3">
   <h4 class="section-heading-4" id="_i54">7.2.2. Reinforcement learning (RL)</h4>
   <p>RL can handle policy learning [<span class="ref-lnk lazy-ref"><a data-rid="CIT0171" data-reflink="_i80 _i82" href="#"><span class="off-screen">Citation</span>171</a></span>,<span class="ref-lnk lazy-ref"><a data-rid="CIT0172" data-reflink="_i80 _i82" href="#"><span class="off-screen">Citation</span>172</a></span>]. Thus, the problem mentioned above regarding the decision-making process can be solved by RL. RL techniques are roughly divided into two categories, the model-free and model-based methods. Q-learning is a well-known model-free RL method, which is suitable for discrete actions. Recent advances in deep learning have contributed to the improved performance of Q-learning using functional approximation. Deep Q-network (DQN) is the most famous method in this line of research [<span class="ref-lnk lazy-ref"><a data-rid="CIT0173" data-reflink="_i80 _i82" href="#"><span class="off-screen">Citation</span>173</a></span>]. Owing to the end-to-end nature of deep learning, policy learning with continuous actions generalizes action learning, i.e. the modeling and segmentation of trajectories, and RL can be combined naturally through end-to-end learning. Deep deterministic policy gradient (DDPG) [<span class="ref-lnk lazy-ref"><a data-rid="CIT0174" data-reflink="_i80 _i82" href="#"><span class="off-screen">Citation</span>174</a></span>] is one of the most frequently used algorithms for this purpose. In robotics, Levine developed a method that can be used to learn policies by mapping raw image observations directly to the torque of the motors of the robot [<span class="ref-lnk lazy-ref"><a data-rid="CIT0175" data-reflink="_i80 _i82" href="#"><span class="off-screen">Citation</span>175</a></span>]. The authors demonstrated that the joint end-to-end training of the perception and control systems achieves better performance than the separate training of each component. From the perspective of connecting actions and language, end-to-end learning does not provide an explicit structure. However, an interesting approach to language acquisition by a computer agent in a simulation environment has been proposed by DeepMind Lab using deep reinforcement learning [<span class="ref-lnk lazy-ref"><a data-rid="CIT0176" data-reflink="_i80 _i82" href="#"><span class="off-screen">Citation</span>176</a></span>]. The authors present an agent that learns to interpret language in a simulated 3D environment, where it is rewarded for the successful execution of written instructions. A combination of reinforcement and unsupervised learning enables the agent to learn to relate linguistic symbols to emergent perceptual representations of its physical surroundings and to pertinent sequences of actions. Although the actions contemplated in the study, such as pick-ups, are limited and relatively simple, the approach has potential for creating agents with a genuine understanding of language.</p>
   <p>By contrast, model-based RL tries to capture the dynamics of the environment [<span class="ref-lnk lazy-ref"><a data-rid="CIT0177" data-reflink="_i80 _i82" href="#"><span class="off-screen">Citation</span>177</a></span>]. Therefore, the agent can use the model to learn the policy. Although current performance is limited, model-based RL may potentially have advantages making it more applicable to complex tasks in the real world compared to model-free RL. From a language learning perspective, there have been few attempts to apply model-based RL to the language learning task, which could be a solution.</p>
  </div>
  <div id="S007-S2002-S3003" class="NLM_sec NLM_sec_level_3">
   <h4 class="section-heading-4" id="_i55">7.2.3. Syntax and actions</h4>
   <p>In generative grammar, the basic design of language capability in humans is assumed to comprise the following three modules: (1) the sensor–motor system, which is related to externalization such as utterance and gestures; (2) the conceptual and intentional system linked to the concept of semantics; and (3) the syntactic computational system, i.e. syntax, which connects the sensor–motor system to the conceptual and intentional system.</p>
   <p>From the perspective of evolutionary linguistics, it can be argued that the syntactic operation system precedes hierarchical and sequential object manipulation abilities (action grammar [<span class="ref-lnk lazy-ref"><a data-rid="CIT0178" data-reflink="_i80 _i82" href="#"><span class="off-screen">Citation</span>178</a></span>]), typically observed in the use and creation of tools. One must also insist that the conceptual and intentional interface and lexicon arise from this syntactic operation system [<span class="ref-lnk lazy-ref"><a data-rid="CIT0153" data-reflink="_i80 _i82" href="#"><span class="off-screen">Citation</span>153</a></span>,<span class="ref-lnk lazy-ref"><a data-rid="CIT0179" data-reflink="_i80 _i82" href="#"><span class="off-screen">Citation</span>179</a></span>]. In other words, the claim is that the syntactic operation system is derived from the sensor–motor system, i.e. syntactic bootstrapping&nbsp;[<span class="ref-lnk lazy-ref"><a data-rid="CIT0180" data-reflink="_i80 _i82" href="#"><span class="off-screen">Citation</span>180</a></span>,<span class="ref-lnk lazy-ref"><a data-rid="CIT0181" data-reflink="_i80 _i82" href="#"><span class="off-screen">Citation</span>181</a></span>]. These evolutionary linguistic viewpoints are very important for language and robotics, because affordance and action learning using robots is indispensable for acquiring language in a true sense.</p>
  </div>
 </div>
 <div id="S007-S2003" class="NLM_sec NLM_sec_level_2">
  <h3 class="section-heading-3" id="_i56">7.3. Challenges in affordance and action learning</h3>
  <p>In this section, we described several studies on affordance and action learning in robotics with a close connection to language learning. From the above evolutionary linguistic viewpoint, language, especially syntax, has a strong relationship with affordance and action planning. Unfortunately, there are few existing studies focusing on learning affordance, actions, and language using real robots. Therefore, a future challenge will be to study the links among affordance, actions, and language. However, there have certainly been some pioneering attempts. In [<span class="ref-lnk lazy-ref"><a data-rid="CIT0182" data-reflink="_i80 _i82" href="#"><span class="off-screen">Citation</span>182</a></span>], the authors proposed a bidirectional mapping between whole-body motion and language using deep recurrent networks. Yamada et&nbsp;al. proposed paired recurrent autoencoders, translating robot actions and language in a bidirectional way [<span class="ref-lnk lazy-ref"><a data-rid="CIT0183" data-reflink="_i80 _i82" href="#"><span class="off-screen">Citation</span>183</a></span>]. Although the network architectures are different, these recent studies both share the concept of end-to-end learning. These deep-learning-based studies yielded promising results; however, the end-to-end learning approach has a limitation, as it does necessarily clarify the structure and the relationship between language and actions, as we explained earlier. Moreover, affordance is not explicitly taken into consideration in these studies. Therefore, achieving a comprehensive understanding of the language system, taking affordance and action learning into account, is a major challenge.</p>
  <p>The challenges in this section can be summarized as follows:</p>
  <ul class="NLM_list NLM_list-list_type-bullet">
   <li><p class="inline">Developing a synchronous method for learning action and affordance for language use and understanding.</p></li>
   <li><p class="inline">Developing a computational model that forms a joint representation of action planning and syntax learning, i.e. a computational model realizing syntactic bootstrapping.</p></li>
   <li><p class="inline">Inventing an action learning method that leads to the emergence of the concept of a verb.</p></li>
  </ul>
  <p></p>
  <p>The use of robots with physical bodies is an indispensable step toward this goal.</p>
 </div>
</div>
<div id="S008" class="NLM_sec NLM_sec_level_1">
 <h2 id="_i57" class="section-heading-2">8. Pragmatics and social language</h2>
 <p>In daily conversations, spoken sentences do not always mean ‘what they literally mean.’ For example, in Figure&nbsp;<a href="#F0001">1</a>, a person says ‘I'm thirsty.’ His utterance is a type of request to the robot rather than a declaration of his appetitive state. In many cases, language use cannot be handled without pragmatics.</p>
 <p>There are three representative theories currently supported in pragmatics: (1) speech act theory [<span class="ref-lnk lazy-ref"><a data-rid="CIT0184" data-reflink="_i80 _i82" href="#"><span class="off-screen">Citation</span>184</a></span>], (2) a theory of implicature by Grices [<span class="ref-lnk lazy-ref"><a data-rid="CIT0185" data-reflink="_i80 _i82" href="#"><span class="off-screen">Citation</span>185</a></span>], and (3) relevance theory [<span class="ref-lnk lazy-ref"><a data-rid="CIT0186" data-reflink="_i80 _i82" href="#"><span class="off-screen">Citation</span>186</a></span>]. These theories have provided many reasonable explanations, analyses, suggestions, and implications regarding language use, and have had a great influence on several academic disciplines. However, pragmatics and the social aspect of language have rarely been taken into consideration in robotics.</p>
 <p>These theories tend to analyze the phenomena of language use based on reductionism, and have not yet been successful in dealing with holistic properties linked to the interdependency between foreground spoken stimuli and background beliefs. Explaining and analyzing the phenomenon in terms of language alone may lead to a limitation. Another issue may be that these approaches manipulate languages, but do not have any consideration of the body, nor its surroundings.</p>
 <p>In addition, in terms of the social aspect of language, meaning is determined by use in a social context; here, context is linked to the culture and situation of a specific social group.</p>
 <p>Considering these issues, it is important to equip the robot with knowledge of pragmatics and a sense for the use of language in a social context.</p>
 <div id="S008-S2001" class="NLM_sec NLM_sec_level_2">
  <h3 class="section-heading-3" id="_i58">8.1. Pragmatics</h3>
  <div id="S008-S2001-S3001" class="NLM_sec NLM_sec_level_3">
   <h4 class="section-heading-4" id="_i59">8.1.1. Pragmatics in AI</h4>
   <p>In terms of AI research, several studies have been conducted on pragmatics. SHRDLU was primarily a language parser that allowed user interaction with a robot in a simulated physical world. The user instructed SHRDLU to move various objects around in the ‘blocks world’ containing various basic objects. This program was very innovative and promising, and made many important suggestions not just for the development of artificial conversational systems, but also for the human cognition of language use in the physical world. This scheme still has a great influence on conversational systems. However, Winograd&nbsp;[<span class="ref-lnk lazy-ref"><a data-rid="CIT0187" data-reflink="_i80 _i82" href="#"><span class="off-screen">Citation</span>187</a></span>] himself pointed to the limits of the SHRDLU scheme in terms of background and subjectivity as follows:</p>
   <p></p>
   <ul class="NLM_list NLM_list-list_type-bullet">
    <li><p class="inline">We cannot provide a program with background in the sense of pre-understanding emphasized by Heidegger&nbsp;[<span class="ref-lnk lazy-ref"><a data-rid="CIT0188" data-reflink="_i80 _i82" href="#"><span class="off-screen">Citation</span>188</a></span>].</p></li>
    <li><p class="inline">The rationalistic approach to meaning that underlies systems such as SHRDLU is founded on the assumption that the meaning of words, and of the sentences and phrases they form, can be characterized independently of the interpretation given by individuals in a situation.</p></li>
   </ul>
   <p></p>
   <p>In terms of background, Heidegger&nbsp;[<span class="ref-lnk lazy-ref"><a data-rid="CIT0188" data-reflink="_i80 _i82" href="#"><span class="off-screen">Citation</span>188</a></span>] mentioned that a hearer interprets the intention of a speaker based on the background beliefs that were formed through past experiences; these prior understandings themselves were based on background beliefs that had been formed through more distant experiences, and this recursive structure continues indefinitely. Based on this, he concluded that such background beliefs could not be described explicitly in any way.</p>
   <p>Regarding subjectivity, the theories of pragmatics mentioned above have not contributed sufficient analysis or a reasonable explanation. This may be because, until recently, science has tended to exclude or ignore subjectivity in all research subjects. As mentioned above, the body plays an important role in holistic cognition, and can be a hub for subjectivity. Searle&nbsp;[<span class="ref-lnk lazy-ref"><a data-rid="CIT0189" data-reflink="_i80 _i82" href="#"><span class="off-screen">Citation</span>189</a></span>] thought the philosophy of language had reached relative stagnation because it took the position of so-called externalism, the idea that the meaning of words does not reside inside our heads but is a matter of causal relations between our heads and the external world.</p>
  </div>
  <div id="S008-S2001-S3002" class="NLM_sec NLM_sec_level_3">
   <h4 class="section-heading-4" id="_i60">8.1.2. Holistic property of language use</h4>
   <p>Cognitive linguistics attempts to explain language phenomena in a holistic way rather than by reductionism [<span class="ref-lnk lazy-ref"><a data-rid="CIT0190" data-reflink="_i80 _i82" href="#"><span class="off-screen">Citation</span>190</a></span>,<span class="ref-lnk lazy-ref"><a data-rid="CIT0191" data-reflink="_i80 _i82" href="#"><span class="off-screen">Citation</span>191</a></span>]. However, it suffers from the same shortcomings as the pragmatics theories. That is, it has not been successful in analyzing and explaining holistic phenomena in language, because the description is again inherently elementary, even if it effectively makes use of figures or images as a bridge between linguistic meanings and embodied experiences.</p>
   <p>We have been tackling this problem for approximately two decades based on a constructive approach [<span class="ref-lnk lazy-ref"><a data-rid="CIT0012" data-reflink="_i80 _i82" href="#"><span class="off-screen">Citation</span>12</a></span>,<span class="ref-lnk lazy-ref"><a data-rid="CIT0013" data-reflink="_i80 _i82" href="#"><span class="off-screen">Citation</span>13</a></span>,<span class="ref-lnk lazy-ref"><a data-rid="CIT0019" data-reflink="_i80 _i82" href="#"><span class="off-screen">Citation</span>19</a></span>,<span class="ref-lnk lazy-ref"><a data-rid="CIT0192 CIT0193 CIT0194 CIT0195 CIT0196 CIT0197 CIT0198 CIT0199" data-reflink="_i80 _i82" href="#"><span class="off-screen">Citation</span>192–199</a></span>]. In our approach, language is viewed as a holistic system that merges other cognitive capabilities. We focused on the connection to sensory–motor capabilities in the context of multimodal processes. We developed a language acquisition robot, LCore&nbsp;[<span class="ref-lnk lazy-ref"><a data-rid="CIT0192" data-reflink="_i80 _i82" href="#"><span class="off-screen">Citation</span>192</a></span>], that could learn a whole language system, including language, multimodal concepts, and actions through interactions with humans from scratch. The sophisticated constructive scheme for information integration, SERKET&nbsp;[<span class="ref-lnk lazy-ref"><a data-rid="CIT0200" data-reflink="_i80 _i82" href="#"><span class="off-screen">Citation</span>200</a></span>], developed by Nakamura et&nbsp;al. seems to be promising to model the phenomena in pragmatics in a holistic way.</p>
  </div>
  <div id="S008-S2001-S3003" class="NLM_sec NLM_sec_level_3">
   <h4 class="section-heading-4" id="_i61">8.1.3. Recursive property of background beliefs</h4>
   <p>With the rapid progress in machine learning, we gained access to a powerful tool for representing the recursive property of phenomena, i.e. recurrent neural networks. Hauser et&nbsp;al. [<span class="ref-lnk lazy-ref"><a data-rid="CIT0201" data-reflink="_i80 _i82" href="#"><span class="off-screen">Citation</span>201</a></span>] hypothesized that recursion was a uniquely human component in the faculty of language from the perspective of ethology and linguistics. In contrast, in the AI field, Elman et&nbsp;al. [<span class="ref-lnk lazy-ref"><a data-rid="CIT0202" data-reflink="_i80 _i82" href="#"><span class="off-screen">Citation</span>202</a></span>] found that simple recurrent neural networks could learn a nested structure of grammar with a small amount of learning data and raised doubts with respect to the innate nature of grammatical structures. Owing to advances in AI, new discoveries are occurring at a rapid pace. Sophisticated recurrent neural networks recently enabled the development of high-accuracy machine translation systems (e.g. [<span class="ref-lnk lazy-ref"><a data-rid="CIT0203" data-reflink="_i80 _i82" href="#"><span class="off-screen">Citation</span>203</a></span>]. In addition, a dialogue system was also enabled to utilize sequential contextual information as background&nbsp;[<span class="ref-lnk lazy-ref"><a data-rid="CIT0204" data-reflink="_i80 _i82" href="#"><span class="off-screen">Citation</span>204</a></span>]. Heidegger insisted that background knowledge could not be described. However, recent advances in AI suggest that such background could be described.</p>
  </div>
  <div id="S008-S2001-S3004" class="NLM_sec NLM_sec_level_3">
   <h4 class="section-heading-4" id="_i62">8.1.4. Cooperation with humans</h4>
   <p>The creation of robots capable of cooperating with humans must be the ultimate goal of our language and robotics research. During the cooperation, humans and robots need to understand the physical and mental state of the other and share a common target for their activities. We tend to think that such capability is higher-level than either expressing something accurately using language or independently acting sophisticatedly. However, even twenty-four-month-old infants can cooperate with others in collaborative physical activities&nbsp;[<span class="ref-lnk lazy-ref"><a data-rid="CIT0205" data-reflink="_i80 _i82" href="#"><span class="off-screen">Citation</span>205</a></span>]. They can understand the intentions of others, share a common target, and infer the roles of the participants attending to the activities. During the process of language acquisition by infants, they must understand the roles of others and themselves, here referred to as speaker–hearer or teacher–learner, and further understand the reversibility of such roles. However, twenty-month-old infants do not yet have a sophisticated grammatical capability. Thus, we can say that cooperation capability is the basis of language capability. In fact, psychological experiments have shown the correlation between language capability and the physical role reversal imitation capability&nbsp;[<span class="ref-lnk lazy-ref"><a data-rid="CIT0206" data-reflink="_i80 _i82" href="#"><span class="off-screen">Citation</span>206</a></span>].</p>
   <p>Dialogue is also a cooperative activity. We clarify the difference between physical cooperation and dialogue. During physical cooperation, information is explicitly exchanged through physical behaviors, whereas during dialogue, information is exchanged in a deictic and implicit way. Therefore, managing the ambiguity that invariably occurs during a dialogue is a major challenge. In order to manage such ambiguity in artificial cooperative systems, the probability that information linked to utterances is transmitted and received correctly should be calculated [<span class="ref-lnk lazy-ref"><a data-rid="CIT0195" data-reflink="_i80 _i82" href="#"><span class="off-screen">Citation</span>195</a></span>,<span class="ref-lnk lazy-ref"><a data-rid="CIT0207" data-reflink="_i80 _i82" href="#"><span class="off-screen">Citation</span>207</a></span>]. Whereas physical cooperation achieves common purpose in the here and now, conversational cooperation also enables achieving common purpose, not just here and now, but also in a way that transcends time and space.</p>
  </div>
 </div>
 <div id="S008-S2002" class="NLM_sec NLM_sec_level_2">
  <h3 class="section-heading-3" id="_i63">8.2. Social language</h3>
  <p>In order to clarify differences between the cognitive approach that has been discussed so far and the social approach, we describe the perspective of social language. For this purpose, we introduce a systemic functional linguistic theory representative of functional linguistics, with the notion that language is a social semiotic system&nbsp;[<span class="ref-lnk lazy-ref"><a data-rid="CIT0208" data-reflink="_i80 _i82" href="#"><span class="off-screen">Citation</span>208</a></span>].</p>
  <div id="S008-S2002-S3001" class="NLM_sec NLM_sec_level_3">
   <h4 class="section-heading-4" id="_i64">8.2.1. Language as a social semiotic system</h4>
   <p>As a functional approach to understanding the nature of social language, we introduce systemic functional linguistics (SFL) in this section. SFL is a linguistics developed by Halliday [<span class="ref-lnk lazy-ref"><a data-rid="CIT0209" data-reflink="_i80 _i82" href="#"><span class="off-screen">Citation</span>209</a></span>]. The big difference between other linguistics and SFL is that SFL introduces the ideas of ‘context’ and ‘system’ as part of its theoretical framework and discusses the linguistic system from the functional view of language in society [<span class="ref-lnk lazy-ref"><a data-rid="CIT0210" data-reflink="_i80 _i82" href="#"><span class="off-screen">Citation</span>210</a></span>], although many linguistics only focus on grammar, owing to the difficulty in dealing comprehensively with the various characteristics of semantics. The idea of ‘context’ comes from the study [<span class="ref-lnk lazy-ref"><a data-rid="CIT0211" data-reflink="_i80 _i82" href="#"><span class="off-screen">Citation</span>211</a></span>] by Malinowski, a cultural anthropologist. The previous idea of ‘context’ before the study by Malinowski focused on particular sentences before or after the current one; in other words, it indicated the idea of ‘togetherness with a text', i.e. ‘con-text.’ By contrast, the concept of context according to Malinowski points to the necessity of considering the cultural and situational backgrounds against which the text originated. To represent these two types of background as context, he introduced new concepts for the context defined as the ‘context of culture’ and the ‘context of situation.’</p>
   <p>As for the relation between a context and a text, Halliday regards a text in a context as functional language. Here, ‘functional’ means the function to have a certain meaning in a specific context; in other words, it corresponds to conveying the appropriate meaning to the participants in a communication in the context of a culture and a situation [<span class="ref-lnk lazy-ref"><a data-rid="CIT0212" data-reflink="_i80 _i82" href="#"><span class="off-screen">Citation</span>212</a></span>]. The meaning accompanying the functions to be conveyed to the participants is instantiated as a text in the context using the appropriate linguistic resources in terms of lexico-grammar and expressions.</p>
   <p>Halliday indicated that linguistics is a semiotics, but tends to be regarded as independent rather than being related to another semiotics. Based on this, he did not study language as a semiotics but defined linguistics as the study of the system of semiosis, and regarded the meaning of language as the system being operated through semiosis. In these circumstances, a text has a systematically representable structure and is instantiated through the process of choosing a meaning from the system of semiosis, also called the ‘system network.’ The process of instantiating a text through picking the meaning from a system is called the ‘meaning making process.’ The text is instantiated by reflecting social functions to exchange meaning, and is regarded as that which encapsulates meaning through the system that binds the social environment and the functional constituency of the language.</p>
   <p>Halliday's perspective on language is that meaning comes from being used in a social context and is an instrument to exchange messages and values. This is consistent with the later viewpoint on language in Wittgenstein's philosophy, namely that meaning is defined by the use of language in our everyday lives. In this sense, the use of language and actions by a robot must be based on its environment and instantiated as that which has meaning in this environment. Hence, the information a robot observes and recognizes must be closely related to the context of its actions and the use of language in order for a robot to truly use language.</p>
  </div>
  <div id="S008-S2002-S3002" class="NLM_sec NLM_sec_level_3">
   <h4 class="section-heading-4" id="_i65">8.2.2. Linguistic system defined by SFL</h4>
   <p>In SFL, the situation is instantiated against the cultural background of a social group, i.e. context of culture. Figure&nbsp;<a href="#F0005">5</a> shows the hierarchy of a linguistic system in SFL. The situation, i.e. context of situation, consists of three elements, i.e. ‘field', ‘tenor’, and ‘mode', which represent the register in terms of linguistics, social role, and method of communication, respectively.</p>
   <div class="figure figureViewer" id="F0005">
    <div class="hidden figureViewerArticleInfo">
     <span class="figViewerTitle">Survey on frontiers of language and robotics</span>
     <div class="articleAuthors articleInfoSection">
      <div class="authorsHeading">
       All authors
      </div>
      <div class="authors">
       <a class="entryAuthor" href="/action/doSearch?Contrib=Taniguchi%2C+T"><span class="hlFld-ContribAuthor"></span></a><a href="/author/Taniguchi%2C+T"><span class="NLM_given-names">T.</span> Taniguchi</a>, <a class="entryAuthor" href="/action/doSearch?Contrib=Mochihashi%2C+D"><span class="hlFld-ContribAuthor"></span></a><a href="/author/Mochihashi%2C+D"><span class="NLM_given-names">D.</span> Mochihashi</a>, <a class="entryAuthor" href="/action/doSearch?Contrib=Nagai%2C+T"><span class="hlFld-ContribAuthor"></span></a><a href="/author/Nagai%2C+T"><span class="NLM_given-names">T.</span> Nagai</a>, <a class="entryAuthor" href="/action/doSearch?Contrib=Uchida%2C+S"><span class="hlFld-ContribAuthor"></span></a><a href="/author/Uchida%2C+S"><span class="NLM_given-names">S.</span> Uchida</a>, <a class="entryAuthor" href="/action/doSearch?Contrib=Inoue%2C+N"><span class="hlFld-ContribAuthor"></span></a><a href="/author/Inoue%2C+N"><span class="NLM_given-names">N.</span> Inoue</a>, <a class="entryAuthor" href="/action/doSearch?Contrib=Kobayashi%2C+I"><span class="hlFld-ContribAuthor"></span></a><a href="/author/Kobayashi%2C+I"><span class="NLM_given-names">I.</span> Kobayashi</a>, <a class="entryAuthor" href="/action/doSearch?Contrib=Nakamura%2C+T"><span class="hlFld-ContribAuthor"></span></a><a href="/author/Nakamura%2C+T"><span class="NLM_given-names">T.</span> Nakamura</a>, <a class="entryAuthor" href="/action/doSearch?Contrib=Hagiwara%2C+Y"><span class="hlFld-ContribAuthor"></span></a><a href="/author/Hagiwara%2C+Y"><span class="NLM_given-names">Y.</span> Hagiwara</a>, <a class="entryAuthor" href="/action/doSearch?Contrib=Iwahashi%2C+N"><span class="hlFld-ContribAuthor"></span></a><a href="/author/Iwahashi%2C+N"><span class="NLM_given-names">N.</span> Iwahashi</a> &amp; <a class="entryAuthor" href="/action/doSearch?Contrib=Inamura%2C+T"><span class="hlFld-ContribAuthor"></span></a><a href="/author/Inamura%2C+T"><span class="NLM_given-names">T.</span> Inamura</a>
      </div>
     </div>
     <div class="articleLowerInfo articleInfoSection">
      <div class="articleLowerInfoSection articleInfoDOI">
       <a href="https://doi.org/10.1080/01691864.2019.1632223">https://doi.org/10.1080/01691864.2019.1632223</a>
      </div>
      <div class="articleInfoPublicationDate articleLowerInfoSection border">
       <h6>Published online:</h6>24 June 2019
      </div>
     </div>
    </div>
    <div class="figureThumbnailContainer">
     <div class="figureInfo">
      <div class="short-legend">
       <p class="captionText"><span class="captionLabel">Figure 5. </span> Linguistic system.</p>
      </div>
     </div><a href="#" class="thumbnail" aria-label="thumbnail image"><img id="F0005image" src="/na101/home/literatum/publisher/tandf/journals/content/tadr20/2019/tadr20.v033.i15-16/01691864.2019.1632223/20210828/images/medium/tadr_a_1632223_f0005_ob.jpg" loading="lazy" height="316" width="500"></a>
     <div class="figureDownloadOptions">
      <a href="#" class="downloadBtn btn btn-sm" role="button">Display full size</a>
     </div>
    </div>
   </div>
   <div class="hidden rs_skip" id="fig-description-F0005">
    <p class="captionText"><span class="captionLabel">Figure 5. </span> Linguistic system.</p>
   </div>
   <div class="hidden rs_skip" id="figureFootNote-F0005">
    <div class="figureFootNote-F0005"></div>
   </div>
   <p></p>
   <p>The field refers to the content of the conversations. For example, when one talks about a professional topic, one speaks differently from when they discuss daily affairs. Tenor refers to the context related to a social role. One uses different words or expressions when acting as a teacher or as a husband. Mode refers to the context related to the communication style. When one calls a friend over the phone, the talk is different from a conversation face to face. These clearly reflect our ability to use language adequately for communicating with each other in a social environment. Future service robots should definitely use language naturally in social contexts.</p>
   <p>Furthermore, the linguistic system constitutes a stratification with separate semiotic systems, i.e. semantics, lexico-grammar, and expression, linking each other. The linguistic system exists in the context and a linguistic text is instantiated to realize three metafunctions, called the ‘ideational function', ‘interpersonal function', and ‘textual function', reflecting the three contextual elements, respectively.</p>
   <p>Meaning is not static but dynamic, and its essential characteristics reside in instantiating a text by dynamically choosing linguistic resources based on the social context. The linguistic resources chosen from the linguistic system are used to instantiate a text followed by a rule, i.e. grammar. The process of instantiating appropriate texts in their context is called ‘wording’ and, therefore, SFL says ‘wording makes meaning.’</p>
   <p>In the same vein, when considering the meaning of language, it is quite important to consider the role of grammar. Meaning without grammar is the meaning according to semiotics, which is different from the meaning of language.</p>
  </div>
 </div>
 <div id="S008-S2003" class="NLM_sec NLM_sec_level_2">
  <h3 class="section-heading-3" id="_i67">8.3. Challenges in pragmatics and social language</h3>
  <p>We have so far demonstrated that pragmatics plays a big role in achieving communication, as well as the necessity of considering social factors to use language properly for exchanging meaning among dialogue participants. Based on these considerations, to achieve more advanced robotics, future challenges in pragmatics and social language include the following:</p>
  <ul class="NLM_list NLM_list-list_type-bullet">
   <li><p class="inline">Creating holistic language processing systems that involve physical, psychological, social, conceptual, and experiential constraints.</p></li>
   <li><p class="inline">Inventing machine learning methods to represent the recursive property of background beliefs for holistic language processing.</p></li>
   <li><p class="inline">Developing computational models for collaborative tasks in the physical world, leading to the emergence of dialogue (see e.g. [<span class="ref-lnk lazy-ref"><a data-rid="CIT0207" data-reflink="_i80 _i82" href="#"><span class="off-screen">Citation</span>207</a></span>,<span class="ref-lnk lazy-ref"><a data-rid="CIT0213" data-reflink="_i80 _i82" href="#"><span class="off-screen">Citation</span>213</a></span>]).</p></li>
   <li><p class="inline">Inventing methods to enable a robot to make use of contexts, e.g. situation and culture, and to grow the ability to use language to exchange meaning by referring to social factors: field, tenor, and mode.</p></li>
  </ul>
  <p></p>
 </div>
</div>
<div id="S009" class="NLM_sec NLM_sec_level_1">
 <h2 id="_i68" class="section-heading-2">9. Dataset, simulator, and competition</h2>
 <p>In research on pattern recognition such as image processing and speech recognition, datasets play an important role for the evaluation of learning and recognition performance. The same is true for the acquisition of languages by robots; however, the differences from conventional pattern recognition and annotation research are roughly divided into two groups. These are:</p>
 <ol class="NLM_list NLM_list-list_type-order">
  <li><p class="inline">handling of multimodal information including time-series data</p></li>
  <li><p class="inline">relationship between actions and information from the sensory systems of the robot.</p></li>
 </ol>
 <p></p>
 <p>Additionally, the ability to handle not just a static dataset but also a mechanism to allow the database to grow dynamically is important for the machine learning technique. To implement the dynamic growth of the dataset, competition and crowdsourcing would be useful strategies and may lead to a huge data collection. In this section, we discuss the dataset repository, simulation platforms, and robot competition to accelerate the research on language acquisition.</p>
 <div id="S009-S2001" class="NLM_sec NLM_sec_level_2">
  <h3 class="section-heading-3" id="_i69">9.1. Dataset</h3>
  <div id="S009-S2001-S3001" class="NLM_sec NLM_sec_level_3">
   <h4 class="section-heading-4" id="_i70">9.1.1. Scenes dataset</h4>
   <p>One of the most fundamental datasets related to language acquisition is the image and scene datasets contributed by the computer vision community. ImageNet [<span class="ref-lnk lazy-ref"><a data-rid="CIT0214" data-reflink="_i80 _i82" href="#"><span class="off-screen">Citation</span>214</a></span>], TinyImage [<span class="ref-lnk lazy-ref"><a data-rid="CIT0215" data-reflink="_i80 _i82" href="#"><span class="off-screen">Citation</span>215</a></span>], LabelMe [<span class="ref-lnk lazy-ref"><a data-rid="CIT0216" data-reflink="_i80 _i82" href="#"><span class="off-screen">Citation</span>216</a></span>], Flickr 8K/30K, MS COCO [<span class="ref-lnk lazy-ref"><a data-rid="CIT0217" data-reflink="_i80 _i82" href="#"><span class="off-screen">Citation</span>217</a></span>] are brief examples of image and scene datasets. These datasets comprise a set of images and category labels. The PASCAL dataset extends the category information to sentences to describe the context of the image. The DAQUAR dataset includes question-answering sentences with the image data.</p>
  </div>
  <div id="S009-S2001-S3002" class="NLM_sec NLM_sec_level_3">
   <h4 class="section-heading-4" id="_i71">9.1.2. Locations dataset</h4>
   <p>In addition to objects, locations and scenes are also important factors to convert into symbolic representations. In terms of indoor environments, there are several image scene datasets [<span class="ref-lnk lazy-ref"><a data-rid="CIT0218" data-reflink="_i80 _i82" href="#"><span class="off-screen">Citation</span>218</a></span>,<span class="ref-lnk lazy-ref"><a data-rid="CIT0219" data-reflink="_i80 _i82" href="#"><span class="off-screen">Citation</span>219</a></span>]. Zhou et&nbsp;al. [<span class="ref-lnk lazy-ref"><a data-rid="CIT0220" data-reflink="_i80 _i82" href="#"><span class="off-screen">Citation</span>220</a></span>] proposed a huge location dataset that contains more than seven million images with 476 location categories to overcome the data shortage in conventional location datasets.</p>
  </div>
  <div id="S009-S2001-S3003" class="NLM_sec NLM_sec_level_3">
   <h4 class="section-heading-4" id="_i72">9.1.3. Multimodal dataset including motion and action</h4>
   <p>Another important multimodal dataset for robots to understand activity in daily life consists of the motion and action of human users. Conventional motion datasets consist of motion patterns and category information, such as the CMU Mocap database, the CMU motion of body database, the HumanEva database, the HDM05 database, and the Human Motion database [<span class="ref-lnk lazy-ref"><a data-rid="CIT0221" data-reflink="_i80 _i82" href="#"><span class="off-screen">Citation</span>221</a></span>]. In addition to category information, the KIT Motion-Language dataset [<span class="ref-lnk lazy-ref"><a data-rid="CIT0221" data-reflink="_i80 _i82" href="#"><span class="off-screen">Citation</span>221</a></span>] contains sentence descriptions corresponding to the motion patterns. The sentences are collected using crowdsourcing as well as Takano's work [<span class="ref-lnk lazy-ref"><a data-rid="CIT0222" data-reflink="_i80 _i82" href="#"><span class="off-screen">Citation</span>222</a></span>].</p>
   <p>The datasets above include motion capture data, but movie datasets with language descriptions have also been provided. For example, cooking behaviors [<span class="ref-lnk lazy-ref"><a data-rid="CIT0223" data-reflink="_i80 _i82" href="#"><span class="off-screen">Citation</span>223</a></span>,<span class="ref-lnk lazy-ref"><a data-rid="CIT0224" data-reflink="_i80 _i82" href="#"><span class="off-screen">Citation</span>224</a></span>] and Hollywood movies [<span class="ref-lnk lazy-ref"><a data-rid="CIT0225" data-reflink="_i80 _i82" href="#"><span class="off-screen">Citation</span>225</a></span>] are often selected as target movies. Because most of these datasets depend on the crowdsourcing process, a more general dataset based on YouTube [<span class="ref-lnk lazy-ref"><a data-rid="CIT0226" data-reflink="_i80 _i82" href="#"><span class="off-screen">Citation</span>226</a></span>] has also been proposed.</p>
  </div>
  <div id="S009-S2001-S3004" class="NLM_sec NLM_sec_level_3">
   <h4 class="section-heading-4" id="_i73">9.1.4. Dialogue and action</h4>
   <p>A dataset with dialogues, namely questions and answers, in a social and embodied environment, is also strongly linked to NLP in robotics. VQA [<span class="ref-lnk lazy-ref"><a data-rid="CIT0227" data-reflink="_i80 _i82" href="#"><span class="off-screen">Citation</span>227</a></span>] is one of the famous datasets for questions and answers linked to images. MovieQA [<span class="ref-lnk lazy-ref"><a data-rid="CIT0228" data-reflink="_i80 _i82" href="#"><span class="off-screen">Citation</span>228</a></span>] contains movie data instead of still images. Embodied question answering (EQA) [<span class="ref-lnk lazy-ref"><a data-rid="CIT0229" data-reflink="_i80 _i82" href="#"><span class="off-screen">Citation</span>229</a></span>] provides questions and answers for a navigation task in a 3D simulation environment. Because the 3D simulation world can be readily connected to machine learning techniques for robotics [<span class="ref-lnk lazy-ref"><a data-rid="CIT0230" data-reflink="_i80 _i82" href="#"><span class="off-screen">Citation</span>230</a></span>], research activity on the use of natural language for navigation in simulation [<span class="ref-lnk lazy-ref"><a data-rid="CIT0231" data-reflink="_i80 _i82" href="#"><span class="off-screen">Citation</span>231</a></span>,<span class="ref-lnk lazy-ref"><a data-rid="CIT0232" data-reflink="_i80 _i82" href="#"><span class="off-screen">Citation</span>232</a></span>] should accelerate.</p>
   <p>Talk the walk [<span class="ref-lnk lazy-ref"><a data-rid="CIT0233" data-reflink="_i80 _i82" href="#"><span class="off-screen">Citation</span>233</a></span>] has over 10k dialogues for a tourist navigation task. Both a tourist and a guide communicate via natural language to achieve a goal, consisting of having the tourist navigate to a target location. Because the set of activities for the tourist, walking in New York City, the explanation by the guide, and sequence of realistic landscape pictures from the tourist's view, are collected via crowdsourcing, the database could be applied to the evaluation of grounding a conversation in a navigation task. A similar attempt was also proposed using crowdsourcing through virtual reality [<span class="ref-lnk lazy-ref"><a data-rid="CIT0234" data-reflink="_i80 _i82" href="#"><span class="off-screen">Citation</span>234</a></span>].</p>
  </div>
 </div>
 <div id="S009-S2002" class="NLM_sec NLM_sec_level_2">
  <h3 class="section-heading-3" id="_i74">9.2. Simulation platform</h3>
  <p>To date, an abundance of simulation platforms has originated from various sources including research projects, the robotics community, and software production companies. A conventional robotics simulator tends to focus on physics simulation to provide a reliable evaluation of the control and physical behavior of robots. As the main target is accuracy, it takes a long time to obtain a simulation result. Recently, several simulation environments focusing on approximative physics simulation, but with connectivity to a variety of useful tools in the AI community, were introduced. For example, DeepMind Lab [<span class="ref-lnk lazy-ref"><a data-rid="CIT0235" data-reflink="_i80 _i82" href="#"><span class="off-screen">Citation</span>235</a></span>] by DeepMind, Open AI gym/universe [<span class="ref-lnk lazy-ref"><a data-rid="CIT0236" data-reflink="_i80 _i82" href="#"><span class="off-screen">Citation</span>236</a></span>] by Open AI, and the Malmo project by Microsoft recently launched. In this environment, we can easily put an AI agent into an environment resembling a video game such as an ATARI game or Minecraft, to accelerate machine learning processes such as RL, as the video game environment provides a variety of actions for an AI agent; however, a more realistic and social environment is required for human–robot interaction in daily life. HoME [<span class="ref-lnk lazy-ref"><a data-rid="CIT0237" data-reflink="_i80 _i82" href="#"><span class="off-screen">Citation</span>237</a></span>], AI2-THOR [<span class="ref-lnk lazy-ref"><a data-rid="CIT0238" data-reflink="_i80 _i82" href="#"><span class="off-screen">Citation</span>238</a></span>], and MINOS [<span class="ref-lnk lazy-ref"><a data-rid="CIT0239" data-reflink="_i80 _i82" href="#"><span class="off-screen">Citation</span>239</a></span>] are recent platforms providing multimodal sensor information to AI agents in an indoor house environment. The basic strategy to learn symbols or a lexicon in such an environment is to access the ground truth provided by the simulation platform. The AI agent should use a specific API to access the ground truth. Behind the inquiring action is the use of natural communication between two AI agents, or a human–robot interaction. Therefore, aiming at a more natural fundamental acquisition of language requires an additional communication function. For example, the restaurant game [<span class="ref-lnk lazy-ref"><a data-rid="CIT0240" data-reflink="_i80 _i82" href="#"><span class="off-screen">Citation</span>240</a></span>] and Mars escape [<span class="ref-lnk lazy-ref"><a data-rid="CIT0241" data-reflink="_i80 _i82" href="#"><span class="off-screen">Citation</span>241</a></span>] are famous research projects using communication between an agent and a human. A key technique for both projects is to use crowdsourcing. The general public can access the project software to establish communication with the AI agent through the Internet. The researcher therefore easily collects a large communication log in the simulated world. The aim of a more natural and complicated environment as the next target, is a more natural and wide variety of actions for humans and agents in the simulation. Controlling the humanoid agent in the systems above is difficult. However, the language acquisition process has a strong relationship between not just simple actions such as ‘go forward’ or ‘turn right', but also more complex actions using a realistic body and limbs. To close the gap between a conventional robotics simulation by a humanoid agent and recent simulation systems for AI, Inamura et&nbsp;al. proposed a unique simulation named SIGVerse [<span class="ref-lnk lazy-ref"><a data-rid="CIT0242" data-reflink="_i80 _i82" href="#"><span class="off-screen">Citation</span>242</a></span>]. The latest version of the SIGVerse [<span class="ref-lnk lazy-ref"><a data-rid="CIT0243" data-reflink="_i80 _i82" href="#"><span class="off-screen">Citation</span>243</a></span>] integrates ROS [<span class="ref-lnk lazy-ref"><a data-rid="CIT0244" data-reflink="_i80 _i82" href="#"><span class="off-screen">Citation</span>244</a></span>] and Unity to provide a flexible simulation environment for both robots and human users. A user can log in to a virtual avatar through a head-mounted display and a motion capture device, and have a social and embodied interaction with a virtual robot controlled with the same software as the real robot. The location concept and lexicon acquisition by Taniguchi uses the SIGVerse simulator to confirm the basic performance. Because this simulator works on a cloud computing resource, it is easy to apply the human–robot interaction experiment through crowdsourcing. This software is adopted in the robot competition, as described in the next subsection.</p>
 </div>
 <div id="S009-S2003" class="NLM_sec NLM_sec_level_2">
  <h3 class="section-heading-3" id="_i75">9.3. Competition</h3>
  <p>Competition is another social factor to improve the research on robotics and language acquisition, as well as the multimodal dataset. One of the popular competitions in the robotics field is RoboCup. In the early days, when the RoboCup was first established, the main competition was a soccer game; however, a more social and complex environment was added as the evaluation target, named RoboCup@Home [<span class="ref-lnk lazy-ref"><a data-rid="CIT0245" data-reflink="_i80 _i82" href="#"><span class="off-screen">Citation</span>245</a></span>].</p>
  <p>The RoboCup@Home project aims to promote the development of general service robots that act in a daily life environment. The typical task in the competition is to perform a physical service with an understanding of the user's request by means of a natural language expression. For example, in a task named general purpose service robot (GPSR), a user issues instructions to a robot such as ‘Take the milk from the fridge and bring it to me.’ The robot must understand the meaning of the instructions and carry out the request. Understanding the instructions requires multiple capabilities including object recognition, manipulation of a target object, human and face recognition, speech recognition, and motion planning. This means that the robot must establish a representation linking language expression and physical action in the real world. The evaluation criteria focus on the physical behavior of the robot, including whether the robot grasps the objectives, arrives at the target place, and succeeds in avoiding the obstacles. The score thus indicates the quality of the NLP performance in the robot. The competition framework is one of the best ways to evaluate performance based on the linguistics skills of the robots; however, the evaluation target is biased toward physical functions because (1) the aim is the development of physical robot systems, and (2) objective physical characteristics can readily be evaluated. Another expected evaluation target should be the ability to deal with uncertain or contradictory instructions, because the usual conversation in the daily life is invariably filled with vague expressions and humans sometimes mistakenly issue a contradictory request after misunderstanding a situation. In an ideal competition, service robots should cover this type of situation; however, RoboCup@Home has recently not focused on the problem. Competitions in conversational language processing systems such as conversational intelligence challenge (ConvAI) [<span class="ref-lnk lazy-ref"><a data-rid="CIT0246" data-reflink="_i80 _i82" href="#"><span class="off-screen">Citation</span>246</a></span>], and dialogue system technology challenges (DSTC) [<span class="ref-lnk lazy-ref"><a data-rid="CIT0247" data-reflink="_i80 _i82" href="#"><span class="off-screen">Citation</span>247</a></span>] tend to evaluate the ability to deal with vague expressions and contradictions. The difficulty in recent research is to implement both (1) the generation of physical actions from natural language expression and (2) dealing with uncertainty and contradiction in the real world, and also expected from robot competition systems.</p>
 </div>
 <div id="S009-S2004" class="NLM_sec NLM_sec_level_2">
  <h3 class="section-heading-3" id="_i76">9.4. Challenges in dataset, simulator, and competition</h3>
  <p>The remaining perspective in the dataset for language processing in robot systems is the social and embodied experience data with annotations. It requires subjective sensor information for the robot system, not just the overhead view such as the movie database, to learn the correspondence between sensor information and language expression. The simulation environment is one of the important components to predict and associate with sensor information in robot systems. To summarize this section, the remaining challenges are:</p>
  <ul class="NLM_list NLM_list-list_type-bullet">
   <li><p class="inline">Building a social and embodied experience dataset for robots.</p></li>
   <li><p class="inline">Developing a method to predict and associate the social behavior of agents in simulation systems, not just the physical simulation.</p></li>
   <li><p class="inline">Designing sophisticated robot competition systems that support building social and embodied experience datasets.</p></li>
  </ul>
  <p></p>
 </div>
</div>
<div id="S010" class="NLM_sec NLM_sec_level_1">
 <h2 id="_i77" class="section-heading-2">10. Conclusion</h2>
 <p>In this paper, we surveyed the existing challenges at the intersection of NLP and robotics. First, we discussed the importance of the intersection of NLP and robotics from both perspectives: language and robotics. Language understanding and acquisition in real-world environments is an important task for future service robotics, because our social environment is full of rich and dynamic linguistic interactions. Service robots must perform tasks based on understanding people's utterances and contexts, e.g. situation and culture. Based on this, we visited seven frontiers that we will need to explore further in future studies of language and robotics.</p>
 <p>It is now clear that many challenges remain, ranging from semantics to pragmatics. In particular, the social aspect of language has not been explored in robotics, although the future language used in human–robot interactions will be based on pragmatics and social language in the same way as human-to-human communication.</p>
 <p>We would like to address the notion of symbol emergence systems&nbsp;[<span class="ref-lnk lazy-ref"><a data-rid="CIT0017" data-reflink="_i80 _i82" href="#"><span class="off-screen">Citation</span>17</a></span>,<span class="ref-lnk lazy-ref"><a data-rid="CIT0018" data-reflink="_i80 _i82" href="#"><span class="off-screen">Citation</span>18</a></span>]. Figure&nbsp;<a href="#F0006">6</a> shows a schematic view of a symbol emergence system, with a comprehensive view of a multi-agent system using language and a dynamic symbol system emerging through semiotic interactions between agents. The emergent symbol system is organized through inter-agent semiotic communication and internal representations formed by the agents. The internal representations are formed based on both semiotic communications and physical interactions, e.g. multimodal categorization and affordance learning. The problems in language and robotics can be interpreted as a part of the phenomena in a symbol emergence system.</p>
 <div class="figure figureViewer" id="F0006">
  <div class="hidden figureViewerArticleInfo">
   <span class="figViewerTitle">Survey on frontiers of language and robotics</span>
   <div class="articleAuthors articleInfoSection">
    <div class="authorsHeading">
     All authors
    </div>
    <div class="authors">
     <a class="entryAuthor" href="/action/doSearch?Contrib=Taniguchi%2C+T"><span class="hlFld-ContribAuthor"></span></a><a href="/author/Taniguchi%2C+T"><span class="NLM_given-names">T.</span> Taniguchi</a>, <a class="entryAuthor" href="/action/doSearch?Contrib=Mochihashi%2C+D"><span class="hlFld-ContribAuthor"></span></a><a href="/author/Mochihashi%2C+D"><span class="NLM_given-names">D.</span> Mochihashi</a>, <a class="entryAuthor" href="/action/doSearch?Contrib=Nagai%2C+T"><span class="hlFld-ContribAuthor"></span></a><a href="/author/Nagai%2C+T"><span class="NLM_given-names">T.</span> Nagai</a>, <a class="entryAuthor" href="/action/doSearch?Contrib=Uchida%2C+S"><span class="hlFld-ContribAuthor"></span></a><a href="/author/Uchida%2C+S"><span class="NLM_given-names">S.</span> Uchida</a>, <a class="entryAuthor" href="/action/doSearch?Contrib=Inoue%2C+N"><span class="hlFld-ContribAuthor"></span></a><a href="/author/Inoue%2C+N"><span class="NLM_given-names">N.</span> Inoue</a>, <a class="entryAuthor" href="/action/doSearch?Contrib=Kobayashi%2C+I"><span class="hlFld-ContribAuthor"></span></a><a href="/author/Kobayashi%2C+I"><span class="NLM_given-names">I.</span> Kobayashi</a>, <a class="entryAuthor" href="/action/doSearch?Contrib=Nakamura%2C+T"><span class="hlFld-ContribAuthor"></span></a><a href="/author/Nakamura%2C+T"><span class="NLM_given-names">T.</span> Nakamura</a>, <a class="entryAuthor" href="/action/doSearch?Contrib=Hagiwara%2C+Y"><span class="hlFld-ContribAuthor"></span></a><a href="/author/Hagiwara%2C+Y"><span class="NLM_given-names">Y.</span> Hagiwara</a>, <a class="entryAuthor" href="/action/doSearch?Contrib=Iwahashi%2C+N"><span class="hlFld-ContribAuthor"></span></a><a href="/author/Iwahashi%2C+N"><span class="NLM_given-names">N.</span> Iwahashi</a> &amp; <a class="entryAuthor" href="/action/doSearch?Contrib=Inamura%2C+T"><span class="hlFld-ContribAuthor"></span></a><a href="/author/Inamura%2C+T"><span class="NLM_given-names">T.</span> Inamura</a>
    </div>
   </div>
   <div class="articleLowerInfo articleInfoSection">
    <div class="articleLowerInfoSection articleInfoDOI">
     <a href="https://doi.org/10.1080/01691864.2019.1632223">https://doi.org/10.1080/01691864.2019.1632223</a>
    </div>
    <div class="articleInfoPublicationDate articleLowerInfoSection border">
     <h6>Published online:</h6>24 June 2019
    </div>
   </div>
  </div>
  <div class="figureThumbnailContainer">
   <div class="figureInfo">
    <div class="short-legend">
     <p class="captionText"><span class="captionLabel">Figure 6. </span> Symbol emergence system&nbsp;[<span class="ref-lnk lazy-ref"><a data-rid="CIT0017" data-reflink="_i80 _i82" href="#"><span class="off-screen">Citation</span>17</a></span>] is an integral view of dynamic language phenomena involving cognitive and social processes.</p>
    </div>
   </div><a href="#" class="thumbnail" aria-label="thumbnail image"><img id="F0006image" src="/na101/home/literatum/publisher/tandf/journals/content/tadr20/2019/tadr20.v033.i15-16/01691864.2019.1632223/20210828/images/medium/tadr_a_1632223_f0006_ob.jpg" loading="lazy" height="312" width="500"></a>
   <div class="figureDownloadOptions">
    <a href="#" class="downloadBtn btn btn-sm" role="button">Display full size</a>
   </div>
  </div>
 </div>
 <div class="hidden rs_skip" id="fig-description-F0006">
  <p class="captionText"><span class="captionLabel">Figure 6. </span> Symbol emergence system&nbsp;[<span class="ref-lnk lazy-ref"><a data-rid="CIT0017" data-reflink="_i80 _i82" href="#"><span class="off-screen">Citation</span>17</a></span>] is an integral view of dynamic language phenomena involving cognitive and social processes.</p>
 </div>
 <div class="hidden rs_skip" id="figureFootNote-F0006">
  <div class="figureFootNote-F0006"></div>
 </div>
 <p></p>
 <p>Language itself is a dynamic, systemic, cognitive, and social phenomenon. Therefore, to understand language in a scientific manner and to engineer it, we need a model with a sensor–motor system that can be involved in human linguistic communication: this is definitely a robot. We now repeat our claim that language and robotics are becoming an inevitably important academic field. To push this important research field forward, our future work will be to tackle the challenges identified systematically.</p>
</div>