<div id="S001" class="NLM_sec NLM_sec-type_intro NLM_sec_level_1">
 <h2 id="_i4" class="section-heading-2">Introduction</h2>
 <p>Contemporary linguistic research is characterised by a great variety of methodological approaches. In particular, in the fields of psycholinguistics and neurobiology of language a vast number of different methods are applied in order to investigate the neural and mental processing principles of language acquisition, representation, comprehension and production (De Groot &amp; Hagoort, <span class="ref-lnk lazy-ref"><a data-rid="CIT0023" data-reflink="_i34 _i35 _i36" href="#"><span class="off-screen">Citation</span>2017</a></span>). Besides functional magnetic resonance imaging (fMRI) studies (Deniz et al., <span class="ref-lnk lazy-ref"><a data-rid="CIT0024" data-reflink="_i34 _i35 _i36" href="#"><span class="off-screen">Citation</span>2019</a></span>; Huth et al., <span class="ref-lnk lazy-ref"><a data-rid="CIT0056" data-reflink="_i34 _i35 _i36" href="#"><span class="off-screen">Citation</span>2016</a></span>; Spitzer et al., <span class="ref-lnk lazy-ref"><a data-rid="CIT0124" data-reflink="_i34 _i35 _i36" href="#"><span class="off-screen">Citation</span>1998</a></span>), electrophysiological measurements, i.e. magnetoencephalography (MEG) (Hämäläinen et al., <span class="ref-lnk lazy-ref"><a data-rid="CIT0047" data-reflink="_i34 _i35 _i36" href="#"><span class="off-screen">Citation</span>1993</a></span>) and electroencephalography (EEG) (Files, <span class="ref-lnk lazy-ref"><a data-rid="CIT0031" data-reflink="_i34 _i35 _i36" href="#"><span class="off-screen">Citation</span>2011</a></span>; Millett, <span class="ref-lnk lazy-ref"><a data-rid="CIT0090" data-reflink="_i34 _i35 _i36" href="#"><span class="off-screen">Citation</span>2001</a></span>), are widely used in neurolinguistics to investigate the neural and mental correlates underlying language processing in the human brain (Bambini et al., <span class="ref-lnk lazy-ref"><a data-rid="CIT0005" data-reflink="_i34 _i35 _i36" href="#"><span class="off-screen">Citation</span>2016</a></span>; Lai et al., <span class="ref-lnk lazy-ref"><a data-rid="CIT0079" data-reflink="_i34 _i35 _i36" href="#"><span class="off-screen">Citation</span>2019</a></span>; Pulvermüller &amp; Shtyrov, <span class="ref-lnk lazy-ref"><a data-rid="CIT0109" data-reflink="_i34 _i35 _i36" href="#"><span class="off-screen">Citation</span>2008</a></span>; Pulvermüller et al., <span class="ref-lnk lazy-ref"><a data-rid="CIT0110" data-reflink="_i34 _i35 _i36" href="#"><span class="off-screen">Citation</span>2009</a></span>; Schmidt-Snoek et al., <span class="ref-lnk lazy-ref"><a data-rid="CIT0118" data-reflink="_i34 _i35 _i36" href="#"><span class="off-screen">Citation</span>2015</a></span>; Tomasello et al., <span class="ref-lnk lazy-ref"><a data-rid="CIT0127" data-reflink="_i34 _i35 _i36" href="#"><span class="off-screen">Citation</span>2019</a></span>).</p>
 <p>However, most of the experimental studies on language processing conducted so far have focused on one aspect of linguistic information at a time. For instance, neurocognitive studies have explored the neural responses of words compared to pseudo words (Craddock et al., <span class="ref-lnk lazy-ref"><a data-rid="CIT0019" data-reflink="_i34 _i35 _i36" href="#"><span class="off-screen">Citation</span>2015</a></span>; Pulvermüller et al., <span class="ref-lnk lazy-ref"><a data-rid="CIT0107" data-reflink="_i34 _i35 _i36" href="#"><span class="off-screen">Citation</span>1994</a></span>), between different conceptual semantic categories (Moseley et al., <span class="ref-lnk lazy-ref"><a data-rid="CIT0093" data-reflink="_i34 _i35 _i36" href="#"><span class="off-screen">Citation</span>2013</a></span>), complex against simple grammatical sentences (Friederici et al., <span class="ref-lnk lazy-ref"><a data-rid="CIT0032" data-reflink="_i34 _i35 _i36" href="#"><span class="off-screen">Citation</span>2006</a></span>), or during pragmatic processing of different communicative actions (Tomasello et al., <span class="ref-lnk lazy-ref"><a data-rid="CIT0127" data-reflink="_i34 _i35 _i36" href="#"><span class="off-screen">Citation</span>2019</a></span>). Although, all these studies shed light on the correlates of language processing in the human brain, it is still not fully understood whether similar brain responses during single words or sentence understanding also emerge during perception of natural speech, similar to everyday experience. However, recently a growing number of approaches address this issue (Brodbeck et al., <span class="ref-lnk lazy-ref"><a data-rid="CIT0014" data-reflink="_i34 _i35 _i36" href="#"><span class="off-screen">Citation</span>2018</a></span>; Broderick et al., <span class="ref-lnk lazy-ref"><a data-rid="CIT0015" data-reflink="_i34 _i35 _i36" href="#"><span class="off-screen">Citation</span>2018</a></span>; Deniz et al., <span class="ref-lnk lazy-ref"><a data-rid="CIT0024" data-reflink="_i34 _i35 _i36" href="#"><span class="off-screen">Citation</span>2019</a></span>; Ding &amp; Simon, <span class="ref-lnk lazy-ref"><a data-rid="CIT0026" data-reflink="_i34 _i35 _i36" href="#"><span class="off-screen">Citation</span>2012</a></span>; Silbert et al., <span class="ref-lnk lazy-ref"><a data-rid="CIT0121" data-reflink="_i34 _i35 _i36" href="#"><span class="off-screen">Citation</span>2014</a></span>).</p>
 <p>Furthermore, traditional experimental designs typically consist of at least two different conditions studied under carefully controlled circumstances (Bambini et al., <span class="ref-lnk lazy-ref"><a data-rid="CIT0005" data-reflink="_i34 _i35 _i36" href="#"><span class="off-screen">Citation</span>2016</a></span>; Lai et al., <span class="ref-lnk lazy-ref"><a data-rid="CIT0079" data-reflink="_i34 _i35 _i36" href="#"><span class="off-screen">Citation</span>2019</a></span>; Schmidt-Snoek et al., <span class="ref-lnk lazy-ref"><a data-rid="CIT0118" data-reflink="_i34 _i35 _i36" href="#"><span class="off-screen">Citation</span>2015</a></span>). The measured data are then pre-processed, i.e. referenced, filtered, epoched and averaged, and finally contrasted according to the different stimulation conditions (De Groot &amp; Hagoort, <span class="ref-lnk lazy-ref"><a data-rid="CIT0023" data-reflink="_i34 _i35 _i36" href="#"><span class="off-screen">Citation</span>2017</a></span>). To obtain a good signal to noise ratio (SNR) of the acquired brain responses, each of these conditions must contain dozens of different items or stimulus repetitions.</p>
 <p>For instance, the evaluation of event-related potentials (ERPs) from the EEG data, or, in the case of MEG, event-related fields (ERFs), requires a relatively large number of stimuli (40–120 trials) per condition to achieve high SNR and ensure sufficient statistical power. This is due to the fact that the signal of a specific condition remains constant across multiple repetitions while the noise signal which is assumed to be randomly distributed, is reduced when large number of time-locked stimuli are pooled together (Coles &amp; Rugg, <span class="ref-lnk lazy-ref"><a data-rid="CIT0016" data-reflink="_i34 _i35 _i36" href="#"><span class="off-screen">Citation</span>1995</a></span>; Handy, <span class="ref-lnk lazy-ref"><a data-rid="CIT0049" data-reflink="_i34 _i35 _i36" href="#"><span class="off-screen">Citation</span>2005</a></span>; Luck, <span class="ref-lnk lazy-ref"><a data-rid="CIT0084" data-reflink="_i34 _i35 _i36" href="#"><span class="off-screen">Citation</span>2014</a></span>; Pfurtscheller &amp; Da Silva, <span class="ref-lnk lazy-ref"><a data-rid="CIT0101" data-reflink="_i34 _i35 _i36" href="#"><span class="off-screen">Citation</span>1999</a></span>; Woodman, <span class="ref-lnk lazy-ref"><a data-rid="CIT0136" data-reflink="_i34 _i35 _i36" href="#"><span class="off-screen">Citation</span>2010</a></span>).</p>
 <p>However, creating a large number of stimuli to increase SNR is associated with a serious drawback. It is well known that repeated presentation of a stimulus causes a diminished neural activation, a phenomenon for which the term <i>repetition suppression</i> has been coined (Arnaud et al., <span class="ref-lnk lazy-ref"><a data-rid="CIT0003" data-reflink="_i34 _i35 _i36" href="#"><span class="off-screen">Citation</span>2013</a></span>; Grill-Spector et al., <span class="ref-lnk lazy-ref"><a data-rid="CIT0044" data-reflink="_i34 _i35 _i36" href="#"><span class="off-screen">Citation</span>2006</a></span>; Henson, <span class="ref-lnk lazy-ref"><a data-rid="CIT0051" data-reflink="_i34 _i35 _i36" href="#"><span class="off-screen">Citation</span>2003</a></span>; Mayrhauser et al., <span class="ref-lnk lazy-ref"><a data-rid="CIT0088" data-reflink="_i34 _i35 _i36" href="#"><span class="off-screen">Citation</span>2014</a></span>; Summerfield et al., <span class="ref-lnk lazy-ref"><a data-rid="CIT0126" data-reflink="_i34 _i35 _i36" href="#"><span class="off-screen">Citation</span>2008</a></span>). In fMRI, repetition suppression is observed as a reduced blood oxygen-level-dependent (BOLD) response elicited by a repeated stimulus, also called fMRI adaptation (Grill-Spector &amp; Malach, <span class="ref-lnk lazy-ref"><a data-rid="CIT0045" data-reflink="_i34 _i35 _i36" href="#"><span class="off-screen">Citation</span>2001</a></span>); for a recent review, see also (Segaert et al., <span class="ref-lnk lazy-ref"><a data-rid="CIT0119" data-reflink="_i34 _i35 _i36" href="#"><span class="off-screen">Citation</span>2013</a></span>). The underlying neuronal mechanisms are still a matter of debate, and range from neuronal fatigue (Grill-Spector et al., <span class="ref-lnk lazy-ref"><a data-rid="CIT0044" data-reflink="_i34 _i35 _i36" href="#"><span class="off-screen">Citation</span>2006</a></span>), or neuronal sharpening (Martens &amp; Gruber, <span class="ref-lnk lazy-ref"><a data-rid="CIT0087" data-reflink="_i34 _i35 _i36" href="#"><span class="off-screen">Citation</span>2012</a></span>), through neuronal facilitation (Grill-Spector et al., <span class="ref-lnk lazy-ref"><a data-rid="CIT0044" data-reflink="_i34 _i35 _i36" href="#"><span class="off-screen">Citation</span>2006</a></span>) as relatively automatic bottom-up mechanisms, to predictive coding (Friston, <span class="ref-lnk lazy-ref"><a data-rid="CIT0035" data-reflink="_i34 _i35 _i36" href="#"><span class="off-screen">Citation</span>2005</a></span>). There, top-down backward influences from higher to lower cortical layers modulate processing in case of a correct prediction of the upcoming stimulus. Hence, repetition suppression reflects a smaller prediction error for expected stimuli, i.e. decreased activation for repeated stimuli. Thus, in order to prevent repetition suppression, it is necessary to design a certain number of different stimuli from each condition to avoid repetition, which is often very challenging or even impossible. One strategy is to focus on single-item ERPs/ERFs, but in such cases it is necessary to compensate by testing more participants to obtain stable signals (Laszlo &amp; Federmeier, <span class="ref-lnk lazy-ref"><a data-rid="CIT0081" data-reflink="_i34 _i35 _i36" href="#"><span class="off-screen">Citation</span>2011</a></span>).</p>
 <p>Here, we present an alternative approach to overcome the aforementioned limitations of the electrophysiological assessment of language processing and to open up the possibility of investigating different levels of linguistic information during natural speech comprehension within a single experiment. In particular, in the present study, we investigated brain responses elicited during listening to the audio book edition of a German-language novel by means of MEG measurements (for similar approaches, see Huth et al., <span class="ref-lnk lazy-ref"><a data-rid="CIT0056" data-reflink="_i34 _i35 _i36" href="#"><span class="off-screen">Citation</span>2016</a></span>; Wehbe et al., <span class="ref-lnk lazy-ref"><a data-rid="CIT0134" data-reflink="_i34 _i35 _i36" href="#"><span class="off-screen">Citation</span>2014</a></span>). Repetition suppression is not expected to occur here, as the same linguistic utterance is not repeatedly presented among a few stimuli types, and if repetition happens, it does in different linguistic contexts (i.e. it possibly occurs with different linguistic units) and also more sparsely.</p>
 <p>Other previously published papers describe the use of continuously written stimuli in reading studies while recording EEG/MEG (Barca et al., <span class="ref-lnk lazy-ref"><a data-rid="CIT0006" data-reflink="_i34 _i35 _i36" href="#"><span class="off-screen">Citation</span>2011</a></span>; Cornelissen et al., <span class="ref-lnk lazy-ref"><a data-rid="CIT0017" data-reflink="_i34 _i35 _i36" href="#"><span class="off-screen">Citation</span>2009</a></span>; Dalal et al., <span class="ref-lnk lazy-ref"><a data-rid="CIT0020" data-reflink="_i34 _i35 _i36" href="#"><span class="off-screen">Citation</span>2009</a></span>; Laine et al., <span class="ref-lnk lazy-ref"><a data-rid="CIT0080" data-reflink="_i34 _i35 _i36" href="#"><span class="off-screen">Citation</span>2000</a></span>). Remarkably, it turned out that the representation of semantic information across human cerebral cortex during listening versus reading is invariant to stimulus modality (Deniz et al., <span class="ref-lnk lazy-ref"><a data-rid="CIT0024" data-reflink="_i34 _i35 _i36" href="#"><span class="off-screen">Citation</span>2019</a></span>). Since listening to an audio book during the 1-h measurement session seems to be less strenuous for the participants than reading for the same period of time, we chose acoustic stimulation rather than visual stimulation.</p>
 <p>Using computational corpus linguistics (CCL) (Sinclair, <span class="ref-lnk lazy-ref"><a data-rid="CIT0122" data-reflink="_i34 _i35 _i36" href="#"><span class="off-screen">Citation</span>2004</a></span>; Souter &amp; Atwell, <span class="ref-lnk lazy-ref"><a data-rid="CIT0123" data-reflink="_i34 _i35 _i36" href="#"><span class="off-screen">Citation</span>1993</a></span>) applied to the analyses of large text corpora, which usually consist of hundreds of thousands or even billions of tokens (Aston &amp; Burnard, <span class="ref-lnk lazy-ref"><a data-rid="CIT0004" data-reflink="_i34 _i35 _i36" href="#"><span class="off-screen">Citation</span>1998</a></span>; Davies, <span class="ref-lnk lazy-ref"><a data-rid="CIT0022" data-reflink="_i34 _i35 _i36" href="#"><span class="off-screen">Citation</span>2010</a></span>; Ide &amp; Suderman, <span class="ref-lnk lazy-ref"><a data-rid="CIT0057" data-reflink="_i34 _i35 _i36" href="#"><span class="off-screen">Citation</span>2004</a></span>; Michel et al., <span class="ref-lnk lazy-ref"><a data-rid="CIT0089" data-reflink="_i34 _i35 _i36" href="#"><span class="off-screen">Citation</span>2011</a></span>; Schäfer &amp; Bildhauer, <span class="ref-lnk lazy-ref"><a data-rid="CIT0112" data-reflink="_i34 _i35 _i36" href="#"><span class="off-screen">Citation</span>2012</a></span>; Trinkle et al., <span class="ref-lnk lazy-ref"><a data-rid="CIT0129" data-reflink="_i34 _i35 _i36" href="#"><span class="off-screen">Citation</span>2016</a></span>), offers the opportunity to test a vast number of hypotheses by repeatedly re-analysing the data (Evert, <span class="ref-lnk lazy-ref"><a data-rid="CIT0028" data-reflink="_i34 _i35 _i36" href="#"><span class="off-screen">Citation</span>2005</a></span>) and to deploy modern machine learning techniques on such datasets (Koskinen &amp; Seppä, <span class="ref-lnk lazy-ref"><a data-rid="CIT0065" data-reflink="_i34 _i35 _i36" href="#"><span class="off-screen">Citation</span>2014</a></span>). Furthermore, text corpora also allow for exploratory data analyses in order to generate new hypotheses (Leech, <span class="ref-lnk lazy-ref"><a data-rid="CIT0083" data-reflink="_i34 _i35 _i36" href="#"><span class="off-screen">Citation</span>2014</a></span>). In our approach, we can generate a large database of neuronal activity in a single measurement session, corresponding to the comprehension of several thousands of words of continuous speech similarly to everyday language. However, for later studies, the data set has to be split into multiple parts (e.g. development/training/test or training/validation/test) in analogy to standard machine learning data sets as MNIST (50,000 training images, 10,000 test images (Bottou et al., <span class="ref-lnk lazy-ref"><a data-rid="CIT0011" data-reflink="_i34 _i35 _i36" href="#"><span class="off-screen">Citation</span>1994</a></span>)), as hypothesis generation and checking for statistical significance have to be done in two disjoint steps, in order to prevent <i>HARKing</i> (hypothesising after the results are known) (Kerr, <span class="ref-lnk lazy-ref"><a data-rid="CIT0062" data-reflink="_i34 _i35 _i36" href="#"><span class="off-screen">Citation</span>1998</a></span>). In such cases, inferential statistical analysis is not valid and applicable (Munafò et al., <span class="ref-lnk lazy-ref"><a data-rid="CIT0095" data-reflink="_i34 _i35 _i36" href="#"><span class="off-screen">Citation</span>2017</a></span>). Thus, this approach is only possible with a large dataset.</p>
 <p>Here, we provide the proof-of-principle of this approach by calculating ERFs and normalised power spectra of word onsets and offsets overall as well as for the group of content words (nouns, verbs, adjectives) and for the group of function words (determiners, prepositions, conjunctions), which are known to differ semantically to a substantial extent. Hence, greater activation for content compared to function words can be expected, as reported in previous studies (e.g. Diaz &amp; McCarthy, <span class="ref-lnk lazy-ref"><a data-rid="CIT0025" data-reflink="_i34 _i35 _i36" href="#"><span class="off-screen">Citation</span>2009</a></span>; Pulvermüller et al., <span class="ref-lnk lazy-ref"><a data-rid="CIT0105" data-reflink="_i34 _i35 _i36" href="#"><span class="off-screen">Citation</span>1995</a></span>).<span class="ref-lnk fn-ref-lnk lazy-ref"><a data-rid="EN0001" href="#" data-reflink="fn"><span class="off-screen">Footnote</span><sup>1</sup></a></span> Furthermore, we check for consistency of the data, by comparing intra-individual differences of neural activity in different brain regions and we perform non-parametric cluster permutation tests to determine significant differences between conditions.</p>
</div>
<div id="S002" class="NLM_sec NLM_sec-type_methods NLM_sec_level_1">
 <h2 id="_i5" class="section-heading-2">Methods</h2>
 <div id="S002-S2001" class="NLM_sec NLM_sec_level_2">
  <h3 class="section-heading-3" id="_i6">Human participants</h3>
  <p>Participants were 15 (8 females and 7 males) healthy right-handed (augmented laterality index: <span class="NLM_disp-formula-image inline-formula rs_preserve">
    <noscript>
     <img src="/na101/home/literatum/publisher/tandf/journals/content/plcp21/2021/plcp21.v036.i02/23273798.2020.1803375/20210511/images/plcp_a_1803375_ilm0001.gif" alt="">
    </noscript><img src="//:0" alt="" class="mml-formula" data-formula-source="{&quot;type&quot; : &quot;image&quot;, &quot;src&quot; : &quot;/na101/home/literatum/publisher/tandf/journals/content/plcp21/2021/plcp21.v036.i02/23273798.2020.1803375/20210511/images/plcp_a_1803375_ilm0001.gif&quot;}"><span class="mml-formula"></span></span><span class="NLM_disp-formula inline-formula rs_preserve"><img src="//:0" alt="" data-formula-source="{&quot;type&quot; : &quot;mathjax&quot;}">
    <math>
     <mi>
      μ
     </mi><mo>
      =
     </mo><mn>
      85.7
     </mn>
    </math></span>, <span class="NLM_disp-formula-image inline-formula rs_preserve">
    <noscript>
     <img src="/na101/home/literatum/publisher/tandf/journals/content/plcp21/2021/plcp21.v036.i02/23273798.2020.1803375/20210511/images/plcp_a_1803375_ilm0002.gif" alt="">
    </noscript><img src="//:0" alt="" class="mml-formula" data-formula-source="{&quot;type&quot; : &quot;image&quot;, &quot;src&quot; : &quot;/na101/home/literatum/publisher/tandf/journals/content/plcp21/2021/plcp21.v036.i02/23273798.2020.1803375/20210511/images/plcp_a_1803375_ilm0002.gif&quot;}"><span class="mml-formula"></span></span><span class="NLM_disp-formula inline-formula rs_preserve"><img src="//:0" alt="" data-formula-source="{&quot;type&quot; : &quot;mathjax&quot;}">
    <math>
     <mi>
      σ
     </mi><mo>
      =
     </mo><mn>
      10.4
     </mn>
    </math></span>) and monolingual native speakers of German aged 20–42 years. They had normal hearing and did not report any history of neurological illness or drug abuse. They were paid for their participation after signing an informed consent form. Ethical permission for the study was granted by the ethics board of the University Hospital Erlangen (registration no. 161-18 B). For the questionnaire-based assessment and analysis of handedness, we used the Edinburgh Inventory (Oldfield, <span class="ref-lnk lazy-ref"><a data-rid="CIT0096" data-reflink="_i34 _i35 _i36" href="#"><span class="off-screen">Citation</span>1971</a></span>).</p>
 </div>
 <div id="S002-S2002" class="NLM_sec NLM_sec_level_2">
  <h3 class="section-heading-3" id="_i7">Speech stimuli and natural language text data</h3>
  <p>As natural language text data, we used the German novel <i>Gut gegen Nordwind</i> by Daniel Glattauer (@ <i>Deuticke im Paul Zsolnay Verlag</i>, Wien 2006) which was published by <i>Deuticke Verlag</i>. As speech stimuli, we used the corresponding audio book which was published by <i>Hörbuch Hamburg</i>. Both the novel and the audio book are available in stores, and the respective publishers gave us permission to use them for the present and future scientific studies.</p>
  <p>Book and audio book consist of a total number of 40,460 tokens (number of words) and 6117 types (number of unique words). The distribution of single word classes and bi-gram word class combinations occurring in the (audio) book were analysed and compared to a number of German reference corpora (Goldhahn et al., <span class="ref-lnk lazy-ref"><a data-rid="CIT0041" data-reflink="_i34 _i35 _i36" href="#"><span class="off-screen">Citation</span>2012</a></span>), and in addition, other German novels, by applying <i>part-of-speech (POS) tagging</i> (Jurafsky &amp; Martin, <span class="ref-lnk lazy-ref"><a data-rid="CIT0058" data-reflink="_i34 _i35 _i36" href="#"><span class="off-screen">Citation</span>2014</a></span>; Màrquez &amp; Rodríguez, <span class="ref-lnk lazy-ref"><a data-rid="CIT0086" data-reflink="_i34 _i35 _i36" href="#"><span class="off-screen">Citation</span>1998</a></span>; Ratnaparkhi, <span class="ref-lnk lazy-ref"><a data-rid="CIT0111" data-reflink="_i34 _i35 _i36" href="#"><span class="off-screen">Citation</span>1996</a></span>) as implemented in the python library <i>spaCy</i> (Explosion, <span class="ref-lnk lazy-ref"><a data-rid="CIT0030" data-reflink="_i34 _i35 _i36" href="#"><span class="off-screen">Citation</span>2017</a></span>). The similarities or dissimilarities, respectively, of all distributions are visualised using <i>multi-dimensional scaling (MDS)</i> (Cox &amp; Cox, <span class="ref-lnk lazy-ref"><a data-rid="CIT0018" data-reflink="_i34 _i35 _i36" href="#"><span class="off-screen">Citation</span>2008</a></span>; Kruskal, <span class="ref-lnk lazy-ref"><a data-rid="CIT0076" data-reflink="_i34 _i35 _i36" href="#"><span class="off-screen">Citation</span>1964</a></span>, <span class="ref-lnk lazy-ref"><a data-rid="CIT0077" data-reflink="_i34 _i35 _i36" href="#"><span class="off-screen">Citation</span>1978</a></span>; Torgerson, <span class="ref-lnk lazy-ref"><a data-rid="CIT0128" data-reflink="_i34 _i35 _i36" href="#"><span class="off-screen">Citation</span>1952</a></span>).</p>
  <p>The total duration of the audio book is approximately 4.5 h. For our study, we only used the first 40 min of the audio book, divided into 10 parts of approximately 4 min (<span class="NLM_disp-formula-image inline-formula rs_preserve">
    <noscript>
     <img src="/na101/home/literatum/publisher/tandf/journals/content/plcp21/2021/plcp21.v036.i02/23273798.2020.1803375/20210511/images/plcp_a_1803375_ilm0003.gif" alt="">
    </noscript><img src="//:0" alt="" class="mml-formula" data-formula-source="{&quot;type&quot; : &quot;image&quot;, &quot;src&quot; : &quot;/na101/home/literatum/publisher/tandf/journals/content/plcp21/2021/plcp21.v036.i02/23273798.2020.1803375/20210511/images/plcp_a_1803375_ilm0003.gif&quot;}"><span class="mml-formula"></span></span><span class="NLM_disp-formula inline-formula rs_preserve"><img src="//:0" alt="" data-formula-source="{&quot;type&quot; : &quot;mathjax&quot;}">
    <math>
     <mi>
      μ
     </mi><mo>
      =
     </mo><mn>
      245
     </mn><mspace width="thinmathspace"></mspace><mrow>
      <mi mathvariant="normal">
       s
      </mi>
     </mrow>
    </math></span>, <span class="NLM_disp-formula-image inline-formula rs_preserve">
    <noscript>
     <img src="/na101/home/literatum/publisher/tandf/journals/content/plcp21/2021/plcp21.v036.i02/23273798.2020.1803375/20210511/images/plcp_a_1803375_ilm0004.gif" alt="">
    </noscript><img src="//:0" alt="" class="mml-formula" data-formula-source="{&quot;type&quot; : &quot;image&quot;, &quot;src&quot; : &quot;/na101/home/literatum/publisher/tandf/journals/content/plcp21/2021/plcp21.v036.i02/23273798.2020.1803375/20210511/images/plcp_a_1803375_ilm0004.gif&quot;}"><span class="mml-formula"></span></span><span class="NLM_disp-formula inline-formula rs_preserve"><img src="//:0" alt="" data-formula-source="{&quot;type&quot; : &quot;mathjax&quot;}">
    <math>
     <mi>
      σ
     </mi><mo>
      =
     </mo><mn>
      39
     </mn><mspace width="thinmathspace"></mspace><mrow>
      <mi mathvariant="normal">
       s
      </mi>
     </mrow>
    </math></span>). This corresponds to approximately 6000 words, or 800 sentences, respectively, of spoken language, where each sentence consists on average of 7.5 words and has a mean duration of 3 s.</p>
  <p>In order to avoid cutting the text in the middle of a sentence or even in the middle of a word, we manually cut at paragraph boundaries, which resulted in more meaningful interruptions of the text. For the present study, only the first three sections (roughly 12 min of continuous speech) of the recordings and the corresponding measurements were analysed.</p>
 </div>
 <div id="S002-S2003" class="NLM_sec NLM_sec_level_2">
  <h3 class="section-heading-3" id="_i8">Stimulation protocol</h3>
  <p>The continuous speech from the audio book was presented in 10 subsequent parts (cf. above) at a sensory level of approximately 30–60 dB SPL. The actual loudness varied from participant to participant. It was chosen individually to ensure good intelligibility during the entire measurement but also to prevent it from being unpleasant. Simultaneously with auditory stimulation, a fixation cross at the centre of the screen was presented all the time to minimise artefacts from eye movements. After each audio book part, three multiple-choice questions on the content of the previously presented part were presented on the screen in order to test the participants' attention. Participants had to answer the questions by pressing previously defined keys on a MEG-compatible keyboard. MEG recording was stopped during the question blocks, since these short breaks were also used to allow participants to move and make themselves comfortable again. Furthermore, stimulation was interrupted for a short break of approximately 5 min after audio book parts number 4 and 7. The total duration of the protocol is approximately 1 h. The complete stimulation protocol is shown in <a href="#F0001">Figure 1</a>.</p>
  <div class="figure figureViewer" id="F0001">
   <div class="hidden figureViewerArticleInfo">
    <span class="figViewerTitle">Analysis of continuous neuronal activity evoked by natural speech with computational corpus linguistics methods</span>
    <div class="articleAuthors articleInfoSection">
     <div class="authorsHeading">
      All authors
     </div>
     <div class="authors">
      <a class="entryAuthor" href="/action/doSearch?Contrib=Schilling%2C+Achim"><span class="hlFld-ContribAuthor"></span></a><a href="/author/Schilling%2C+Achim"><span class="NLM_given-names">Achim</span> Schilling</a>, <a class="entryAuthor" href="/action/doSearch?Contrib=Tomasello%2C+Rosario"><span class="hlFld-ContribAuthor"></span></a><a href="/author/Tomasello%2C+Rosario"><span class="NLM_given-names">Rosario</span> Tomasello</a>, <a class="entryAuthor" href="/action/doSearch?Contrib=Henningsen-Schomers%2C+Malte+R"><span class="hlFld-ContribAuthor"></span></a><a href="/author/Henningsen-Schomers%2C+Malte+R"><span class="NLM_given-names">Malte R.</span> Henningsen-Schomers</a>, <a class="entryAuthor" href="/action/doSearch?Contrib=Zankl%2C+Alexandra"><span class="hlFld-ContribAuthor"></span></a><a href="/author/Zankl%2C+Alexandra"><span class="NLM_given-names">Alexandra</span> Zankl</a>, <a class="entryAuthor" href="/action/doSearch?Contrib=Surendra%2C+Kishore"><span class="hlFld-ContribAuthor"></span></a><a href="/author/Surendra%2C+Kishore"><span class="NLM_given-names">Kishore</span> Surendra</a>, <a class="entryAuthor" href="/action/doSearch?Contrib=Haller%2C+Martin"><span class="hlFld-ContribAuthor"></span></a><a href="/author/Haller%2C+Martin"><span class="NLM_given-names">Martin</span> Haller</a>, <a class="entryAuthor" href="/action/doSearch?Contrib=Karl%2C+Valerie"><span class="hlFld-ContribAuthor"></span></a><a href="/author/Karl%2C+Valerie"><span class="NLM_given-names">Valerie</span> Karl</a>, <a class="entryAuthor" href="/action/doSearch?Contrib=Uhrig%2C+Peter"><span class="hlFld-ContribAuthor"></span></a><a href="/author/Uhrig%2C+Peter"><span class="NLM_given-names">Peter</span> Uhrig</a>, <a class="entryAuthor" href="/action/doSearch?Contrib=Maier%2C+Andreas"><span class="hlFld-ContribAuthor"></span></a><a href="/author/Maier%2C+Andreas"><span class="NLM_given-names">Andreas</span> Maier</a> &amp; <a class="entryAuthor" href="/action/doSearch?Contrib=Krauss%2C+Patrick"><span class="hlFld-ContribAuthor"></span></a><a href="/author/Krauss%2C+Patrick"><span class="NLM_given-names">Patrick</span> Krauss</a> <a href="https://orcid.org/0000-0002-6611-7733"><img src="/templates/jsp/images/orcid.png"></a>
     </div>
    </div>
    <div class="articleLowerInfo articleInfoSection">
     <div class="articleLowerInfoSection articleInfoDOI">
      <a href="https://doi.org/10.1080/23273798.2020.1803375">https://doi.org/10.1080/23273798.2020.1803375</a>
     </div>
     <div class="articleInfoPublicationDate articleLowerInfoSection border">
      <h6>Published online:</h6>10 August 2020
     </div>
    </div>
   </div>
   <div class="figureThumbnailContainer">
    <div class="figureInfo">
     <div class="short-legend">
      <p class="captionText"><span class="captionLabel">Figure 1. </span> Stimulation protocol. The total duration of the protocol was approximately 1 h. The audio book was presented in 10 subsequent parts with an average duration of 4 min. After each part, three multiple-choice questions on the content of the previous part of the audio book were presented. After audio book parts number 4 and 7, stimulation was interrupted for a short break of approximately 5 min.</p>
     </div>
    </div><a href="#" class="thumbnail" aria-label="thumbnail image"><img id="F0001image" src="/na101/home/literatum/publisher/tandf/journals/content/plcp21/2021/plcp21.v036.i02/23273798.2020.1803375/20210511/images/medium/plcp_a_1803375_f0001_oc.jpg" loading="lazy" height="174" width="500"></a>
    <div class="figureDownloadOptions">
     <a href="#" class="downloadBtn btn btn-sm" role="button">Display full size</a>
    </div>
   </div>
  </div>
  <div class="hidden rs_skip" id="fig-description-F0001">
   <p class="captionText"><span class="captionLabel">Figure 1. </span> Stimulation protocol. The total duration of the protocol was approximately 1 h. The audio book was presented in 10 subsequent parts with an average duration of 4 min. After each part, three multiple-choice questions on the content of the previous part of the audio book were presented. After audio book parts number 4 and 7, stimulation was interrupted for a short break of approximately 5 min.</p>
  </div>
  <div class="hidden rs_skip" id="figureFootNote-F0001">
   <div class="figureFootNote-F0001"></div>
  </div>
  <p></p>
 </div>
 <div id="S002-S2004" class="NLM_sec NLM_sec_level_2">
  <h3 class="section-heading-3" id="_i10">Generation of trigger pulses with forced alignment</h3>
  <p>In order to automatically create trigger pulses for both, the synchronisation of the speech stream with the MEG recordings, and to mark the boundaries of words, phonemes, and silence for further segmentation of the continuous data streams, <i>forced alignment</i> (Katsamanis et al., <span class="ref-lnk lazy-ref"><a data-rid="CIT0060" data-reflink="_i34 _i35 _i36" href="#"><span class="off-screen">Citation</span>2011</a></span>; Moreno et al., <span class="ref-lnk lazy-ref"><a data-rid="CIT0092" data-reflink="_i34 _i35 _i36" href="#"><span class="off-screen">Citation</span>1998</a></span>; Yuan &amp; Liberman, <span class="ref-lnk lazy-ref"><a data-rid="CIT0137" data-reflink="_i34 _i35 _i36" href="#"><span class="off-screen">Citation</span>2009</a></span>) was applied to the text and recording. For this study, we used the free web service <i>WebMAUS</i> (Kisler et al., <span class="ref-lnk lazy-ref"><a data-rid="CIT0064" data-reflink="_i34 _i35 _i36" href="#"><span class="off-screen">Citation</span>2017</a></span>; Schiel, <span class="ref-lnk lazy-ref"><a data-rid="CIT0113" data-reflink="_i34 _i35 _i36" href="#"><span class="off-screen">Citation</span>1999</a></span>). It takes a wave file containing the speech signal, and a corresponding text file as input and gives three files as output: the time tags of word boundaries, a phonetic transcription of the text file, and the time tags of phone boundaries. Even though forced alignment is a fast and reliable method for the automatic phonetic transcription of continuous speech, we carried out random manual inspections in order to ensure that the method actually worked correctly. Although forced alignment is not <span class="NLM_disp-formula-image inline-formula rs_preserve">
    <noscript>
     <img src="/na101/home/literatum/publisher/tandf/journals/content/plcp21/2021/plcp21.v036.i02/23273798.2020.1803375/20210511/images/plcp_a_1803375_ilm0005.gif" alt="">
    </noscript><img src="//:0" alt="" class="mml-formula" data-formula-source="{&quot;type&quot; : &quot;image&quot;, &quot;src&quot; : &quot;/na101/home/literatum/publisher/tandf/journals/content/plcp21/2021/plcp21.v036.i02/23273798.2020.1803375/20210511/images/plcp_a_1803375_ilm0005.gif&quot;}"><span class="mml-formula"></span></span><span class="NLM_disp-formula inline-formula rs_preserve"><img src="//:0" alt="" data-formula-source="{&quot;type&quot; : &quot;mathjax&quot;}">
    <math>
     <mn>
      100
     </mn><mtext>
      %
     </mtext>
    </math></span> reliable, manual spot checks found no errors in our alignment. Of course, the high-quality recording of an audio book is among the best possible inputs for such software.</p>
  <p>For simplicity, we only used the time tags of word boundaries in this study. However, a more fine grained analysis on the level of speech sounds could easily be performed retrospectively, since the time tags of beginning and ending of a given word correspond to the beginning of the word's first phone and the ending of the word's last phone, respectively. Thus, the two lists containing the time tags of the word and phoneme boundaries can easily be aligned with each other.</p>
 </div>
 <div id="S002-S2005" class="NLM_sec NLM_sec_level_2">
  <h3 class="section-heading-3" id="_i11">Speech presentation and synchronisation with MEG</h3>
  <p>The speech signal was presented using a custom-made setup (<a href="#F0002">Figure 2</a>). It consists of a stimulation computer connected to an external USB sound device (Asus Xonar MKII, 7.1 channels) providing five analogue outputs. The first and second analogue outputs are connected to an audio amplifier (AIWA, XA-003), where the first output is connected in parallel to an analogue input channel of the MEG data logger in order to enable an exact alignment of the presented stimuli and the recorded MEG signals (cf. <a href="#F0002">Figure 2</a>(a)). In addition, the third analogue output of the sound device is used to feed the trigger pulses derived from forced alignment into the MEG recording system via another analogue input channel. In doing so, our setup prevents temporal jittering of the presented signal caused by multi-threading of the stimulation PC's operating system, for instance. For an overview of the wiring scheme of all devices, see <a href="#F0002">Figure 2</a>(a).</p>
  <div class="figure figureViewer" id="F0002">
   <div class="hidden figureViewerArticleInfo">
    <span class="figViewerTitle">Analysis of continuous neuronal activity evoked by natural speech with computational corpus linguistics methods</span>
    <div class="articleAuthors articleInfoSection">
     <div class="authorsHeading">
      All authors
     </div>
     <div class="authors">
      <a class="entryAuthor" href="/action/doSearch?Contrib=Schilling%2C+Achim"><span class="hlFld-ContribAuthor"></span></a><a href="/author/Schilling%2C+Achim"><span class="NLM_given-names">Achim</span> Schilling</a>, <a class="entryAuthor" href="/action/doSearch?Contrib=Tomasello%2C+Rosario"><span class="hlFld-ContribAuthor"></span></a><a href="/author/Tomasello%2C+Rosario"><span class="NLM_given-names">Rosario</span> Tomasello</a>, <a class="entryAuthor" href="/action/doSearch?Contrib=Henningsen-Schomers%2C+Malte+R"><span class="hlFld-ContribAuthor"></span></a><a href="/author/Henningsen-Schomers%2C+Malte+R"><span class="NLM_given-names">Malte R.</span> Henningsen-Schomers</a>, <a class="entryAuthor" href="/action/doSearch?Contrib=Zankl%2C+Alexandra"><span class="hlFld-ContribAuthor"></span></a><a href="/author/Zankl%2C+Alexandra"><span class="NLM_given-names">Alexandra</span> Zankl</a>, <a class="entryAuthor" href="/action/doSearch?Contrib=Surendra%2C+Kishore"><span class="hlFld-ContribAuthor"></span></a><a href="/author/Surendra%2C+Kishore"><span class="NLM_given-names">Kishore</span> Surendra</a>, <a class="entryAuthor" href="/action/doSearch?Contrib=Haller%2C+Martin"><span class="hlFld-ContribAuthor"></span></a><a href="/author/Haller%2C+Martin"><span class="NLM_given-names">Martin</span> Haller</a>, <a class="entryAuthor" href="/action/doSearch?Contrib=Karl%2C+Valerie"><span class="hlFld-ContribAuthor"></span></a><a href="/author/Karl%2C+Valerie"><span class="NLM_given-names">Valerie</span> Karl</a>, <a class="entryAuthor" href="/action/doSearch?Contrib=Uhrig%2C+Peter"><span class="hlFld-ContribAuthor"></span></a><a href="/author/Uhrig%2C+Peter"><span class="NLM_given-names">Peter</span> Uhrig</a>, <a class="entryAuthor" href="/action/doSearch?Contrib=Maier%2C+Andreas"><span class="hlFld-ContribAuthor"></span></a><a href="/author/Maier%2C+Andreas"><span class="NLM_given-names">Andreas</span> Maier</a> &amp; <a class="entryAuthor" href="/action/doSearch?Contrib=Krauss%2C+Patrick"><span class="hlFld-ContribAuthor"></span></a><a href="/author/Krauss%2C+Patrick"><span class="NLM_given-names">Patrick</span> Krauss</a> <a href="https://orcid.org/0000-0002-6611-7733"><img src="/templates/jsp/images/orcid.png"></a>
     </div>
    </div>
    <div class="articleLowerInfo articleInfoSection">
     <div class="articleLowerInfoSection articleInfoDOI">
      <a href="https://doi.org/10.1080/23273798.2020.1803375">https://doi.org/10.1080/23273798.2020.1803375</a>
     </div>
     <div class="articleInfoPublicationDate articleLowerInfoSection border">
      <h6>Published online:</h6>10 August 2020
     </div>
    </div>
   </div>
   <div class="figureThumbnailContainer">
    <div class="figureInfo">
     <div class="short-legend">
      <p class="captionText"><span class="captionLabel">Figure 2. </span> Setup configuration. (a) Wiring scheme of the different devices. (b) The speech sound is transmitted into the magnetically shielded chamber via a custom-made construction consisting of two loudspeakers (1) which are coupled to silicone funnels and (2) each connected to a flexible tube. (c) Through a small whole in the magnetically shielded chamber (1), speech sound is transmitted via the two flexible tubes (2).</p>
     </div>
    </div><a href="#" class="thumbnail" aria-label="thumbnail image"><img id="F0002image" src="/na101/home/literatum/publisher/tandf/journals/content/plcp21/2021/plcp21.v036.i02/23273798.2020.1803375/20210511/images/medium/plcp_a_1803375_f0002_oc.jpg" loading="lazy" height="386" width="500"></a>
    <div class="figureDownloadOptions">
     <a href="#" class="downloadBtn btn btn-sm" role="button">Display full size</a>
    </div>
   </div>
  </div>
  <div class="hidden rs_skip" id="fig-description-F0002">
   <p class="captionText"><span class="captionLabel">Figure 2. </span> Setup configuration. (a) Wiring scheme of the different devices. (b) The speech sound is transmitted into the magnetically shielded chamber via a custom-made construction consisting of two loudspeakers (1) which are coupled to silicone funnels and (2) each connected to a flexible tube. (c) Through a small whole in the magnetically shielded chamber (1), speech sound is transmitted via the two flexible tubes (2).</p>
  </div>
  <div class="hidden rs_skip" id="figureFootNote-F0002">
   <div class="figureFootNote-F0002"></div>
  </div>
  <p></p>
  <p>The speech sound was transmitted into the magnetically shielded MEG chamber to the participants' ears via a custom-made device consisting of two loudspeakers (Pioneer, TS-G1020F) which are coupled to silicone funnels each connected to a flexible tube of <span class="NLM_disp-formula-image inline-formula rs_preserve">
    <noscript>
     <img src="/na101/home/literatum/publisher/tandf/journals/content/plcp21/2021/plcp21.v036.i02/23273798.2020.1803375/20210511/images/plcp_a_1803375_ilm0006.gif" alt="">
    </noscript><img src="//:0" alt="" class="mml-formula" data-formula-source="{&quot;type&quot; : &quot;image&quot;, &quot;src&quot; : &quot;/na101/home/literatum/publisher/tandf/journals/content/plcp21/2021/plcp21.v036.i02/23273798.2020.1803375/20210511/images/plcp_a_1803375_ilm0006.gif&quot;}"><span class="mml-formula"></span></span><span class="NLM_disp-formula inline-formula rs_preserve"><img src="//:0" alt="" data-formula-source="{&quot;type&quot; : &quot;mathjax&quot;}">
    <math>
     <mrow>
      <mo>
       ≈
      </mo>
     </mrow><mn>
      2
     </mn><mspace width="thinmathspace"></mspace><mrow>
      <mi mathvariant="normal">
       m
      </mi>
     </mrow>
    </math></span> length and with an inner diameter of <span class="NLM_disp-formula-image inline-formula rs_preserve">
    <noscript>
     <img src="/na101/home/literatum/publisher/tandf/journals/content/plcp21/2021/plcp21.v036.i02/23273798.2020.1803375/20210511/images/plcp_a_1803375_ilm0007.gif" alt="">
    </noscript><img src="//:0" alt="" class="mml-formula" data-formula-source="{&quot;type&quot; : &quot;image&quot;, &quot;src&quot; : &quot;/na101/home/literatum/publisher/tandf/journals/content/plcp21/2021/plcp21.v036.i02/23273798.2020.1803375/20210511/images/plcp_a_1803375_ilm0007.gif&quot;}"><span class="mml-formula"></span></span><span class="NLM_disp-formula inline-formula rs_preserve"><img src="//:0" alt="" data-formula-source="{&quot;type&quot; : &quot;mathjax&quot;}">
    <math>
     <mrow>
      <mo>
       ≈
      </mo>
     </mrow><mn>
      2
     </mn><mspace width="thinmathspace"></mspace><mrow>
      <mi mathvariant="normal">
       c
      </mi>
      <mi mathvariant="normal">
       m
      </mi>
     </mrow>
    </math></span> (<a href="#F0002">Figure 2</a>(b)). These tubes are led through a small hole in the magnetically shielded chamber to prevent artefacts produced by interfering magnetic fields generated by the loudspeakers (<a href="#F0002">Figure 2</a>(c)). We carried out calibration tests to ensure that the acoustical distortions caused by the tube system do not affect speech intelligibility. Furthermore, due to the length of the tubes and the speed of sound, there is a constant time delay from the generation of sound to the arrival of the sound at the participant of <span class="NLM_disp-formula-image inline-formula rs_preserve">
    <noscript>
     <img src="/na101/home/literatum/publisher/tandf/journals/content/plcp21/2021/plcp21.v036.i02/23273798.2020.1803375/20210511/images/plcp_a_1803375_ilm0008.gif" alt="">
    </noscript><img src="//:0" alt="" class="mml-formula" data-formula-source="{&quot;type&quot; : &quot;image&quot;, &quot;src&quot; : &quot;/na101/home/literatum/publisher/tandf/journals/content/plcp21/2021/plcp21.v036.i02/23273798.2020.1803375/20210511/images/plcp_a_1803375_ilm0008.gif&quot;}"><span class="mml-formula"></span></span><span class="NLM_disp-formula inline-formula rs_preserve"><img src="//:0" alt="" data-formula-source="{&quot;type&quot; : &quot;mathjax&quot;}">
    <math>
     <mrow>
      <mo>
       ≈
      </mo>
     </mrow><mn>
      6
     </mn><mspace width="thinmathspace"></mspace><mrow>
      <mi mathvariant="normal">
       m
      </mi>
      <mi mathvariant="normal">
       s
      </mi>
     </mrow>
    </math></span>, which we took into account for the alignment described below.</p>
  <p>The stimulation software is implemented using the programming language <i>Python 3.6</i>, together with Python's sound device library, the <i>PsychoPy</i> library (Peirce, <span class="ref-lnk lazy-ref"><a data-rid="CIT0099" data-reflink="_i34 _i35 _i36" href="#"><span class="off-screen">Citation</span>2007</a></span>, <span class="ref-lnk lazy-ref"><a data-rid="CIT0100" data-reflink="_i34 _i35 _i36" href="#"><span class="off-screen">Citation</span>2009</a></span>) for the stimulation protocol, and the <i>NumPy</i> library for basic mathematical and numerical operations.</p>
 </div>
 <div id="S002-S2006" class="NLM_sec NLM_sec_level_2">
  <h3 class="section-heading-3" id="_i13">Magnetoencephalography and data processing</h3>
  <p>MEG data (248 magnetometers, 4D Neuroimaging, San Diego, CA, USA) were recorded (1017.25 Hz sampling rate, filtering: 0.1–200 Hz analogue band pass, supine position, eyes open) during speech stimulation. Positions of five landmarks (nasion, LPA, RPA, Cz, inion) were acquired using an integrated digitiser (Polhemus, Colchester, Vermont, Canada). MEG data were corrected for environmental noise using a calibrated linear weighting of 23 reference sensors (manufacturers algorithm, 4D Neuroimaging, San Diego, CA, USA).</p>
  <p>Further processing was performed using the Python library <i>MNE</i> (Gramfort et al., <span class="ref-lnk lazy-ref"><a data-rid="CIT0042" data-reflink="_i34 _i35 _i36" href="#"><span class="off-screen">Citation</span>2013</a></span>, <span class="ref-lnk lazy-ref"><a data-rid="CIT0043" data-reflink="_i34 _i35 _i36" href="#"><span class="off-screen">Citation</span>2014</a></span>). Data were digitally filtered offline (1–10 Hz bandpass for ERF analyses; 50 Hz notch on for power spectra analysis) and downsampled to a sampling rate of 1000 Hz. MEG sensor positions were co-registered to the ICBM-152 standard head model (Fuchs et al., <span class="ref-lnk lazy-ref"><a data-rid="CIT0036" data-reflink="_i34 _i35 _i36" href="#"><span class="off-screen">Citation</span>2002</a></span>) and atlas 19–21 (Evans et al., <span class="ref-lnk lazy-ref"><a data-rid="CIT0027" data-reflink="_i34 _i35 _i36" href="#"><span class="off-screen">Citation</span>2012</a></span>), as individual MRI data sets for the participants were not available. Furthermore, recordings were corrected for eye blinks and electrocardiography artefacts based on signal space projection of averaged artefact patterns, as implemented in <i>MNE</i> (Gramfort et al., <span class="ref-lnk lazy-ref"><a data-rid="CIT0042" data-reflink="_i34 _i35 _i36" href="#"><span class="off-screen">Citation</span>2013</a></span>, <span class="ref-lnk lazy-ref"><a data-rid="CIT0043" data-reflink="_i34 _i35 _i36" href="#"><span class="off-screen">Citation</span>2014</a></span>).</p>
  <p>Additionally, we performed an independent component analysis (ICA) and deleted the first two independent components of the data, to further improve data quality. However, it appears that this processing step does not affect the observed differences of neural responses to function and content words (Figures <a href="#F0008">8</a> and S8).</p>
  <p>Trials with amplitudes higher than <span class="NLM_disp-formula-image inline-formula rs_preserve">
    <noscript>
     <img src="/na101/home/literatum/publisher/tandf/journals/content/plcp21/2021/plcp21.v036.i02/23273798.2020.1803375/20210511/images/plcp_a_1803375_ilm0009.gif" alt="">
    </noscript><img src="//:0" alt="" class="mml-formula" data-formula-source="{&quot;type&quot; : &quot;image&quot;, &quot;src&quot; : &quot;/na101/home/literatum/publisher/tandf/journals/content/plcp21/2021/plcp21.v036.i02/23273798.2020.1803375/20210511/images/plcp_a_1803375_ilm0009.gif&quot;}"><span class="mml-formula"></span></span><span class="NLM_disp-formula inline-formula rs_preserve"><img src="//:0" alt="" data-formula-source="{&quot;type&quot; : &quot;mathjax&quot;}">
    <math>
     <mn>
      2
     </mn><mo>
      ⋅
     </mo><msup>
      <mn>
       10
      </mn>
      <mrow>
       <mo>
        −
       </mo>
       <mn>
        12
       </mn>
      </mrow>
     </msup>
    </math></span> T would have been rejected, as they were supposed to arise from artefacts. However, none of the trials fit this condition, and hence no trail was rejected.</p>
  <p>For this study, we restricted our analyses to sensor space, and did not perform source localisation in analogy to other ERF studies (Hauk et al., <span class="ref-lnk lazy-ref"><a data-rid="CIT0050" data-reflink="_i34 _i35 _i36" href="#"><span class="off-screen">Citation</span>2006</a></span>; Højlund et al., <span class="ref-lnk lazy-ref"><a data-rid="CIT0055" data-reflink="_i34 _i35 _i36" href="#"><span class="off-screen">Citation</span>2019</a></span>; Shtyrov &amp; Pulvermüller, <span class="ref-lnk lazy-ref"><a data-rid="CIT0120" data-reflink="_i34 _i35 _i36" href="#"><span class="off-screen">Citation</span>2007</a></span>).</p>
 </div>
 <div id="S002-S2007" class="NLM_sec NLM_sec_level_2">
  <h3 class="section-heading-3" id="_i14">Alignment, segmentation and tagging</h3>
  <p>Since we have both, the original audio book wave file together with the time tags of word boundaries from forced alignment (<a href="#F0003">Figure 3</a>(a)), and the corresponding recordings of two analogue auxiliary channels of the MEG (<a href="#F0003">Figure 3</a>(b)), all 248 MEG recording channels could easily be aligned offline with the speech stream (<a href="#F0003">Figure 3</a>(c)). Subsequently, the continuous multi-channel MEG recordings were segmented using the time tags as boundaries and labelled with the corresponding types, in our case individual words (<a href="#F0004">Figure 4</a>). Note that, in principle, the process of segmentation can also be performed at different levels of granularity. For instance, using the time tags of phone boundaries would result in a more fine-grained segmentation, whereas grouping several words together to n-grams with appropriate labels to larger linguistic units (i.e. collocations, phrases, clauses, sentences) would result in a more coarse-grained segmentation.</p>
  <div class="figure figureViewer" id="F0003">
   <div class="hidden figureViewerArticleInfo">
    <span class="figViewerTitle">Analysis of continuous neuronal activity evoked by natural speech with computational corpus linguistics methods</span>
    <div class="articleAuthors articleInfoSection">
     <div class="authorsHeading">
      All authors
     </div>
     <div class="authors">
      <a class="entryAuthor" href="/action/doSearch?Contrib=Schilling%2C+Achim"><span class="hlFld-ContribAuthor"></span></a><a href="/author/Schilling%2C+Achim"><span class="NLM_given-names">Achim</span> Schilling</a>, <a class="entryAuthor" href="/action/doSearch?Contrib=Tomasello%2C+Rosario"><span class="hlFld-ContribAuthor"></span></a><a href="/author/Tomasello%2C+Rosario"><span class="NLM_given-names">Rosario</span> Tomasello</a>, <a class="entryAuthor" href="/action/doSearch?Contrib=Henningsen-Schomers%2C+Malte+R"><span class="hlFld-ContribAuthor"></span></a><a href="/author/Henningsen-Schomers%2C+Malte+R"><span class="NLM_given-names">Malte R.</span> Henningsen-Schomers</a>, <a class="entryAuthor" href="/action/doSearch?Contrib=Zankl%2C+Alexandra"><span class="hlFld-ContribAuthor"></span></a><a href="/author/Zankl%2C+Alexandra"><span class="NLM_given-names">Alexandra</span> Zankl</a>, <a class="entryAuthor" href="/action/doSearch?Contrib=Surendra%2C+Kishore"><span class="hlFld-ContribAuthor"></span></a><a href="/author/Surendra%2C+Kishore"><span class="NLM_given-names">Kishore</span> Surendra</a>, <a class="entryAuthor" href="/action/doSearch?Contrib=Haller%2C+Martin"><span class="hlFld-ContribAuthor"></span></a><a href="/author/Haller%2C+Martin"><span class="NLM_given-names">Martin</span> Haller</a>, <a class="entryAuthor" href="/action/doSearch?Contrib=Karl%2C+Valerie"><span class="hlFld-ContribAuthor"></span></a><a href="/author/Karl%2C+Valerie"><span class="NLM_given-names">Valerie</span> Karl</a>, <a class="entryAuthor" href="/action/doSearch?Contrib=Uhrig%2C+Peter"><span class="hlFld-ContribAuthor"></span></a><a href="/author/Uhrig%2C+Peter"><span class="NLM_given-names">Peter</span> Uhrig</a>, <a class="entryAuthor" href="/action/doSearch?Contrib=Maier%2C+Andreas"><span class="hlFld-ContribAuthor"></span></a><a href="/author/Maier%2C+Andreas"><span class="NLM_given-names">Andreas</span> Maier</a> &amp; <a class="entryAuthor" href="/action/doSearch?Contrib=Krauss%2C+Patrick"><span class="hlFld-ContribAuthor"></span></a><a href="/author/Krauss%2C+Patrick"><span class="NLM_given-names">Patrick</span> Krauss</a> <a href="https://orcid.org/0000-0002-6611-7733"><img src="/templates/jsp/images/orcid.png"></a>
     </div>
    </div>
    <div class="articleLowerInfo articleInfoSection">
     <div class="articleLowerInfoSection articleInfoDOI">
      <a href="https://doi.org/10.1080/23273798.2020.1803375">https://doi.org/10.1080/23273798.2020.1803375</a>
     </div>
     <div class="articleInfoPublicationDate articleLowerInfoSection border">
      <h6>Published online:</h6>10 August 2020
     </div>
    </div>
   </div>
   <div class="figureThumbnailContainer">
    <div class="figureInfo">
     <div class="short-legend">
      <p class="captionText"><span class="captionLabel">Figure 3. </span> Alignment of speech stream and MEG signal. (a) Sample audio book wave file together with time tags of word boundaries from forced alignment. (b) Corresponding recordings of two analogue auxiliary channels of the MEG. (c) Alignment of data streams from a and b, together with one sample MEG channel.</p>
     </div>
    </div><a href="#" class="thumbnail" aria-label="thumbnail image"><img id="F0003image" src="/na101/home/literatum/publisher/tandf/journals/content/plcp21/2021/plcp21.v036.i02/23273798.2020.1803375/20210511/images/medium/plcp_a_1803375_f0003_oc.jpg" loading="lazy" height="388" width="500"></a>
    <div class="figureDownloadOptions">
     <a href="#" class="downloadBtn btn btn-sm" role="button">Display full size</a>
    </div>
   </div>
  </div>
  <div class="hidden rs_skip" id="fig-description-F0003">
   <p class="captionText"><span class="captionLabel">Figure 3. </span> Alignment of speech stream and MEG signal. (a) Sample audio book wave file together with time tags of word boundaries from forced alignment. (b) Corresponding recordings of two analogue auxiliary channels of the MEG. (c) Alignment of data streams from a and b, together with one sample MEG channel.</p>
  </div>
  <div class="hidden rs_skip" id="figureFootNote-F0003">
   <div class="figureFootNote-F0003"></div>
  </div>
  <div class="figure figureViewer" id="F0004">
   <div class="hidden figureViewerArticleInfo">
    <span class="figViewerTitle">Analysis of continuous neuronal activity evoked by natural speech with computational corpus linguistics methods</span>
    <div class="articleAuthors articleInfoSection">
     <div class="authorsHeading">
      All authors
     </div>
     <div class="authors">
      <a class="entryAuthor" href="/action/doSearch?Contrib=Schilling%2C+Achim"><span class="hlFld-ContribAuthor"></span></a><a href="/author/Schilling%2C+Achim"><span class="NLM_given-names">Achim</span> Schilling</a>, <a class="entryAuthor" href="/action/doSearch?Contrib=Tomasello%2C+Rosario"><span class="hlFld-ContribAuthor"></span></a><a href="/author/Tomasello%2C+Rosario"><span class="NLM_given-names">Rosario</span> Tomasello</a>, <a class="entryAuthor" href="/action/doSearch?Contrib=Henningsen-Schomers%2C+Malte+R"><span class="hlFld-ContribAuthor"></span></a><a href="/author/Henningsen-Schomers%2C+Malte+R"><span class="NLM_given-names">Malte R.</span> Henningsen-Schomers</a>, <a class="entryAuthor" href="/action/doSearch?Contrib=Zankl%2C+Alexandra"><span class="hlFld-ContribAuthor"></span></a><a href="/author/Zankl%2C+Alexandra"><span class="NLM_given-names">Alexandra</span> Zankl</a>, <a class="entryAuthor" href="/action/doSearch?Contrib=Surendra%2C+Kishore"><span class="hlFld-ContribAuthor"></span></a><a href="/author/Surendra%2C+Kishore"><span class="NLM_given-names">Kishore</span> Surendra</a>, <a class="entryAuthor" href="/action/doSearch?Contrib=Haller%2C+Martin"><span class="hlFld-ContribAuthor"></span></a><a href="/author/Haller%2C+Martin"><span class="NLM_given-names">Martin</span> Haller</a>, <a class="entryAuthor" href="/action/doSearch?Contrib=Karl%2C+Valerie"><span class="hlFld-ContribAuthor"></span></a><a href="/author/Karl%2C+Valerie"><span class="NLM_given-names">Valerie</span> Karl</a>, <a class="entryAuthor" href="/action/doSearch?Contrib=Uhrig%2C+Peter"><span class="hlFld-ContribAuthor"></span></a><a href="/author/Uhrig%2C+Peter"><span class="NLM_given-names">Peter</span> Uhrig</a>, <a class="entryAuthor" href="/action/doSearch?Contrib=Maier%2C+Andreas"><span class="hlFld-ContribAuthor"></span></a><a href="/author/Maier%2C+Andreas"><span class="NLM_given-names">Andreas</span> Maier</a> &amp; <a class="entryAuthor" href="/action/doSearch?Contrib=Krauss%2C+Patrick"><span class="hlFld-ContribAuthor"></span></a><a href="/author/Krauss%2C+Patrick"><span class="NLM_given-names">Patrick</span> Krauss</a> <a href="https://orcid.org/0000-0002-6611-7733"><img src="/templates/jsp/images/orcid.png"></a>
     </div>
    </div>
    <div class="articleLowerInfo articleInfoSection">
     <div class="articleLowerInfoSection articleInfoDOI">
      <a href="https://doi.org/10.1080/23273798.2020.1803375">https://doi.org/10.1080/23273798.2020.1803375</a>
     </div>
     <div class="articleInfoPublicationDate articleLowerInfoSection border">
      <h6>Published online:</h6>10 August 2020
     </div>
    </div>
   </div>
   <div class="figureThumbnailContainer">
    <div class="figureInfo">
     <div class="short-legend">
      <p class="captionText"><span class="captionLabel">Figure 4. </span> Segmentation of speech stream and MEG signal. After alignment, the continuous wave file (top panel) and multi-channel MEG recordings (bottom panel) are segmented using the time tags from forced alignment as boundaries and labelled with the corresponding types, i.e. words.</p>
     </div>
    </div><a href="#" class="thumbnail" aria-label="thumbnail image"><img id="F0004image" src="/na101/home/literatum/publisher/tandf/journals/content/plcp21/2021/plcp21.v036.i02/23273798.2020.1803375/20210511/images/medium/plcp_a_1803375_f0004_oc.jpg" loading="lazy" height="344" width="500"></a>
    <div class="figureDownloadOptions">
     <a href="#" class="downloadBtn btn btn-sm" role="button">Display full size</a>
    </div>
   </div>
  </div>
  <div class="hidden rs_skip" id="fig-description-F0004">
   <p class="captionText"><span class="captionLabel">Figure 4. </span> Segmentation of speech stream and MEG signal. After alignment, the continuous wave file (top panel) and multi-channel MEG recordings (bottom panel) are segmented using the time tags from forced alignment as boundaries and labelled with the corresponding types, i.e. words.</p>
  </div>
  <div class="hidden rs_skip" id="figureFootNote-F0004">
   <div class="figureFootNote-F0004"></div>
  </div>
  <p></p>
  <p>For the analysis of function and content words, we additionally applied POS tagging (Jurafsky &amp; Martin, <span class="ref-lnk lazy-ref"><a data-rid="CIT0058" data-reflink="_i34 _i35 _i36" href="#"><span class="off-screen">Citation</span>2014</a></span>; Màrquez &amp; Rodríguez, <span class="ref-lnk lazy-ref"><a data-rid="CIT0086" data-reflink="_i34 _i35 _i36" href="#"><span class="off-screen">Citation</span>1998</a></span>; Ratnaparkhi, <span class="ref-lnk lazy-ref"><a data-rid="CIT0111" data-reflink="_i34 _i35 _i36" href="#"><span class="off-screen">Citation</span>1996</a></span>) using spaCy (Explosion, <span class="ref-lnk lazy-ref"><a data-rid="CIT0030" data-reflink="_i34 _i35 _i36" href="#"><span class="off-screen">Citation</span>2017</a></span>) to assign word classes (e.g. nouns, verbs, adjectives, conjunctions, determiners, prepositions) to the individual words. According to Ortmann et al. (<span class="ref-lnk lazy-ref"><a data-rid="CIT0097" data-reflink="_i34 _i35 _i36" href="#"><span class="off-screen">Citation</span>2019</a></span>), spaCy's accuracy for POS tagging of German texts is 92.5%. This value could be confirmed by two German native speakers who cross-checked a random sample of sentences that have been POS tagged using spaCy. However, the most frequent errors observed in spaCy are confusions of nouns and proper names, adverbs and adverbial adjectives, and of different verb forms (Ortmann et al., <span class="ref-lnk lazy-ref"><a data-rid="CIT0097" data-reflink="_i34 _i35 _i36" href="#"><span class="off-screen">Citation</span>2019</a></span>). Since all these word classes belong to the domain of content words, these confusions are irrelevant for the classification in function and content words analysed in this study. So that the accuracy for this distinction is expected to be much higher.</p>
 </div>
 <div id="S002-S2008" class="NLM_sec NLM_sec_level_2">
  <h3 class="section-heading-3" id="_i17">Event-related fields</h3>
  <p>In order to provide the proof-of-principle of our approach, we analysed event-related fields (ERF) evoked by word onsets (Figure <a href="#F0007">7</a>). Since the continuous MEG signals of all 248 channels are already segmented according to word boundaries, we can compute ERFs of word onsets for each channel by simply averaging the pre-processed signals over the word tokens in our database. Here, we included only those words that follow a short pause, instead of using all words occurring in the data set. Thus, there is a short period of silence, ranging from approximately 50 ms to 1.5 s, before the actual word onsets which improves signal quality, yet with the drawback that only a fraction of all tokens can be used. However, there were still 291 remaining events, baseline corrected, within the first three parts of the audio book, corresponding to approx. 12 min of continuous speech, that fit this condition.</p>
  <p>In addition, we also analysed ERFs evoked by prototypical content words (nouns, verbs, adjectives) and compared them with ERFs evoked by function words (determiners, prepositions, conjunctions) (<a href="#F0008">Figure 8</a>). Again, we included only those words that follow a short interval of silence, instead of using all words occurring in the data set. Within the first three parts of the audio book, this resulted in 81 remaining events for the content word condition and 106 events for the function word condition.<span class="ref-lnk fn-ref-lnk lazy-ref"><a data-rid="EN0002" href="#" data-reflink="fn"><span class="off-screen">Footnote</span><sup>2</sup></a></span></p>
 </div>
 <div id="S002-S2009" class="NLM_sec NLM_sec_level_2">
  <h3 class="section-heading-3" id="_i18">Permutation test</h3>
  <p>We performed intra-individual permutation tests (Maris &amp; Oostenveld, <span class="ref-lnk lazy-ref"><a data-rid="CIT0085" data-reflink="_i34 _i35 _i36" href="#"><span class="off-screen">Citation</span>2007</a></span>) to estimate the <i>p</i>-value for the ERF comparison between content and function words. Thus, the ERF was cut into four subsequent time frames, each with a duration of 250 ms, and the root-mean-square amplitude (<i>RMS</i>) was calculated (<a href="#F0008">Figure 8</a>(e)):</p>
  <p><span class="NLM_disp-formula-image inline-formula rs_preserve">
    <noscript>
     <img src="/na101/home/literatum/publisher/tandf/journals/content/plcp21/2021/plcp21.v036.i02/23273798.2020.1803375/20210511/images/plcp_a_1803375_ilm0010.gif" alt="">
    </noscript><img src="//:0" alt="" class="mml-formula" data-formula-source="{&quot;type&quot; : &quot;image&quot;, &quot;src&quot; : &quot;/na101/home/literatum/publisher/tandf/journals/content/plcp21/2021/plcp21.v036.i02/23273798.2020.1803375/20210511/images/plcp_a_1803375_ilm0010.gif&quot;}"><span class="mml-formula"></span></span><span class="NLM_disp-formula inline-formula rs_preserve"><img src="//:0" alt="" data-formula-source="{&quot;type&quot; : &quot;mathjax&quot;}">
    <math>
     <mi>
      R
     </mi><mi>
      M
     </mi><mi>
      S
     </mi><mo>
      =
     </mo><msqrt>
      <mfenced open="(" close=")">
       <mrow>
        <mrow>
         <mn>
          1
         </mn>
         <mrow>
          <mo>
           /
          </mo>
         </mrow>
         <mi>
          N
         </mi>
        </mrow>
       </mrow>
      </mfenced>
      <msubsup>
       <mo movablelimits="false">
        ∑
       </mo>
       <mrow>
        <mi>
         i
        </mi>
        <mo>
         =
        </mo>
        <mn>
         0
        </mn>
       </mrow>
       <mi>
        N
       </mi>
      </msubsup>
      <mrow>
       <msubsup>
        <mi>
         v
        </mi>
        <mi>
         i
        </mi>
        <mn>
         2
        </mn>
       </msubsup>
      </mrow>
     </msqrt>
    </math></span>, with the signal values within a 250 ms interval <span class="NLM_disp-formula-image inline-formula rs_preserve">
    <noscript>
     <img src="/na101/home/literatum/publisher/tandf/journals/content/plcp21/2021/plcp21.v036.i02/23273798.2020.1803375/20210511/images/plcp_a_1803375_ilm0011.gif" alt="">
    </noscript><img src="//:0" alt="" class="mml-formula" data-formula-source="{&quot;type&quot; : &quot;image&quot;, &quot;src&quot; : &quot;/na101/home/literatum/publisher/tandf/journals/content/plcp21/2021/plcp21.v036.i02/23273798.2020.1803375/20210511/images/plcp_a_1803375_ilm0011.gif&quot;}"><span class="mml-formula"></span></span><span class="NLM_disp-formula inline-formula rs_preserve"><img src="//:0" alt="" data-formula-source="{&quot;type&quot; : &quot;mathjax&quot;}">
    <math>
     <msub>
      <mi>
       v
      </mi>
      <mi>
       i
      </mi>
     </msub>
    </math></span>, the total number of values within a 250 ms interval <span class="NLM_disp-formula-image inline-formula rs_preserve">
    <noscript>
     <img src="/na101/home/literatum/publisher/tandf/journals/content/plcp21/2021/plcp21.v036.i02/23273798.2020.1803375/20210511/images/plcp_a_1803375_ilm0012.gif" alt="">
    </noscript><img src="//:0" alt="" class="mml-formula" data-formula-source="{&quot;type&quot; : &quot;image&quot;, &quot;src&quot; : &quot;/na101/home/literatum/publisher/tandf/journals/content/plcp21/2021/plcp21.v036.i02/23273798.2020.1803375/20210511/images/plcp_a_1803375_ilm0012.gif&quot;}"><span class="mml-formula"></span></span><span class="NLM_disp-formula inline-formula rs_preserve"><img src="//:0" alt="" data-formula-source="{&quot;type&quot; : &quot;mathjax&quot;}">
    <math>
     <mi>
      N
     </mi><mo>
      =
     </mo><msub>
      <mi>
       f
      </mi>
      <mi>
       s
      </mi>
     </msub><mo>
      ⋅
     </mo><mn>
      250
     </mn>
    </math></span> ms, and the sampling rate <span class="NLM_disp-formula-image inline-formula rs_preserve">
    <noscript>
     <img src="/na101/home/literatum/publisher/tandf/journals/content/plcp21/2021/plcp21.v036.i02/23273798.2020.1803375/20210511/images/plcp_a_1803375_ilm0013.gif" alt="">
    </noscript><img src="//:0" alt="" class="mml-formula" data-formula-source="{&quot;type&quot; : &quot;image&quot;, &quot;src&quot; : &quot;/na101/home/literatum/publisher/tandf/journals/content/plcp21/2021/plcp21.v036.i02/23273798.2020.1803375/20210511/images/plcp_a_1803375_ilm0013.gif&quot;}"><span class="mml-formula"></span></span><span class="NLM_disp-formula inline-formula rs_preserve"><img src="//:0" alt="" data-formula-source="{&quot;type&quot; : &quot;mathjax&quot;}">
    <math>
     <msub>
      <mi>
       f
      </mi>
      <mi>
       s
      </mi>
     </msub><mo>
      =
     </mo><mn>
      1000
     </mn>
    </math></span> Hz.</p>
  <p>10,000 different random permutations of content word and function word labels were generated. For each of these samples the four RMS amplitudes for the different time frames were calculated based on the baseline corrected<span class="ref-lnk fn-ref-lnk lazy-ref"><a data-rid="EN0003" href="#" data-reflink="fn"><span class="off-screen">Footnote</span><sup>3</sup></a></span> single trials, resulting in a distribution of amplitudes for each time frame (<a href="#F0008">Figure 8</a>(f)). The amplitude values corresponding to the true labelling are compared with amplitudes derived from random permutations in order to estimate the statistical significance, i.e. the <i>p</i>-values for content and function words <span class="NLM_disp-formula-image inline-formula rs_preserve">
    <noscript>
     <img src="/na101/home/literatum/publisher/tandf/journals/content/plcp21/2021/plcp21.v036.i02/23273798.2020.1803375/20210511/images/plcp_a_1803375_ilm0014.gif" alt="">
    </noscript><img src="//:0" alt="" class="mml-formula" data-formula-source="{&quot;type&quot; : &quot;image&quot;, &quot;src&quot; : &quot;/na101/home/literatum/publisher/tandf/journals/content/plcp21/2021/plcp21.v036.i02/23273798.2020.1803375/20210511/images/plcp_a_1803375_ilm0014.gif&quot;}"><span class="mml-formula"></span></span><span class="NLM_disp-formula inline-formula rs_preserve"><img src="//:0" alt="" data-formula-source="{&quot;type&quot; : &quot;mathjax&quot;}">
    <math>
     <msub>
      <mi>
       p
      </mi>
      <mrow>
       <mi>
        c
       </mi>
       <mi>
        o
       </mi>
       <mi>
        n
       </mi>
      </mrow>
     </msub>
    </math></span> and <span class="NLM_disp-formula-image inline-formula rs_preserve">
    <noscript>
     <img src="/na101/home/literatum/publisher/tandf/journals/content/plcp21/2021/plcp21.v036.i02/23273798.2020.1803375/20210511/images/plcp_a_1803375_ilm0015.gif" alt="">
    </noscript><img src="//:0" alt="" class="mml-formula" data-formula-source="{&quot;type&quot; : &quot;image&quot;, &quot;src&quot; : &quot;/na101/home/literatum/publisher/tandf/journals/content/plcp21/2021/plcp21.v036.i02/23273798.2020.1803375/20210511/images/plcp_a_1803375_ilm0015.gif&quot;}"><span class="mml-formula"></span></span><span class="NLM_disp-formula inline-formula rs_preserve"><img src="//:0" alt="" data-formula-source="{&quot;type&quot; : &quot;mathjax&quot;}">
    <math>
     <msub>
      <mi>
       p
      </mi>
      <mrow>
       <mspace width="thinmathspace"></mspace>
       <mi>
        f
       </mi>
       <mi>
        u
       </mi>
       <mi>
        n
       </mi>
      </mrow>
     </msub>
    </math></span>, respectively, in <a href="#F0008">Figure 8</a>(f).</p>
 </div>
 <div id="S002-S20010" class="NLM_sec NLM_sec_level_2">
  <h3 class="section-heading-3" id="_i19">Normalised power spectra</h3>
  <p>Using Fourier transformation, we also analysed the averaged normalised power spectra (alpha, beta and gamma frequency range) for words in contrast to pauses and for function and content words. The frequency bands were defined as follows : <span class="NLM_disp-formula-image inline-formula rs_preserve">
    <noscript>
     <img src="/na101/home/literatum/publisher/tandf/journals/content/plcp21/2021/plcp21.v036.i02/23273798.2020.1803375/20210511/images/plcp_a_1803375_ilm0016.gif" alt="">
    </noscript><img src="//:0" alt="" class="mml-formula" data-formula-source="{&quot;type&quot; : &quot;image&quot;, &quot;src&quot; : &quot;/na101/home/literatum/publisher/tandf/journals/content/plcp21/2021/plcp21.v036.i02/23273798.2020.1803375/20210511/images/plcp_a_1803375_ilm0016.gif&quot;}"><span class="mml-formula"></span></span><span class="NLM_disp-formula inline-formula rs_preserve"><img src="//:0" alt="" data-formula-source="{&quot;type&quot; : &quot;mathjax&quot;}">
    <math>
     <mi>
      α
     </mi><mo>
      :
     </mo><mn>
      8
     </mn><mrow>
      <mo>
       −
      </mo>
     </mrow><mn>
      12
     </mn>
    </math></span> Hz, <span class="NLM_disp-formula-image inline-formula rs_preserve">
    <noscript>
     <img src="/na101/home/literatum/publisher/tandf/journals/content/plcp21/2021/plcp21.v036.i02/23273798.2020.1803375/20210511/images/plcp_a_1803375_ilm0017.gif" alt="">
    </noscript><img src="//:0" alt="" class="mml-formula" data-formula-source="{&quot;type&quot; : &quot;image&quot;, &quot;src&quot; : &quot;/na101/home/literatum/publisher/tandf/journals/content/plcp21/2021/plcp21.v036.i02/23273798.2020.1803375/20210511/images/plcp_a_1803375_ilm0017.gif&quot;}"><span class="mml-formula"></span></span><span class="NLM_disp-formula inline-formula rs_preserve"><img src="//:0" alt="" data-formula-source="{&quot;type&quot; : &quot;mathjax&quot;}">
    <math>
     <mtext fontfamily="times">
      β
     </mtext><mo>
      :
     </mo><mn>
      12
     </mn><mrow>
      <mo>
       −
      </mo>
     </mrow><mn>
      30
     </mn>
    </math></span> Hz, <span class="NLM_disp-formula-image inline-formula rs_preserve">
    <noscript>
     <img src="/na101/home/literatum/publisher/tandf/journals/content/plcp21/2021/plcp21.v036.i02/23273798.2020.1803375/20210511/images/plcp_a_1803375_ilm0018.gif" alt="">
    </noscript><img src="//:0" alt="" class="mml-formula" data-formula-source="{&quot;type&quot; : &quot;image&quot;, &quot;src&quot; : &quot;/na101/home/literatum/publisher/tandf/journals/content/plcp21/2021/plcp21.v036.i02/23273798.2020.1803375/20210511/images/plcp_a_1803375_ilm0018.gif&quot;}"><span class="mml-formula"></span></span><span class="NLM_disp-formula inline-formula rs_preserve"><img src="//:0" alt="" data-formula-source="{&quot;type&quot; : &quot;mathjax&quot;}">
    <math>
     <mi>
      γ
     </mi><mo>
      :
     </mo><mn>
      30
     </mn><mrow>
      <mo>
       −
      </mo>
     </mrow><mn>
      45
     </mn>
    </math></span> Hz. The epoch length for this analysis was 400 ms for both, short periods of silence and word onsets. Furthermore, we projected the resulting values to the corresponding spatial position of the sensors. This was done by the usage of the plot_psd_topomap function of the <i>MNE</i> library with Python interface (Gramfort et al., <span class="ref-lnk lazy-ref"><a data-rid="CIT0042" data-reflink="_i34 _i35 _i36" href="#"><span class="off-screen">Citation</span>2013</a></span>, <span class="ref-lnk lazy-ref"><a data-rid="CIT0043" data-reflink="_i34 _i35 _i36" href="#"><span class="off-screen">Citation</span>2014</a></span>).</p>
 </div>
</div>
<div id="S003" class="NLM_sec NLM_sec_level_1">
 <h2 id="_i20" class="section-heading-2">Results</h2>
 <p>The general idea of our approach was to perform MEG measurements of participants listening to an audio book. By synchronising the continuous speech stream with the ongoing multi-channel neuronal activity and subsequently automatically segmenting the data streams according to word boundaries derived from forced alignment, we generated a database of annotated speech evoked neuronal activity. This corpus may then be analysed offline by applying the full range of methods from statistics, natural language processing, and computational corpus linguistics. In order to demonstrate the feasibility of our approach, we restricted our analyses to sensor space and did not perform any kind of source localisation (cf. Methods). More specifically, we calculate averaged ERFs for word onsets, and normalised power spectra for onsets of both, words and short pauses. In addition, we compare averaged ERFs for content and function words, and the corresponding normalised power spectra and discuss the results in the light of existing studies.</p>
 <div id="S003-S2001" class="NLM_sec NLM_sec_level_2">
  <h3 class="section-heading-3" id="_i21">Distribution of word classes</h3>
  <p>We analysed the distributions of word classes and word class combinations in the audio book and compared them with five different German corpora (German mixed 10k, 30k, and 100k; German news 30k; German Wikipedia 30k) taken from the <i>Leipzig Corpora Collection</i> (Goldhahn et al., <span class="ref-lnk lazy-ref"><a data-rid="CIT0041" data-reflink="_i34 _i35 _i36" href="#"><span class="off-screen">Citation</span>2012</a></span>), and in addition with a number of other German novels. A sample of the resulting distribtuions is provided in <a href="#F0005">Figure 5</a>. It turns out that <i>Gut gegen Nordwind</i> seems to have a very typical word class distribution (<a href="#F0005">Figure 5</a>(a,b)), especially in comparison to other German novels (<a href="#F0005">Figure 5</a>(c–h)). In contrast, in the German mixed corpus, there seem to be an under representation of pronouns (<a href="#F0005">Figure 5</a>(i,j)) compared to all analysed novels. Using multi-dimensional scaling (MDS), we visualise the mutual (dis-)similarities between all word class distributions (<a href="#F0006">Figure 6</a>(a)), and distributions of word class combinations (<a href="#F0006">Figure 6</a>(b)). We find that <i>Gut gegen Nordwind</i> is closer, i.e. more similar, to the German novels than to the German corpora. The five corpora seem to cluster apart from the novels. In particular, the distributions of the German mixed corpora of three different sizes (10k, 30k, 100k words) are almost indistinguishable, and hence the corresponding MDS projections are overlapping. Furthermore, it remarkably turns out, that different novels from the same author are closer, i.e. more similar in terms of word class and word class combination distributions, than novels from different authors.</p>
  <div class="figure figureViewer" id="F0005">
   <div class="hidden figureViewerArticleInfo">
    <span class="figViewerTitle">Analysis of continuous neuronal activity evoked by natural speech with computational corpus linguistics methods</span>
    <div class="articleAuthors articleInfoSection">
     <div class="authorsHeading">
      All authors
     </div>
     <div class="authors">
      <a class="entryAuthor" href="/action/doSearch?Contrib=Schilling%2C+Achim"><span class="hlFld-ContribAuthor"></span></a><a href="/author/Schilling%2C+Achim"><span class="NLM_given-names">Achim</span> Schilling</a>, <a class="entryAuthor" href="/action/doSearch?Contrib=Tomasello%2C+Rosario"><span class="hlFld-ContribAuthor"></span></a><a href="/author/Tomasello%2C+Rosario"><span class="NLM_given-names">Rosario</span> Tomasello</a>, <a class="entryAuthor" href="/action/doSearch?Contrib=Henningsen-Schomers%2C+Malte+R"><span class="hlFld-ContribAuthor"></span></a><a href="/author/Henningsen-Schomers%2C+Malte+R"><span class="NLM_given-names">Malte R.</span> Henningsen-Schomers</a>, <a class="entryAuthor" href="/action/doSearch?Contrib=Zankl%2C+Alexandra"><span class="hlFld-ContribAuthor"></span></a><a href="/author/Zankl%2C+Alexandra"><span class="NLM_given-names">Alexandra</span> Zankl</a>, <a class="entryAuthor" href="/action/doSearch?Contrib=Surendra%2C+Kishore"><span class="hlFld-ContribAuthor"></span></a><a href="/author/Surendra%2C+Kishore"><span class="NLM_given-names">Kishore</span> Surendra</a>, <a class="entryAuthor" href="/action/doSearch?Contrib=Haller%2C+Martin"><span class="hlFld-ContribAuthor"></span></a><a href="/author/Haller%2C+Martin"><span class="NLM_given-names">Martin</span> Haller</a>, <a class="entryAuthor" href="/action/doSearch?Contrib=Karl%2C+Valerie"><span class="hlFld-ContribAuthor"></span></a><a href="/author/Karl%2C+Valerie"><span class="NLM_given-names">Valerie</span> Karl</a>, <a class="entryAuthor" href="/action/doSearch?Contrib=Uhrig%2C+Peter"><span class="hlFld-ContribAuthor"></span></a><a href="/author/Uhrig%2C+Peter"><span class="NLM_given-names">Peter</span> Uhrig</a>, <a class="entryAuthor" href="/action/doSearch?Contrib=Maier%2C+Andreas"><span class="hlFld-ContribAuthor"></span></a><a href="/author/Maier%2C+Andreas"><span class="NLM_given-names">Andreas</span> Maier</a> &amp; <a class="entryAuthor" href="/action/doSearch?Contrib=Krauss%2C+Patrick"><span class="hlFld-ContribAuthor"></span></a><a href="/author/Krauss%2C+Patrick"><span class="NLM_given-names">Patrick</span> Krauss</a> <a href="https://orcid.org/0000-0002-6611-7733"><img src="/templates/jsp/images/orcid.png"></a>
     </div>
    </div>
    <div class="articleLowerInfo articleInfoSection">
     <div class="articleLowerInfoSection articleInfoDOI">
      <a href="https://doi.org/10.1080/23273798.2020.1803375">https://doi.org/10.1080/23273798.2020.1803375</a>
     </div>
     <div class="articleInfoPublicationDate articleLowerInfoSection border">
      <h6>Published online:</h6>10 August 2020
     </div>
    </div>
   </div>
   <div class="figureThumbnailContainer">
    <div class="figureInfo">
     <div class="short-legend">
      <p class="captionText"><span class="captionLabel">Figure 5. </span> Distributions of word classes and bi-gram word classes. (a,c,e,g,i) Distribution of word classes according to POS tagging. Adjectives (ADJ), adverbs (ADV), nouns (NOUN), proper nouns (PROPN), verbs (VERB), adpositions (ADP), auxiliary verbs (AUX), determiners (DET), particles (PART), pronouns (PRON), subordinating conjunctions (SCONJ). (b,d,f,h,j) Distribution of word classes of 2-word sequences. Rows: word class of first word. Columns: word class of second word.</p>
     </div>
    </div><a href="#" class="thumbnail" aria-label="thumbnail image"><img id="F0005image" src="/na101/home/literatum/publisher/tandf/journals/content/plcp21/2021/plcp21.v036.i02/23273798.2020.1803375/20210511/images/medium/plcp_a_1803375_f0005_oc.jpg" loading="lazy" height="500" width="359"></a>
    <div class="figureDownloadOptions">
     <a href="#" class="downloadBtn btn btn-sm" role="button">Display full size</a>
    </div>
   </div>
  </div>
  <div class="hidden rs_skip" id="fig-description-F0005">
   <p class="captionText"><span class="captionLabel">Figure 5. </span> Distributions of word classes and bi-gram word classes. (a,c,e,g,i) Distribution of word classes according to POS tagging. Adjectives (ADJ), adverbs (ADV), nouns (NOUN), proper nouns (PROPN), verbs (VERB), adpositions (ADP), auxiliary verbs (AUX), determiners (DET), particles (PART), pronouns (PRON), subordinating conjunctions (SCONJ). (b,d,f,h,j) Distribution of word classes of 2-word sequences. Rows: word class of first word. Columns: word class of second word.</p>
  </div>
  <div class="hidden rs_skip" id="figureFootNote-F0005">
   <div class="figureFootNote-F0005"></div>
  </div>
  <div class="figure figureViewer" id="F0006">
   <div class="hidden figureViewerArticleInfo">
    <span class="figViewerTitle">Analysis of continuous neuronal activity evoked by natural speech with computational corpus linguistics methods</span>
    <div class="articleAuthors articleInfoSection">
     <div class="authorsHeading">
      All authors
     </div>
     <div class="authors">
      <a class="entryAuthor" href="/action/doSearch?Contrib=Schilling%2C+Achim"><span class="hlFld-ContribAuthor"></span></a><a href="/author/Schilling%2C+Achim"><span class="NLM_given-names">Achim</span> Schilling</a>, <a class="entryAuthor" href="/action/doSearch?Contrib=Tomasello%2C+Rosario"><span class="hlFld-ContribAuthor"></span></a><a href="/author/Tomasello%2C+Rosario"><span class="NLM_given-names">Rosario</span> Tomasello</a>, <a class="entryAuthor" href="/action/doSearch?Contrib=Henningsen-Schomers%2C+Malte+R"><span class="hlFld-ContribAuthor"></span></a><a href="/author/Henningsen-Schomers%2C+Malte+R"><span class="NLM_given-names">Malte R.</span> Henningsen-Schomers</a>, <a class="entryAuthor" href="/action/doSearch?Contrib=Zankl%2C+Alexandra"><span class="hlFld-ContribAuthor"></span></a><a href="/author/Zankl%2C+Alexandra"><span class="NLM_given-names">Alexandra</span> Zankl</a>, <a class="entryAuthor" href="/action/doSearch?Contrib=Surendra%2C+Kishore"><span class="hlFld-ContribAuthor"></span></a><a href="/author/Surendra%2C+Kishore"><span class="NLM_given-names">Kishore</span> Surendra</a>, <a class="entryAuthor" href="/action/doSearch?Contrib=Haller%2C+Martin"><span class="hlFld-ContribAuthor"></span></a><a href="/author/Haller%2C+Martin"><span class="NLM_given-names">Martin</span> Haller</a>, <a class="entryAuthor" href="/action/doSearch?Contrib=Karl%2C+Valerie"><span class="hlFld-ContribAuthor"></span></a><a href="/author/Karl%2C+Valerie"><span class="NLM_given-names">Valerie</span> Karl</a>, <a class="entryAuthor" href="/action/doSearch?Contrib=Uhrig%2C+Peter"><span class="hlFld-ContribAuthor"></span></a><a href="/author/Uhrig%2C+Peter"><span class="NLM_given-names">Peter</span> Uhrig</a>, <a class="entryAuthor" href="/action/doSearch?Contrib=Maier%2C+Andreas"><span class="hlFld-ContribAuthor"></span></a><a href="/author/Maier%2C+Andreas"><span class="NLM_given-names">Andreas</span> Maier</a> &amp; <a class="entryAuthor" href="/action/doSearch?Contrib=Krauss%2C+Patrick"><span class="hlFld-ContribAuthor"></span></a><a href="/author/Krauss%2C+Patrick"><span class="NLM_given-names">Patrick</span> Krauss</a> <a href="https://orcid.org/0000-0002-6611-7733"><img src="/templates/jsp/images/orcid.png"></a>
     </div>
    </div>
    <div class="articleLowerInfo articleInfoSection">
     <div class="articleLowerInfoSection articleInfoDOI">
      <a href="https://doi.org/10.1080/23273798.2020.1803375">https://doi.org/10.1080/23273798.2020.1803375</a>
     </div>
     <div class="articleInfoPublicationDate articleLowerInfoSection border">
      <h6>Published online:</h6>10 August 2020
     </div>
    </div>
   </div>
   <div class="figureThumbnailContainer">
    <div class="figureInfo">
     <div class="short-legend">
      <p class="captionText"><span class="captionLabel">Figure 6. </span> MDS projection of word class distributions. (a) MDS projection of distributions of single word classes. (b) MDS projection of distributions of bi-gram word classes combinations.</p>
     </div>
    </div><a href="#" class="thumbnail" aria-label="thumbnail image"><img id="F0006image" src="/na101/home/literatum/publisher/tandf/journals/content/plcp21/2021/plcp21.v036.i02/23273798.2020.1803375/20210511/images/medium/plcp_a_1803375_f0006_oc.jpg" loading="lazy" height="159" width="500"></a>
    <div class="figureDownloadOptions">
     <a href="#" class="downloadBtn btn btn-sm" role="button">Display full size</a>
    </div>
   </div>
  </div>
  <div class="hidden rs_skip" id="fig-description-F0006">
   <p class="captionText"><span class="captionLabel">Figure 6. </span> MDS projection of word class distributions. (a) MDS projection of distributions of single word classes. (b) MDS projection of distributions of bi-gram word classes combinations.</p>
  </div>
  <div class="hidden rs_skip" id="figureFootNote-F0006">
   <div class="figureFootNote-F0006"></div>
  </div>
  <p></p>
 </div>
 <div id="S003-S2002" class="NLM_sec NLM_sec_level_2">
  <h3 class="section-heading-3" id="_i24">Event-related fields of word onsets</h3>
  <p>For a first proof of concept and to determine clear neurophysiological brain responses from continuous speech, we analysed event-related fields (ERFs) for word onsets (irrespective of their word classes) from different topographical sides.</p>
  <p><a href="#F0007">Figure 7</a>(a) shows one example of the resulting ERFs averaged over the aforementioned 291 events corresponding to word onsets for one participant (subject 2 of 15) and parts number 1 to 3 of the audio book and <a href="#F0007">Figure 7</a>(b) shows a projection of the spatial distribution of the ERF amplitudes at 350 ms after word onset. The largest amplitudes occur in channels located at temporal and frontal areas of the left hemisphere known to be associated with language processing (Friederici &amp; Gierhan, <span class="ref-lnk lazy-ref"><a data-rid="CIT0033" data-reflink="_i34 _i35 _i36" href="#"><span class="off-screen">Citation</span>2013</a></span>). The ERFs of those channels with the largest ERF amplitudes are shown in <a href="#F0007">Figure 7</a>(c). Furthermore, we see a clear N400 component for the word onset condition, indicating language associated processing (cf. Broderick et al., <span class="ref-lnk lazy-ref"><a data-rid="CIT0015" data-reflink="_i34 _i35 _i36" href="#"><span class="off-screen">Citation</span>2018</a></span>; Friederici et al., <span class="ref-lnk lazy-ref"><a data-rid="CIT0034" data-reflink="_i34 _i35 _i36" href="#"><span class="off-screen">Citation</span>1993</a></span>; Hagoort &amp; Brown, <span class="ref-lnk lazy-ref"><a data-rid="CIT0046" data-reflink="_i34 _i35 _i36" href="#"><span class="off-screen">Citation</span>2000</a></span>; Kutas &amp; Federmeier, <span class="ref-lnk lazy-ref"><a data-rid="CIT0078" data-reflink="_i34 _i35 _i36" href="#"><span class="off-screen">Citation</span>2011</a></span>; Lau et al., <span class="ref-lnk lazy-ref"><a data-rid="CIT0082" data-reflink="_i34 _i35 _i36" href="#"><span class="off-screen">Citation</span>2009</a></span>; Strauß et al., <span class="ref-lnk lazy-ref"><a data-rid="CIT0125" data-reflink="_i34 _i35 _i36" href="#"><span class="off-screen">Citation</span>2013</a></span>).</p>
  <div class="figure figureViewer" id="F0007">
   <div class="hidden figureViewerArticleInfo">
    <span class="figViewerTitle">Analysis of continuous neuronal activity evoked by natural speech with computational corpus linguistics methods</span>
    <div class="articleAuthors articleInfoSection">
     <div class="authorsHeading">
      All authors
     </div>
     <div class="authors">
      <a class="entryAuthor" href="/action/doSearch?Contrib=Schilling%2C+Achim"><span class="hlFld-ContribAuthor"></span></a><a href="/author/Schilling%2C+Achim"><span class="NLM_given-names">Achim</span> Schilling</a>, <a class="entryAuthor" href="/action/doSearch?Contrib=Tomasello%2C+Rosario"><span class="hlFld-ContribAuthor"></span></a><a href="/author/Tomasello%2C+Rosario"><span class="NLM_given-names">Rosario</span> Tomasello</a>, <a class="entryAuthor" href="/action/doSearch?Contrib=Henningsen-Schomers%2C+Malte+R"><span class="hlFld-ContribAuthor"></span></a><a href="/author/Henningsen-Schomers%2C+Malte+R"><span class="NLM_given-names">Malte R.</span> Henningsen-Schomers</a>, <a class="entryAuthor" href="/action/doSearch?Contrib=Zankl%2C+Alexandra"><span class="hlFld-ContribAuthor"></span></a><a href="/author/Zankl%2C+Alexandra"><span class="NLM_given-names">Alexandra</span> Zankl</a>, <a class="entryAuthor" href="/action/doSearch?Contrib=Surendra%2C+Kishore"><span class="hlFld-ContribAuthor"></span></a><a href="/author/Surendra%2C+Kishore"><span class="NLM_given-names">Kishore</span> Surendra</a>, <a class="entryAuthor" href="/action/doSearch?Contrib=Haller%2C+Martin"><span class="hlFld-ContribAuthor"></span></a><a href="/author/Haller%2C+Martin"><span class="NLM_given-names">Martin</span> Haller</a>, <a class="entryAuthor" href="/action/doSearch?Contrib=Karl%2C+Valerie"><span class="hlFld-ContribAuthor"></span></a><a href="/author/Karl%2C+Valerie"><span class="NLM_given-names">Valerie</span> Karl</a>, <a class="entryAuthor" href="/action/doSearch?Contrib=Uhrig%2C+Peter"><span class="hlFld-ContribAuthor"></span></a><a href="/author/Uhrig%2C+Peter"><span class="NLM_given-names">Peter</span> Uhrig</a>, <a class="entryAuthor" href="/action/doSearch?Contrib=Maier%2C+Andreas"><span class="hlFld-ContribAuthor"></span></a><a href="/author/Maier%2C+Andreas"><span class="NLM_given-names">Andreas</span> Maier</a> &amp; <a class="entryAuthor" href="/action/doSearch?Contrib=Krauss%2C+Patrick"><span class="hlFld-ContribAuthor"></span></a><a href="/author/Krauss%2C+Patrick"><span class="NLM_given-names">Patrick</span> Krauss</a> <a href="https://orcid.org/0000-0002-6611-7733"><img src="/templates/jsp/images/orcid.png"></a>
     </div>
    </div>
    <div class="articleLowerInfo articleInfoSection">
     <div class="articleLowerInfoSection articleInfoDOI">
      <a href="https://doi.org/10.1080/23273798.2020.1803375">https://doi.org/10.1080/23273798.2020.1803375</a>
     </div>
     <div class="articleInfoPublicationDate articleLowerInfoSection border">
      <h6>Published online:</h6>10 August 2020
     </div>
    </div>
   </div>
   <div class="figureThumbnailContainer">
    <div class="figureInfo">
     <div class="short-legend">
      <p class="captionText"><span class="captionLabel">Figure 7. </span> Event–related fields for word onset. Shown are exemplary data of book parts number 1–3 of 10 from subject 2 of 15. (a) Summary of ERFs of all 248 recording channels averaged over 291 trials. (b) Spatial distribution of ERF amplitudes at 350 ms after word onset. (c) The largest amplitudes occur in channels located at temporal and frontal areas of the left hemisphere. (d) The corresponding channels at the right hemisphere show clearly smaller ERF amplitudes. (e) The same is true for occipital channels. (f) Same channels as in c, but averaged over randomly chosen triggers instead of word onset triggers. Also in this control condition, the resulting amplitudes are smaller than those for the word onset condition.</p>
     </div>
    </div><a href="#" class="thumbnail" aria-label="thumbnail image"><img id="F0007image" src="/na101/home/literatum/publisher/tandf/journals/content/plcp21/2021/plcp21.v036.i02/23273798.2020.1803375/20210511/images/medium/plcp_a_1803375_f0007_oc.jpg" loading="lazy" height="500" width="450"></a>
    <div class="figureDownloadOptions">
     <a href="#" class="downloadBtn btn btn-sm" role="button">Display full size</a>
    </div>
   </div>
  </div>
  <div class="hidden rs_skip" id="fig-description-F0007">
   <p class="captionText"><span class="captionLabel">Figure 7. </span> Event–related fields for word onset. Shown are exemplary data of book parts number 1–3 of 10 from subject 2 of 15. (a) Summary of ERFs of all 248 recording channels averaged over 291 trials. (b) Spatial distribution of ERF amplitudes at 350 ms after word onset. (c) The largest amplitudes occur in channels located at temporal and frontal areas of the left hemisphere. (d) The corresponding channels at the right hemisphere show clearly smaller ERF amplitudes. (e) The same is true for occipital channels. (f) Same channels as in c, but averaged over randomly chosen triggers instead of word onset triggers. Also in this control condition, the resulting amplitudes are smaller than those for the word onset condition.</p>
  </div>
  <div class="hidden rs_skip" id="figureFootNote-F0007">
   <div class="figureFootNote-F0007"></div>
  </div>
  <p></p>
  <p>In order to exclude random effects, we compare these channels with the corresponding channels located at the right hemisphere – where we expect less activation due to the asymmetric lateralisation of speech in the brain – (<a href="#F0007">Figure 7</a>(d)), and with some occipital channels (<a href="#F0007">Figure 7</a>(e)). In both cases, the resulting ERF amplitudes are clearly smaller than those of the left temporal and frontal channels (<a href="#F0007">Figure 7</a>(c)). In addition, we calculate control ERFs for the same channels shown in <a href="#F0007">Figure 7</a>(c), but instead of word boundaries we used randomly chosen time tags for segmentation. Also in this control condition, the resulting ERF amplitudes are smaller than those for the word onset condition (<a href="#F0007">Figure 7</a>(f)). This result, in particular, demonstrates that even though there are no or only relatively short inter-stimulus intervals, leading to overlapping effects of late and early responses of subsequent words, there is still enough signal left in the individual trials.</p>
  <p>Finally, we evaluated the re-test reliability of our results using three-fold sub-sampling by separately averaging only over events belonging to the same part of the the audio book (Figures S1–S3). Again, the largest ERF amplitudes were found in the same channels as before and all results show very similar patterns to those shown in <a href="#F0007">Figure 7</a>. In addition, we provide exemplary results of two further participants in the Supplements section (Figures S4 and S5).</p>
 </div>
 <div id="S003-S2003" class="NLM_sec NLM_sec_level_2">
  <h3 class="section-heading-3" id="_i26">Event-related fields of content and function words</h3>
  <p>As a further validity test of the present study, we analysed and compared the brain responses of different word classes. As an example, the resulting ERFs averaged over the respective events (content words: <i>n</i> = 81, function words: <i>n</i> = 106) for one participant (subject 2 of 15) and parts number 1 to 3 of the audio book are shown in <a href="#F0008">Figure 8</a>(a,c) and a projection of the spatial distribution of the ERF amplitudes at <span class="NLM_disp-formula-image inline-formula rs_preserve">
    <noscript>
     <img src="/na101/home/literatum/publisher/tandf/journals/content/plcp21/2021/plcp21.v036.i02/23273798.2020.1803375/20210511/images/plcp_a_1803375_ilm0019.gif" alt="">
    </noscript><img src="//:0" alt="" class="mml-formula" data-formula-source="{&quot;type&quot; : &quot;image&quot;, &quot;src&quot; : &quot;/na101/home/literatum/publisher/tandf/journals/content/plcp21/2021/plcp21.v036.i02/23273798.2020.1803375/20210511/images/plcp_a_1803375_ilm0019.gif&quot;}"><span class="mml-formula"></span></span><span class="NLM_disp-formula inline-formula rs_preserve"><img src="//:0" alt="" data-formula-source="{&quot;type&quot; : &quot;mathjax&quot;}">
    <math>
     <mn>
      550
     </mn><mspace width="thinmathspace"></mspace><mi>
      m
     </mi><mi>
      s
     </mi>
    </math></span> after word onset is provided in <a href="#F0008">Figure 8</a>(b,d). Again, we see a clear N400 component for both conditions, indicating language associated processing (cf. Broderick et al., <span class="ref-lnk lazy-ref"><a data-rid="CIT0015" data-reflink="_i34 _i35 _i36" href="#"><span class="off-screen">Citation</span>2018</a></span>; Friederici et al., <span class="ref-lnk lazy-ref"><a data-rid="CIT0034" data-reflink="_i34 _i35 _i36" href="#"><span class="off-screen">Citation</span>1993</a></span>; Hagoort &amp; Brown, <span class="ref-lnk lazy-ref"><a data-rid="CIT0046" data-reflink="_i34 _i35 _i36" href="#"><span class="off-screen">Citation</span>2000</a></span>; Kutas &amp; Federmeier, <span class="ref-lnk lazy-ref"><a data-rid="CIT0078" data-reflink="_i34 _i35 _i36" href="#"><span class="off-screen">Citation</span>2011</a></span>; Lau et al., <span class="ref-lnk lazy-ref"><a data-rid="CIT0082" data-reflink="_i34 _i35 _i36" href="#"><span class="off-screen">Citation</span>2009</a></span>; Strauß et al., <span class="ref-lnk lazy-ref"><a data-rid="CIT0125" data-reflink="_i34 _i35 _i36" href="#"><span class="off-screen">Citation</span>2013</a></span>).</p>
  <div class="figure figureViewer" id="F0008">
   <div class="hidden figureViewerArticleInfo">
    <span class="figViewerTitle">Analysis of continuous neuronal activity evoked by natural speech with computational corpus linguistics methods</span>
    <div class="articleAuthors articleInfoSection">
     <div class="authorsHeading">
      All authors
     </div>
     <div class="authors">
      <a class="entryAuthor" href="/action/doSearch?Contrib=Schilling%2C+Achim"><span class="hlFld-ContribAuthor"></span></a><a href="/author/Schilling%2C+Achim"><span class="NLM_given-names">Achim</span> Schilling</a>, <a class="entryAuthor" href="/action/doSearch?Contrib=Tomasello%2C+Rosario"><span class="hlFld-ContribAuthor"></span></a><a href="/author/Tomasello%2C+Rosario"><span class="NLM_given-names">Rosario</span> Tomasello</a>, <a class="entryAuthor" href="/action/doSearch?Contrib=Henningsen-Schomers%2C+Malte+R"><span class="hlFld-ContribAuthor"></span></a><a href="/author/Henningsen-Schomers%2C+Malte+R"><span class="NLM_given-names">Malte R.</span> Henningsen-Schomers</a>, <a class="entryAuthor" href="/action/doSearch?Contrib=Zankl%2C+Alexandra"><span class="hlFld-ContribAuthor"></span></a><a href="/author/Zankl%2C+Alexandra"><span class="NLM_given-names">Alexandra</span> Zankl</a>, <a class="entryAuthor" href="/action/doSearch?Contrib=Surendra%2C+Kishore"><span class="hlFld-ContribAuthor"></span></a><a href="/author/Surendra%2C+Kishore"><span class="NLM_given-names">Kishore</span> Surendra</a>, <a class="entryAuthor" href="/action/doSearch?Contrib=Haller%2C+Martin"><span class="hlFld-ContribAuthor"></span></a><a href="/author/Haller%2C+Martin"><span class="NLM_given-names">Martin</span> Haller</a>, <a class="entryAuthor" href="/action/doSearch?Contrib=Karl%2C+Valerie"><span class="hlFld-ContribAuthor"></span></a><a href="/author/Karl%2C+Valerie"><span class="NLM_given-names">Valerie</span> Karl</a>, <a class="entryAuthor" href="/action/doSearch?Contrib=Uhrig%2C+Peter"><span class="hlFld-ContribAuthor"></span></a><a href="/author/Uhrig%2C+Peter"><span class="NLM_given-names">Peter</span> Uhrig</a>, <a class="entryAuthor" href="/action/doSearch?Contrib=Maier%2C+Andreas"><span class="hlFld-ContribAuthor"></span></a><a href="/author/Maier%2C+Andreas"><span class="NLM_given-names">Andreas</span> Maier</a> &amp; <a class="entryAuthor" href="/action/doSearch?Contrib=Krauss%2C+Patrick"><span class="hlFld-ContribAuthor"></span></a><a href="/author/Krauss%2C+Patrick"><span class="NLM_given-names">Patrick</span> Krauss</a> <a href="https://orcid.org/0000-0002-6611-7733"><img src="/templates/jsp/images/orcid.png"></a>
     </div>
    </div>
    <div class="articleLowerInfo articleInfoSection">
     <div class="articleLowerInfoSection articleInfoDOI">
      <a href="https://doi.org/10.1080/23273798.2020.1803375">https://doi.org/10.1080/23273798.2020.1803375</a>
     </div>
     <div class="articleInfoPublicationDate articleLowerInfoSection border">
      <h6>Published online:</h6>10 August 2020
     </div>
    </div>
   </div>
   <div class="figureThumbnailContainer">
    <div class="figureInfo">
     <div class="short-legend">
      <p class="captionText"><span class="captionLabel">Figure 8. </span> Event-related fields for function and content words. Shown are exemplary data of book parts number 1–3 of 10 from subject 2 of 15. (a) Averaged ERFs for content words (<i>n</i> = 81 trials) with largest amplitudes. (b) Spatial distribution of ERF amplitudes at 550 ms after word onset for content words. (c) Averaged ERFs for function words (<i>n</i> = 106 trials) with largest amplitudes. (d) Spatial distribution of ERF amplitudes at 550 ms after word onset for function words. (e) ERF with the largest amplitude for content words and function words, together with ERFs derived from permutation test. (f) Distribution of ERF amplitudes derived from permutation test within four subsequent time frames: 0–250 ms (upper left), 250–500 ms (upper right), 500–750 ms (lower left) and 750–1000 ms (lower right).</p>
     </div>
    </div><a href="#" class="thumbnail" aria-label="thumbnail image"><img id="F0008image" src="/na101/home/literatum/publisher/tandf/journals/content/plcp21/2021/plcp21.v036.i02/23273798.2020.1803375/20210511/images/medium/plcp_a_1803375_f0008_oc.jpg" loading="lazy" height="500" width="445"></a>
    <div class="figureDownloadOptions">
     <a href="#" class="downloadBtn btn btn-sm" role="button">Display full size</a>
    </div>
   </div>
  </div>
  <div class="hidden rs_skip" id="fig-description-F0008">
   <p class="captionText"><span class="captionLabel">Figure 8. </span> Event-related fields for function and content words. Shown are exemplary data of book parts number 1–3 of 10 from subject 2 of 15. (a) Averaged ERFs for content words (<i>n</i> = 81 trials) with largest amplitudes. (b) Spatial distribution of ERF amplitudes at 550 ms after word onset for content words. (c) Averaged ERFs for function words (<i>n</i> = 106 trials) with largest amplitudes. (d) Spatial distribution of ERF amplitudes at 550 ms after word onset for function words. (e) ERF with the largest amplitude for content words and function words, together with ERFs derived from permutation test. (f) Distribution of ERF amplitudes derived from permutation test within four subsequent time frames: 0–250 ms (upper left), 250–500 ms (upper right), 500–750 ms (lower left) and 750–1000 ms (lower right).</p>
  </div>
  <div class="hidden rs_skip" id="figureFootNote-F0008">
   <div class="figureFootNote-F0008"></div>
  </div>
  <p></p>
  <p>Furthermore, we found that content words (<a href="#F0008">Figure 8</a>(a,b) elicited greater activation than function words (<a href="#F0008">Figure 8</a>(c,d)), especially in temporal and frontal areas of the left hemisphere. Since content parts of speech have been shown to differ semantically from function parts of speech (Kemmerer, <span class="ref-lnk lazy-ref"><a data-rid="CIT0061" data-reflink="_i34 _i35 _i36" href="#"><span class="off-screen">Citation</span>2014</a></span>; Pulvermüller, <span class="ref-lnk lazy-ref"><a data-rid="CIT0104" data-reflink="_i34 _i35 _i36" href="#"><span class="off-screen">Citation</span>2003</a></span>), these findings are in line with previous studies (Diaz &amp; McCarthy, <span class="ref-lnk lazy-ref"><a data-rid="CIT0025" data-reflink="_i34 _i35 _i36" href="#"><span class="off-screen">Citation</span>2009</a></span>).</p>
  <p>In addition, we compared the two conditions for the channel yielding the largest ERF amplitude and performed a permutation test (Maris &amp; Oostenveld, <span class="ref-lnk lazy-ref"><a data-rid="CIT0085" data-reflink="_i34 _i35 _i36" href="#"><span class="off-screen">Citation</span>2007</a></span>) independently for four subsequent time frames each with a duration of 250 ms (<a href="#F0008">Figure 8</a>(e,f)). We found that the averaged ERFs for the two conditions (content and function words, intra-individual) are significantly (<i>p</i> &lt; 0.05) different within the first (0 –250 ms) and third (500 –750 ms), but not within the second (250 –500 ms) and fourth (750 –1000 ms) time frame (<a href="#F0008">Figure 8</a>(f)). These results are consistent across all subjects (cf. e.g. Figures S6 and S7 for two further subjects), and are in line with previously reported results (Keurs et al., <span class="ref-lnk lazy-ref"><a data-rid="CIT0063" data-reflink="_i34 _i35 _i36" href="#"><span class="off-screen">Citation</span>1995</a></span>).</p>
 </div>
 <div id="S003-S2004" class="NLM_sec NLM_sec_level_2">
  <h3 class="section-heading-3" id="_i28">Averaged normalised power spectra</h3>
  <p>In our analysis of the averaged normalised power spectra, we were unable to find significant differences between the conditions of word onset and of silence onset (<a href="#F0009">Figure 9</a>) and neither between content words and function words (<a href="#F0010">Figure 10</a>). See discussion section for possible reasons.</p>
  <div class="figure figureViewer" id="F0009">
   <div class="hidden figureViewerArticleInfo">
    <span class="figViewerTitle">Analysis of continuous neuronal activity evoked by natural speech with computational corpus linguistics methods</span>
    <div class="articleAuthors articleInfoSection">
     <div class="authorsHeading">
      All authors
     </div>
     <div class="authors">
      <a class="entryAuthor" href="/action/doSearch?Contrib=Schilling%2C+Achim"><span class="hlFld-ContribAuthor"></span></a><a href="/author/Schilling%2C+Achim"><span class="NLM_given-names">Achim</span> Schilling</a>, <a class="entryAuthor" href="/action/doSearch?Contrib=Tomasello%2C+Rosario"><span class="hlFld-ContribAuthor"></span></a><a href="/author/Tomasello%2C+Rosario"><span class="NLM_given-names">Rosario</span> Tomasello</a>, <a class="entryAuthor" href="/action/doSearch?Contrib=Henningsen-Schomers%2C+Malte+R"><span class="hlFld-ContribAuthor"></span></a><a href="/author/Henningsen-Schomers%2C+Malte+R"><span class="NLM_given-names">Malte R.</span> Henningsen-Schomers</a>, <a class="entryAuthor" href="/action/doSearch?Contrib=Zankl%2C+Alexandra"><span class="hlFld-ContribAuthor"></span></a><a href="/author/Zankl%2C+Alexandra"><span class="NLM_given-names">Alexandra</span> Zankl</a>, <a class="entryAuthor" href="/action/doSearch?Contrib=Surendra%2C+Kishore"><span class="hlFld-ContribAuthor"></span></a><a href="/author/Surendra%2C+Kishore"><span class="NLM_given-names">Kishore</span> Surendra</a>, <a class="entryAuthor" href="/action/doSearch?Contrib=Haller%2C+Martin"><span class="hlFld-ContribAuthor"></span></a><a href="/author/Haller%2C+Martin"><span class="NLM_given-names">Martin</span> Haller</a>, <a class="entryAuthor" href="/action/doSearch?Contrib=Karl%2C+Valerie"><span class="hlFld-ContribAuthor"></span></a><a href="/author/Karl%2C+Valerie"><span class="NLM_given-names">Valerie</span> Karl</a>, <a class="entryAuthor" href="/action/doSearch?Contrib=Uhrig%2C+Peter"><span class="hlFld-ContribAuthor"></span></a><a href="/author/Uhrig%2C+Peter"><span class="NLM_given-names">Peter</span> Uhrig</a>, <a class="entryAuthor" href="/action/doSearch?Contrib=Maier%2C+Andreas"><span class="hlFld-ContribAuthor"></span></a><a href="/author/Maier%2C+Andreas"><span class="NLM_given-names">Andreas</span> Maier</a> &amp; <a class="entryAuthor" href="/action/doSearch?Contrib=Krauss%2C+Patrick"><span class="hlFld-ContribAuthor"></span></a><a href="/author/Krauss%2C+Patrick"><span class="NLM_given-names">Patrick</span> Krauss</a> <a href="https://orcid.org/0000-0002-6611-7733"><img src="/templates/jsp/images/orcid.png"></a>
     </div>
    </div>
    <div class="articleLowerInfo articleInfoSection">
     <div class="articleLowerInfoSection articleInfoDOI">
      <a href="https://doi.org/10.1080/23273798.2020.1803375">https://doi.org/10.1080/23273798.2020.1803375</a>
     </div>
     <div class="articleInfoPublicationDate articleLowerInfoSection border">
      <h6>Published online:</h6>10 August 2020
     </div>
    </div>
   </div>
   <div class="figureThumbnailContainer">
    <div class="figureInfo">
     <div class="short-legend">
      <p class="captionText"><span class="captionLabel">Figure 9. </span> Normalised power spectra for words and silence. Shown are exemplary data of book parts number 1–3 of 10 from subject 2 of 15. (a–c) Power spectra for word offset, i.e. silence. (d–f) Power spectra for word onsets. (a,d) Alpha frequency range. (b,e) Beta frequency range. (c,f) Gamma frequency range.</p>
     </div>
    </div><a href="#" class="thumbnail" aria-label="thumbnail image"><img id="F0009image" src="/na101/home/literatum/publisher/tandf/journals/content/plcp21/2021/plcp21.v036.i02/23273798.2020.1803375/20210511/images/medium/plcp_a_1803375_f0009_oc.jpg" loading="lazy" height="243" width="500"></a>
    <div class="figureDownloadOptions">
     <a href="#" class="downloadBtn btn btn-sm" role="button">Display full size</a>
    </div>
   </div>
  </div>
  <div class="hidden rs_skip" id="fig-description-F0009">
   <p class="captionText"><span class="captionLabel">Figure 9. </span> Normalised power spectra for words and silence. Shown are exemplary data of book parts number 1–3 of 10 from subject 2 of 15. (a–c) Power spectra for word offset, i.e. silence. (d–f) Power spectra for word onsets. (a,d) Alpha frequency range. (b,e) Beta frequency range. (c,f) Gamma frequency range.</p>
  </div>
  <div class="hidden rs_skip" id="figureFootNote-F0009">
   <div class="figureFootNote-F0009"></div>
  </div>
  <div class="figure figureViewer" id="F0010">
   <div class="hidden figureViewerArticleInfo">
    <span class="figViewerTitle">Analysis of continuous neuronal activity evoked by natural speech with computational corpus linguistics methods</span>
    <div class="articleAuthors articleInfoSection">
     <div class="authorsHeading">
      All authors
     </div>
     <div class="authors">
      <a class="entryAuthor" href="/action/doSearch?Contrib=Schilling%2C+Achim"><span class="hlFld-ContribAuthor"></span></a><a href="/author/Schilling%2C+Achim"><span class="NLM_given-names">Achim</span> Schilling</a>, <a class="entryAuthor" href="/action/doSearch?Contrib=Tomasello%2C+Rosario"><span class="hlFld-ContribAuthor"></span></a><a href="/author/Tomasello%2C+Rosario"><span class="NLM_given-names">Rosario</span> Tomasello</a>, <a class="entryAuthor" href="/action/doSearch?Contrib=Henningsen-Schomers%2C+Malte+R"><span class="hlFld-ContribAuthor"></span></a><a href="/author/Henningsen-Schomers%2C+Malte+R"><span class="NLM_given-names">Malte R.</span> Henningsen-Schomers</a>, <a class="entryAuthor" href="/action/doSearch?Contrib=Zankl%2C+Alexandra"><span class="hlFld-ContribAuthor"></span></a><a href="/author/Zankl%2C+Alexandra"><span class="NLM_given-names">Alexandra</span> Zankl</a>, <a class="entryAuthor" href="/action/doSearch?Contrib=Surendra%2C+Kishore"><span class="hlFld-ContribAuthor"></span></a><a href="/author/Surendra%2C+Kishore"><span class="NLM_given-names">Kishore</span> Surendra</a>, <a class="entryAuthor" href="/action/doSearch?Contrib=Haller%2C+Martin"><span class="hlFld-ContribAuthor"></span></a><a href="/author/Haller%2C+Martin"><span class="NLM_given-names">Martin</span> Haller</a>, <a class="entryAuthor" href="/action/doSearch?Contrib=Karl%2C+Valerie"><span class="hlFld-ContribAuthor"></span></a><a href="/author/Karl%2C+Valerie"><span class="NLM_given-names">Valerie</span> Karl</a>, <a class="entryAuthor" href="/action/doSearch?Contrib=Uhrig%2C+Peter"><span class="hlFld-ContribAuthor"></span></a><a href="/author/Uhrig%2C+Peter"><span class="NLM_given-names">Peter</span> Uhrig</a>, <a class="entryAuthor" href="/action/doSearch?Contrib=Maier%2C+Andreas"><span class="hlFld-ContribAuthor"></span></a><a href="/author/Maier%2C+Andreas"><span class="NLM_given-names">Andreas</span> Maier</a> &amp; <a class="entryAuthor" href="/action/doSearch?Contrib=Krauss%2C+Patrick"><span class="hlFld-ContribAuthor"></span></a><a href="/author/Krauss%2C+Patrick"><span class="NLM_given-names">Patrick</span> Krauss</a> <a href="https://orcid.org/0000-0002-6611-7733"><img src="/templates/jsp/images/orcid.png"></a>
     </div>
    </div>
    <div class="articleLowerInfo articleInfoSection">
     <div class="articleLowerInfoSection articleInfoDOI">
      <a href="https://doi.org/10.1080/23273798.2020.1803375">https://doi.org/10.1080/23273798.2020.1803375</a>
     </div>
     <div class="articleInfoPublicationDate articleLowerInfoSection border">
      <h6>Published online:</h6>10 August 2020
     </div>
    </div>
   </div>
   <div class="figureThumbnailContainer">
    <div class="figureInfo">
     <div class="short-legend">
      <p class="captionText"><span class="captionLabel">Figure 10. </span> Normalised power spectra for content and function words. Shown are exemplary data of book parts number 1–3 of 10 from subject 2 of 15. (a–c) Power spectra for content words. (d–f) Power spectra for function words. a,d: Alpha frequency range. (b,e) Beta frequency range. (c,f) Gamma frequency range.</p>
     </div>
    </div><a href="#" class="thumbnail" aria-label="thumbnail image"><img id="F0010image" src="/na101/home/literatum/publisher/tandf/journals/content/plcp21/2021/plcp21.v036.i02/23273798.2020.1803375/20210511/images/medium/plcp_a_1803375_f0010_oc.jpg" loading="lazy" height="242" width="500"></a>
    <div class="figureDownloadOptions">
     <a href="#" class="downloadBtn btn btn-sm" role="button">Display full size</a>
    </div>
   </div>
  </div>
  <div class="hidden rs_skip" id="fig-description-F0010">
   <p class="captionText"><span class="captionLabel">Figure 10. </span> Normalised power spectra for content and function words. Shown are exemplary data of book parts number 1–3 of 10 from subject 2 of 15. (a–c) Power spectra for content words. (d–f) Power spectra for function words. a,d: Alpha frequency range. (b,e) Beta frequency range. (c,f) Gamma frequency range.</p>
  </div>
  <div class="hidden rs_skip" id="figureFootNote-F0010">
   <div class="figureFootNote-F0010"></div>
  </div>
  <p></p>
 </div>
</div>
<div id="S004" class="NLM_sec NLM_sec_level_1">
 <h2 id="_i31" class="section-heading-2">Discussion</h2>
 <p>In this study, we presented an approach where we combine electrophysiological assessment of neuronal activity with computational corpus linguistics, in order to create a corpus as defined in Jurafsky and Martin (<span class="ref-lnk lazy-ref"><a data-rid="CIT0058" data-reflink="_i34 _i35 _i36" href="#"><span class="off-screen">Citation</span>2014</a></span>) of continuous speech-evoked neuronal activity. We demonstrated that using an audio book as natural speech stimulus, and simultaneously performing MEG measurements led to a relatively large number of analysable events (word onsets: <i>n</i>=291, silence onsets: <i>n</i> = 187, content words: <i>n</i> = 81, function words: <i>n</i> = 106), yet within a relatively short measurement time of <span class="NLM_disp-formula-image inline-formula rs_preserve">
   <noscript>
    <img src="/na101/home/literatum/publisher/tandf/journals/content/plcp21/2021/plcp21.v036.i02/23273798.2020.1803375/20210511/images/plcp_a_1803375_ilm0020.gif" alt="">
   </noscript><img src="//:0" alt="" class="mml-formula" data-formula-source="{&quot;type&quot; : &quot;image&quot;, &quot;src&quot; : &quot;/na101/home/literatum/publisher/tandf/journals/content/plcp21/2021/plcp21.v036.i02/23273798.2020.1803375/20210511/images/plcp_a_1803375_ilm0020.gif&quot;}"><span class="mml-formula"></span></span><span class="NLM_disp-formula inline-formula rs_preserve"><img src="//:0" alt="" data-formula-source="{&quot;type&quot; : &quot;mathjax&quot;}">
   <math>
    <mn>
     15
    </mn><mspace width="thinmathspace"></mspace><mi>
     m
    </mi><mi>
     i
    </mi><mi>
     n
    </mi>
   </math></span>. We further provided the proof-of-principle that, in contrast to common study designs, even though our stimulus trials were not presented in isolation, i.e. with appropriate inter-stimulus intervals of a few seconds, averaging over all respective events of a certain condition results in ERFs in left temporal and frontal channels with increased amplitudes compared to those of several control channels (e.g. at right hemisphere or at occipital lobe). The same is true with respect to comparison with control conditions (e.g. random trigger times). These results are well in line with previously published findings (Friederici &amp; Gierhan, <span class="ref-lnk lazy-ref"><a data-rid="CIT0033" data-reflink="_i34 _i35 _i36" href="#"><span class="off-screen">Citation</span>2013</a></span>).</p>
 <p>Furthermore, we analysed ERFs for different categories of words. Although, a frequently investigated and contrasted pair of word classes is that of nouns and verbs (Damasio &amp; Tranel, <span class="ref-lnk lazy-ref"><a data-rid="CIT0021" data-reflink="_i34 _i35 _i36" href="#"><span class="off-screen">Citation</span>1993</a></span>; Preissl et al., <span class="ref-lnk lazy-ref"><a data-rid="CIT0102" data-reflink="_i34 _i35 _i36" href="#"><span class="off-screen">Citation</span>1995</a></span>; Pulvermüller et al., <span class="ref-lnk lazy-ref"><a data-rid="CIT0106" data-reflink="_i34 _i35 _i36" href="#"><span class="off-screen">Citation</span>1999</a></span>, <span class="ref-lnk lazy-ref"><a data-rid="CIT0108" data-reflink="_i34 _i35 _i36" href="#"><span class="off-screen">Citation</span>1996</a></span>; Tsigka et al., <span class="ref-lnk lazy-ref"><a data-rid="CIT0130" data-reflink="_i34 _i35 _i36" href="#"><span class="off-screen">Citation</span>2014</a></span>; Vigliocco et al., <span class="ref-lnk lazy-ref"><a data-rid="CIT0133" data-reflink="_i34 _i35 _i36" href="#"><span class="off-screen">Citation</span>2011</a></span>), for the present study, we opted for the distinction between function words, defined as determiners, prepositions and conjunctions, and content words, defined as nouns, verbs and adjectives. These lexical categories are also frequently used in neuroimaging studies on the neurobiology of language (Bell et al., <span class="ref-lnk lazy-ref"><a data-rid="CIT0009" data-reflink="_i34 _i35 _i36" href="#"><span class="off-screen">Citation</span>2009</a></span>; Bird et al., <span class="ref-lnk lazy-ref"><a data-rid="CIT0010" data-reflink="_i34 _i35 _i36" href="#"><span class="off-screen">Citation</span>2002</a></span>; Diaz &amp; McCarthy, <span class="ref-lnk lazy-ref"><a data-rid="CIT0025" data-reflink="_i34 _i35 _i36" href="#"><span class="off-screen">Citation</span>2009</a></span>; Keurs et al., <span class="ref-lnk lazy-ref"><a data-rid="CIT0063" data-reflink="_i34 _i35 _i36" href="#"><span class="off-screen">Citation</span>1995</a></span>; Mohr et al., <span class="ref-lnk lazy-ref"><a data-rid="CIT0091" data-reflink="_i34 _i35 _i36" href="#"><span class="off-screen">Citation</span>1994</a></span>; Pulvermüller et al., <span class="ref-lnk lazy-ref"><a data-rid="CIT0110" data-reflink="_i34 _i35 _i36" href="#"><span class="off-screen">Citation</span>2009</a></span>). In addition, they differ greatly in the semantic domain, and cover more fully the the totality of the words than the categories of nouns and verbs, since nouns and verbs are both included in the content word category. We found a clear N400 component (cf. <a href="#F0008">Figure 8</a>) especially in left hemispheric frontal regions for both function and content words and a positive component from 400-700 ms which is in line with Brennan et al.'s findings (Brennan &amp; Pylkkänen, <span class="ref-lnk lazy-ref"><a data-rid="CIT0012" data-reflink="_i34 _i35 _i36" href="#"><span class="off-screen">Citation</span>2012</a></span>, <span class="ref-lnk lazy-ref"><a data-rid="CIT0013" data-reflink="_i34 _i35 _i36" href="#"><span class="off-screen">Citation</span>2017</a></span>). Additionally, we found that content words elicit greater activation than function words, especially in temporal and frontal areas of the left hemisphere. Due to their substantial semantic differences (Kemmerer, <span class="ref-lnk lazy-ref"><a data-rid="CIT0061" data-reflink="_i34 _i35 _i36" href="#"><span class="off-screen">Citation</span>2014</a></span>; Pulvermüller, <span class="ref-lnk lazy-ref"><a data-rid="CIT0104" data-reflink="_i34 _i35 _i36" href="#"><span class="off-screen">Citation</span>2003</a></span>), this finding is in line with previous studies (Diaz &amp; McCarthy, <span class="ref-lnk lazy-ref"><a data-rid="CIT0025" data-reflink="_i34 _i35 _i36" href="#"><span class="off-screen">Citation</span>2009</a></span>).</p>
 <p>With respect to the average normalised power spectra, it was found that presentation of speech stimuli was associated with an increase in broadband gamma and a decrease in alpha over auditory cortex, while alpha power was increased in domain unspecific cortical areas (Archila-Meléndez et al., <span class="ref-lnk lazy-ref"><a data-rid="CIT0002" data-reflink="_i34 _i35 _i36" href="#"><span class="off-screen">Citation</span>2018</a></span>; Müller &amp; Weisz, <span class="ref-lnk lazy-ref"><a data-rid="CIT0094" data-reflink="_i34 _i35 _i36" href="#"><span class="off-screen">Citation</span>2012</a></span>; Weisz et al., <span class="ref-lnk lazy-ref"><a data-rid="CIT0135" data-reflink="_i34 _i35 _i36" href="#"><span class="off-screen">Citation</span>2011</a></span>). One reason could be that, since we analysed only very short periods of silence, i.e. between two words, our two conditions of word onset and silence onset can be considered basically, at a larger time scale, to be the same condition, i.e. continuous speech stimulation. This may explain why we found no differences in frequency power here. Even though it has been proposed that in human language networks linguistic information of different types is transferred in different oscillatory bands – in particular attention is assumed to correlate with an increase in gamma and a decrease in alpha band power (Bastiaansen &amp; Hagoort, <span class="ref-lnk lazy-ref"><a data-rid="CIT0007" data-reflink="_i34 _i35 _i36" href="#"><span class="off-screen">Citation</span>2006</a></span>) – the role of different spectral bands in mediating cognitive processes is still not fully understood. Therefore, it remains unclear, whether these findings extend to content and function words. Whether our approach is too insensitive to see differences here remains to be seen and further studies should look more closely at this issue.</p>
 <p>As mentioned above, in contrast to traditional studies that are limited to testing only a small number of stimuli or word categories, the present approach opens the possibility to explore the neuronal correlates underlying different word meaning information across a large range of semantic categories (Huth et al., <span class="ref-lnk lazy-ref"><a data-rid="CIT0056" data-reflink="_i34 _i35 _i36" href="#"><span class="off-screen">Citation</span>2016</a></span>), and syntactic structures (Kaan &amp; Swaab, <span class="ref-lnk lazy-ref"><a data-rid="CIT0059" data-reflink="_i34 _i35 _i36" href="#"><span class="off-screen">Citation</span>2002</a></span>). This is because the ongoing natural speech used here contains both, a large number of words from different semantic domains (Wehbe et al., <span class="ref-lnk lazy-ref"><a data-rid="CIT0134" data-reflink="_i34 _i35 _i36" href="#"><span class="off-screen">Citation</span>2014</a></span>) and a large number of sentences at all levels of linguistic complexity (Bates, <span class="ref-lnk lazy-ref"><a data-rid="CIT0008" data-reflink="_i34 _i35 _i36" href="#"><span class="off-screen">Citation</span>1999</a></span>).</p>
 <p>On the other hand, one may argue that stimulation with ongoing natural speech has, compared to traditional approaches, the drawback that there are virtually no inter-stimulus intervals between the single words. This, of course, introduces a mixture of effects at different temporal scales, e.g. early responses to the actual word are confounded with late responses of the previous word. However, all these effects may be averaged out, as demonstrated by other studies (Brodbeck et al., <span class="ref-lnk lazy-ref"><a data-rid="CIT0014" data-reflink="_i34 _i35 _i36" href="#"><span class="off-screen">Citation</span>2018</a></span>; Broderick et al., <span class="ref-lnk lazy-ref"><a data-rid="CIT0015" data-reflink="_i34 _i35 _i36" href="#"><span class="off-screen">Citation</span>2018</a></span>; Deniz et al., <span class="ref-lnk lazy-ref"><a data-rid="CIT0024" data-reflink="_i34 _i35 _i36" href="#"><span class="off-screen">Citation</span>2019</a></span>; Ding &amp; Simon, <span class="ref-lnk lazy-ref"><a data-rid="CIT0026" data-reflink="_i34 _i35 _i36" href="#"><span class="off-screen">Citation</span>2012</a></span>; Silbert et al., <span class="ref-lnk lazy-ref"><a data-rid="CIT0121" data-reflink="_i34 _i35 _i36" href="#"><span class="off-screen">Citation</span>2014</a></span>) and also by our results.</p>
 <p>In a follow-up study, it will have to be validated whether our approach also works for linguistic units of different complexity other than single words. For instance, smaller linguistic units such as phonemes and morphemes, but also larger linguistic units like collocations, phrases, clauses, sentences, or even beyond, could be investigated. For instance, we might be able to determine what neural correlates of the different association measures used in research on collocation look like (see Evert et al., <span class="ref-lnk lazy-ref"><a data-rid="CIT0029" data-reflink="_i34 _i35 _i36" href="#"><span class="off-screen">Citation</span>2017</a></span> for an overview and further references). Furthermore, more abstract linguistic phenomena need to be analysed, e.g. argument structure constructions (Goldberg, <span class="ref-lnk lazy-ref"><a data-rid="CIT0038" data-reflink="_i34 _i35 _i36" href="#"><span class="off-screen">Citation</span>1995</a></span>, <span class="ref-lnk lazy-ref"><a data-rid="CIT0039" data-reflink="_i34 _i35 _i36" href="#"><span class="off-screen">Citation</span>2003</a></span>, <span class="ref-lnk lazy-ref"><a data-rid="CIT0040" data-reflink="_i34 _i35 _i36" href="#"><span class="off-screen">Citation</span>2006</a></span>) or valency (Herbst, <span class="ref-lnk lazy-ref"><a data-rid="CIT0052" data-reflink="_i34 _i35 _i36" href="#"><span class="off-screen">Citation</span>2011</a></span>, <span class="ref-lnk lazy-ref"><a data-rid="CIT0053" data-reflink="_i34 _i35 _i36" href="#"><span class="off-screen">Citation</span>2014</a></span>; Herbst &amp; Schüller, <span class="ref-lnk lazy-ref"><a data-rid="CIT0054" data-reflink="_i34 _i35 _i36" href="#"><span class="off-screen">Citation</span>2008</a></span>). Finally, our speech-evoked neural data may also be grouped, averaged, and subsequently contrasted according to male and female voice, looking at gender-specific differences (see e.g. Özçalışkan &amp; Goldin-Meadow, <span class="ref-lnk lazy-ref"><a data-rid="CIT0098" data-reflink="_i34 _i35 _i36" href="#"><span class="off-screen">Citation</span>2010</a></span>; Proverbio et al., <span class="ref-lnk lazy-ref"><a data-rid="CIT0103" data-reflink="_i34 _i35 _i36" href="#"><span class="off-screen">Citation</span>2014</a></span>).</p>
 <p>Also, analyses based on source space need to be tested, as well as more sophisticated analyses taking advantage of the multi-dimensionality of the data, such as, for instance, multi-dimensional cluster statistics (Krauss, Metzner, et al., <span class="ref-lnk lazy-ref"><a data-rid="CIT0068" data-reflink="_i34 _i35 _i36" href="#"><span class="off-screen">Citation</span>2018</a></span>; Krauss, Schilling, et al., <span class="ref-lnk lazy-ref"><a data-rid="CIT0070" data-reflink="_i34 _i35 _i36" href="#"><span class="off-screen">Citation</span>2018</a></span>). In addition, state-of-the-art deep learning approaches may be used as a tool for analysing brain data, e.g. for creating so-called <i>embeddings</i> of the raw data (Krauss et al., <span class="ref-lnk lazy-ref"><a data-rid="CIT0066" data-reflink="_i34 _i35 _i36" href="#"><span class="off-screen">Citation</span>2020</a></span>). Moreover, as proposed by Kriegeskorte and Douglas (<span class="ref-lnk lazy-ref"><a data-rid="CIT0075" data-reflink="_i34 _i35 _i36" href="#"><span class="off-screen">Citation</span>2018</a></span>), our neural corpus can serve to test (Schilling et al., <span class="ref-lnk lazy-ref"><a data-rid="CIT0116" data-reflink="_i34 _i35 _i36" href="#"><span class="off-screen">Citation</span>2018</a></span>) computational models of brain function (Krauss et al., <span class="ref-lnk lazy-ref"><a data-rid="CIT0067" data-reflink="_i34 _i35 _i36" href="#"><span class="off-screen">Citation</span>2017</a></span>, <span class="ref-lnk lazy-ref"><a data-rid="CIT0072" data-reflink="_i34 _i35 _i36" href="#"><span class="off-screen">Citation</span>2016</a></span>; Krauss, Tziridis, et al., <span class="ref-lnk lazy-ref"><a data-rid="CIT0073" data-reflink="_i34 _i35 _i36" href="#"><span class="off-screen">Citation</span>2018</a></span>; Schilling, Tziridis, et al., <span class="ref-lnk lazy-ref"><a data-rid="CIT0117" data-reflink="_i34 _i35 _i36" href="#"><span class="off-screen">Citation</span>2020</a></span>), in particular models based on neural networks (Krauss, Prebeck, et al., <span class="ref-lnk lazy-ref"><a data-rid="CIT0069" data-reflink="_i34 _i35 _i36" href="#"><span class="off-screen">Citation</span>2019</a></span>; Krauss, Schuster, et al., <span class="ref-lnk lazy-ref"><a data-rid="CIT0071" data-reflink="_i34 _i35 _i36" href="#"><span class="off-screen">Citation</span>2019</a></span>; Krauss, Zankl, et al., <span class="ref-lnk lazy-ref"><a data-rid="CIT0074" data-reflink="_i34 _i35 _i36" href="#"><span class="off-screen">Citation</span>2019</a></span>) and machine learning architectures (Gerum et al., <span class="ref-lnk lazy-ref"><a data-rid="CIT0037" data-reflink="_i34 _i35 _i36" href="#"><span class="off-screen">Citation</span>2020</a></span>; Schilling, Gerum, et al., <span class="ref-lnk lazy-ref"><a data-rid="CIT0115" data-reflink="_i34 _i35 _i36" href="#"><span class="off-screen">Citation</span>2020</a></span>), in order to iteratively increase biological and cognitive fidelity (Kriegeskorte &amp; Douglas, <span class="ref-lnk lazy-ref"><a data-rid="CIT0075" data-reflink="_i34 _i35 _i36" href="#"><span class="off-screen">Citation</span>2018</a></span>).</p>
 <p>Due to the corpus-like features of our data, all additional analyses mentioned may be performed on the existing database, and without the need for designing new stimulation paradigms, or carrying out additional measurements.</p>
 <p>However, in order to avoid statistical errors due to <i>HARKing</i> (Kerr, <span class="ref-lnk lazy-ref"><a data-rid="CIT0062" data-reflink="_i34 _i35 _i36" href="#"><span class="off-screen">Citation</span>1998</a></span>; Munafò et al., <span class="ref-lnk lazy-ref"><a data-rid="CIT0095" data-reflink="_i34 _i35 _i36" href="#"><span class="off-screen">Citation</span>2017</a></span>) – defined as generating scientific statements exclusively based on the analysis of huge data sets without previous hypotheses – and to guarantee consistency of the data, it is necessary to apply e.g. re-sampling techniques such as sub-sampling as shown above and described in detail in Schilling et al. (<span class="ref-lnk lazy-ref"><a data-rid="CIT0114" data-reflink="_i34 _i35 _i36" href="#"><span class="off-screen">Citation</span>2019</a></span>). Furthermore, the approach presented here allows us to apply the well-established machine learning practice of data set splitting, i.e. to split the dataset into multiple parts before the beginning of the evaluation, where the one part is used for generating new hypotheses, and another part for subsequently testing these hypotheses (or split again into training and testing data). However, since we recorded a whole story, possible order effects should be taken into account for dataset splitting. Hence, instead of splitting the data set according to the chronological order, e.g. using the first parts of the audio book as training, and the subsequent parts as test dataset, it should better be split randomly.</p>
 <p>To conclude, there are two major reasons why we think the study of the neurobiology of language can benefit tremendously from the introduction of corpus-linguistic methodology.</p>
 <p>The first is that we can base our research on naturally occurring language, which should make them more ecologically valid than the more artificial stimuli used in carefully balanced and controlled experiments. Of course, even though audio books are frequently used in similar studies (Brodbeck et al., <span class="ref-lnk lazy-ref"><a data-rid="CIT0014" data-reflink="_i34 _i35 _i36" href="#"><span class="off-screen">Citation</span>2018</a></span>; Broderick et al., <span class="ref-lnk lazy-ref"><a data-rid="CIT0015" data-reflink="_i34 _i35 _i36" href="#"><span class="off-screen">Citation</span>2018</a></span>; Deniz et al., <span class="ref-lnk lazy-ref"><a data-rid="CIT0024" data-reflink="_i34 _i35 _i36" href="#"><span class="off-screen">Citation</span>2019</a></span>; Ding &amp; Simon, <span class="ref-lnk lazy-ref"><a data-rid="CIT0026" data-reflink="_i34 _i35 _i36" href="#"><span class="off-screen">Citation</span>2012</a></span>; Silbert et al., <span class="ref-lnk lazy-ref"><a data-rid="CIT0121" data-reflink="_i34 _i35 _i36" href="#"><span class="off-screen">Citation</span>2014</a></span>), one may also discuss whether audio books actually can be considered natural speech. One could argue that the fact that highly trained professional speakers and actors are usually employed to read audio books, who may use specific intonational patterns to paint a more vivid image of the situation, may lead to unnaturalness and thus possibly to unusual arousal patterns in the hearer. However, this argument is flawed. People spend large portions of their days listening to language produced by such professional speakers for radio, television news and drama, online videos, and podcasts. While probably not predominant for most people, it corresponds to a perfectly normal, everyday type of language experience. Even if we expect deviations from spoken interaction in such stimuli, we could even exploit this to study brain responses to creative language use (see Uhrig, <span class="ref-lnk lazy-ref"><a data-rid="CIT0131" data-reflink="_i34 _i35 _i36" href="#"><span class="off-screen">Citation</span>2018</a></span>, <span class="ref-lnk lazy-ref"><a data-rid="CIT0132" data-reflink="_i34 _i35 _i36" href="#"><span class="off-screen">Citation</span>2020</a></span> and the sources cited there for linguistic studies of creativity). Of course, further studies using recordings of everyday dialogues between untrained subjects, e.g. describing what they have done during the day, should be designed to obtain a more comprehensive picture and more robust results, because, as Kriegeskorte and Douglas pointed out that <i>“as we engage all aspects of the human mind, our tasks will need to simulate natural environments”</i> (Kriegeskorte &amp; Douglas, <span class="ref-lnk lazy-ref"><a data-rid="CIT0075" data-reflink="_i34 _i35 _i36" href="#"><span class="off-screen">Citation</span>2018</a></span>). Still, purely receptive task such as the one used in this study is one type of natural environment, and one that can be studied without too much interference compared to, say, spontaneous interaction.</p>
 <p>The second reason is the fact that measurements can be re-used if they form part of a large corpus of neuroimaging results. Let us look at a few numbers: In the present study, we stimulated 15 participants with 40 min of audio each. Test time spent in the MEG was 60 min due to the questions and pauses mentioned above. With 30 min of preparation, we used the MEG lab for a total of 22.5 h during experimentation. In that period of time, we gathered measurements for roughly 6000 words perceived by 15 participants, totalling 90,000 sets of brain responses to words. These correspond to roughly 35 GB of measurements (4 bytes per value, 1000 per second, 248 channels, 40 min per participant, 15 participants). For this study, we only looked at a tiny fraction of the data (words preceded by a short pause of at least 50 ms in the first 12 min) and already managed to confirm certain patterns found by previous studies with a strict experimental design. If we assume that pauses are equally distributed across the corpus, we can expect to find roughly 1000 such events, with 15 participants for each, i.e. 15,000 data points alone for words preceded by silence. Having these plus all the other words in their immediate linguistics contexts without pauses opens many more avenues for interesting research question at no added laboratory costs. Once we start looking at all words, we expect that the noise introduced through not being able to control for a variety of factors will be counterbalanced by the sheer size of data sets constructed using the methodology presented.</p>
 <p>By that, we agree with the view of Hamilton and Huth that <i>“natural stimuli offer many advantages over simplified, controlled stimuli for studying how language is processed by the brain”</i>, and that <i>“the downsides of using natural language stimuli can be mitigated using modern statistical and computational techniques”</i> (Hamilton &amp; Huth, <span class="ref-lnk lazy-ref"><a data-rid="CIT0048" data-reflink="_i34 _i35 _i36" href="#"><span class="off-screen">Citation</span>2020</a></span>).</p>
</div>