<div id="s0001" class="NLM_sec NLM_sec-type_intro NLM_sec_level_1">
 <h2 id="_i6" class="section-heading-2">1. Introduction</h2>
 <p>Approximately one in five adults in the United States (U.S.) struggle with mental illness [<span class="ref-lnk lazy-ref"><a data-rid="cit0001 cit0002 cit0003" data-reflink="_i29 _i30 _i31 _i32" href="#"><span class="off-screen">Citation</span>1–3</a></span>]; however, many of these individuals are not receiving treatment. For example, in one nationally representative sample, only 41.1% of U.S. adults with a diagnosis of anxiety, mood, impulse control, and/or substance disorders received treatment in the previous 12&nbsp;months [<span class="ref-lnk lazy-ref"><a data-rid="cit0004" data-reflink="_i29 _i30 _i31 _i32" href="#"><span class="off-screen">Citation</span>4</a></span>]. Worldwide, it is estimated that 70% of people with mental illness receive no formal treatment [<span class="ref-lnk lazy-ref"><a data-rid="cit0005" data-reflink="_i29 _i30 _i31 _i32" href="#"><span class="off-screen">Citation</span>5</a></span>]. While some individuals do not seek treatment because of low perceived need [<span class="ref-lnk lazy-ref"><a data-rid="cit0006" data-reflink="_i29 _i30 _i31 _i32" href="#"><span class="off-screen">Citation</span>6</a></span>] or attitudinal barriers like perceived stigma [<span class="ref-lnk lazy-ref"><a data-rid="cit0007" data-reflink="_i29 _i30 _i31 _i32" href="#"><span class="off-screen">Citation</span>7</a></span>], those who desire treatment may not be able to receive it quickly due to a shortage of mental health professionals, particularly in rural and low income areas [<span class="ref-lnk lazy-ref"><a data-rid="cit0008" data-reflink="_i29 _i30 _i31 _i32" href="#"><span class="off-screen">Citation</span>8</a></span>]. Indeed, primary care physicians report that obtaining outpatient mental health services for patients is more difficult than other common referrals [<span class="ref-lnk lazy-ref"><a data-rid="cit0009" data-reflink="_i29 _i30 _i31 _i32" href="#"><span class="off-screen">Citation</span>9</a></span>].</p>
 <p>Issues with the accessibility of mental health care garnered widespread attention during the COVID-19 pandemic, when access to care became more difficult. In addition to typical barriers to treatment, restrictions and lockdowns enacted to mitigate the spread of COVID-19 caused widespread disruptions to face-to-face care. In a survey conducted by the World Health Organization in Summer 2020, 60% of the 130 reporting countries indicated disruptions to mental health services, and disruptions to services for older adults and for youth were closer to 70% [<span class="ref-lnk lazy-ref"><a data-rid="cit0010" data-reflink="_i29 _i30 _i31 _i32" href="#"><span class="off-screen">Citation</span>10</a></span>]. In particular, travel restrictions and the closure of community-based mental health services near people’s homes led to even greater difficulties accessing care for those in lower-income countries [<span class="ref-lnk lazy-ref"><a data-rid="cit0010" data-reflink="_i29 _i30 _i31 _i32" href="#"><span class="off-screen">Citation</span>10</a></span>].</p>
 <p>Accessibility issues have been compounded by the fact that there has been greater demand for mental health services during the pandemic. In the U.S., more than one-third of the population experienced depression or anxiety symptoms, almost a three-fold increase compared to 2019 [<span class="ref-lnk lazy-ref"><a data-rid="cit0011" data-reflink="_i29 _i30 _i31 _i32" href="#"><span class="off-screen">Citation</span>11</a></span>]. . Mental and physical health issues, such as difficulty with sleeping and/or eating, the reemergence of trauma symptoms, substance abuse, and worsening of chronic conditions, have also increased during the pandemic [<span class="ref-lnk lazy-ref"><a data-rid="cit0012" data-reflink="_i29 _i30 _i31 _i32" href="#"><span class="off-screen">Citation</span>12</a></span>,<span class="ref-lnk lazy-ref"><a data-rid="cit0013" data-reflink="_i29 _i30 _i31 _i32" href="#"><span class="off-screen">Citation</span>13</a></span>]. Vulnerable and marginalized populations have experienced the greatest increase in mental health concerns, perhaps due to the larger unmet need for care among these groups [<span class="ref-lnk lazy-ref"><a data-rid="cit0014" data-reflink="_i29 _i30 _i31 _i32" href="#"><span class="off-screen">Citation</span>14</a></span>,<span class="ref-lnk lazy-ref"><a data-rid="cit0015" data-reflink="_i29 _i30 _i31 _i32" href="#"><span class="off-screen">Citation</span>15</a></span>]</p>
 <p>Altogether, this has created a perfect storm where an already overwhelmed system has fewer resources and greater demand, causing an increase in unmet need for mental health services worldwide. Not surprisingly, research suggests the unmet need for psychotherapy and counseling, in addition to the disruption of traditional services, has increased during the COVID-19 pandemic [<span class="ref-lnk lazy-ref"><a data-rid="cit0016" data-reflink="_i29 _i30 _i31 _i32" href="#"><span class="off-screen">Citation</span>16</a></span>]. More specifically, over a quarter of individuals with elevated levels of depression and anxiety, and 12.8% of the U.S. population, reported an unmet need for mental health counseling or therapy [<span class="ref-lnk lazy-ref"><a data-rid="cit0015" data-reflink="_i29 _i30 _i31 _i32" href="#"><span class="off-screen">Citation</span>15</a></span>].</p>
</div>
<div id="s0002" class="NLM_sec NLM_sec_level_1">
 <h2 id="_i7" class="section-heading-2">2. Digital mental health interventions</h2>
 <p>As access to smartphones and the Internet continues to increase [<span class="ref-lnk lazy-ref"><a data-rid="cit0017" data-reflink="_i29 _i30 _i31 _i32" href="#"><span class="off-screen">Citation</span>17</a></span>], digital solutions to mental health care are an important avenue to addressing accessibility issues with traditional in-person care [<span class="ref-lnk lazy-ref"><a data-rid="cit0018" data-reflink="_i29 _i30 _i31 _i32" href="#"><span class="off-screen">Citation</span>18</a></span>]. Digital mental health interventions (DMHIs) can often be disseminated to large populations [<span class="ref-lnk lazy-ref"><a data-rid="cit0019" data-reflink="_i29 _i30 _i31 _i32" href="#"><span class="off-screen">Citation</span>19</a></span>,<span class="ref-lnk lazy-ref"><a data-rid="cit0020" data-reflink="_i29 _i30 _i31 _i32" href="#"><span class="off-screen">Citation</span>20</a></span>] and underserved areas [<span class="ref-lnk lazy-ref"><a data-rid="cit0021" data-reflink="_i29 _i30 _i31 _i32" href="#"><span class="off-screen">Citation</span>21</a></span>]. In fact, in response to concerns about the impact of the pandemic on mental health and accessibility concerns for mental health services, the U.S. Food and Drug Administration (FDA) released new temporary guidance in April 2020 allowing for the distribution of digital therapeutics for some psychiatric conditions, including major depressive disorder and generalized anxiety disorder, without traditional clearance [<span class="ref-lnk lazy-ref"><a data-rid="cit0022" data-reflink="_i29 _i30 _i31 _i32" href="#"><span class="off-screen">Citation</span>22</a></span>,<span class="ref-lnk lazy-ref"><a data-rid="cit0023" data-reflink="_i29 _i30 _i31 _i32" href="#"><span class="off-screen">Citation</span>23</a></span>].</p>
 <p>Although DMHIs have been on the market for over a decade [<span class="ref-lnk lazy-ref"><a data-rid="cit0024" data-reflink="_i29 _i30 _i31 _i32" href="#"><span class="off-screen">Citation</span>24</a></span>], they have become more common in recent years. It is estimated that there are over 10,000 mental health applications currently available for download [<span class="ref-lnk lazy-ref"><a data-rid="cit0025" data-reflink="_i29 _i30 _i31 _i32" href="#"><span class="off-screen">Citation</span>25</a></span>], though as few as 2% of these are supported by empirical evidence [<span class="ref-lnk lazy-ref"><a data-rid="cit0026" data-reflink="_i29 _i30 _i31 _i32" href="#"><span class="off-screen">Citation</span>26</a></span>]. However, the subset of applications tested empirically has been shown to be effective. Meta-analyses of DMHIs show small to medium effects on depression and anxiety symptoms [<span class="ref-lnk lazy-ref"><a data-rid="cit0027 cit0028 cit0029 cit0030" data-reflink="_i29 _i30 _i31 _i32" href="#"><span class="off-screen">Citation</span>27–30</a></span>], with larger effects when programs include more engagement features [<span class="ref-lnk lazy-ref"><a data-rid="cit0031" data-reflink="_i29 _i30 _i31 _i32" href="#"><span class="off-screen">Citation</span>31</a></span>], though other meta-analyses found significant pooled effects for depression but not for other psychological outcomes, including anxiety [<span class="ref-lnk lazy-ref"><a data-rid="cit0032" data-reflink="_i29 _i30 _i31 _i32" href="#"><span class="off-screen">Citation</span>32</a></span>]. Similarly, a review of DMHIs among college students found that most programs (81%) were at least partially effective in improving depressive symptoms, anxiety, psychological distress, or well-being [<span class="ref-lnk lazy-ref"><a data-rid="cit0033" data-reflink="_i29 _i30 _i31 _i32" href="#"><span class="off-screen">Citation</span>33</a></span>]. In addition, DMHIs also may be effective preventative tools (see Ebert et al., 2017, for a discussion) [<span class="ref-lnk lazy-ref"><a data-rid="cit0021" data-reflink="_i29 _i30 _i31 _i32" href="#"><span class="off-screen">Citation</span>21</a></span>].</p>
 <p>DMHIs are effective when used as recommended; however, these interventions have been criticized for low usage rates and high dropout [<span class="ref-lnk lazy-ref"><a data-rid="cit0034 cit0035 cit0036 cit0037" data-reflink="_i29 _i30 _i31 _i32" href="#"><span class="off-screen">Citation</span>34–37</a></span>]. One potential reason for the low engagement in many DMHIs may be the lack of guidance and feedback [<span class="ref-lnk lazy-ref"><a data-rid="cit0038 cit0039 cit0040" data-reflink="_i29 _i30 _i31 _i32" href="#"><span class="off-screen">Citation</span>38–40</a></span>]. Indeed, people report that the lack of contact with a therapist is a disadvantage to DMHIs [<span class="ref-lnk lazy-ref"><a data-rid="cit0041" data-reflink="_i29 _i30 _i31 _i32" href="#"><span class="off-screen">Citation</span>41</a></span>], and people using purely self-guided programs report more difficulties with using the program or remembering to engage with it [<span class="ref-lnk lazy-ref"><a data-rid="cit0040" data-reflink="_i29 _i30 _i31 _i32" href="#"><span class="off-screen">Citation</span>40</a></span>].</p>
 <p>Consequently, some interventions include guidance or support, including support from a therapist or from peers, or more administrative forms of support (i.e. non-clinical support, primarily logistical) offered by people other than clinicians (e.g. nurses, research coordinators, laypeople) [<span class="ref-lnk lazy-ref"><a data-rid="cit0042" data-reflink="_i29 _i30 _i31 _i32" href="#"><span class="off-screen">Citation</span>42</a></span>]. Research suggests this is an effective mechanism for boosting engagement. Meta-analyses have shown that dropout rates are highest for unsupported interventions and lowest for therapist-supported interventions [<span class="ref-lnk lazy-ref"><a data-rid="cit0042" data-reflink="_i29 _i30 _i31 _i32" href="#"><span class="off-screen">Citation</span>42</a></span>]. Similarly, adding therapist support via telephone coaching [<span class="ref-lnk lazy-ref"><a data-rid="cit0043" data-reflink="_i29 _i30 _i31 _i32" href="#"><span class="off-screen">Citation</span>43</a></span>] or non-clinical support via peer-to-peer feedback [<span class="ref-lnk lazy-ref"><a data-rid="cit0044" data-reflink="_i29 _i30 _i31 _i32" href="#"><span class="off-screen">Citation</span>44</a></span>] increases adherence and engagement. DMHIs with support from a therapist also tend to show greater improvement in mental health outcomes compared to unsupported interventions [<span class="ref-lnk lazy-ref"><a data-rid="cit0042" data-reflink="_i29 _i30 _i31 _i32" href="#"><span class="off-screen">Citation</span>42</a></span>,<span class="ref-lnk lazy-ref"><a data-rid="cit0045" data-reflink="_i29 _i30 _i31 _i32" href="#"><span class="off-screen">Citation</span>45</a></span>,<span class="ref-lnk lazy-ref"><a data-rid="cit0046" data-reflink="_i29 _i30 _i31 _i32" href="#"><span class="off-screen">Citation</span>46</a></span>], although other studies show no differences in outcomes despite greater engagement [<span class="ref-lnk lazy-ref"><a data-rid="cit0043" data-reflink="_i29 _i30 _i31 _i32" href="#"><span class="off-screen">Citation</span>43</a></span>,<span class="ref-lnk lazy-ref"><a data-rid="cit0044" data-reflink="_i29 _i30 _i31 _i32" href="#"><span class="off-screen">Citation</span>44</a></span>].</p>
 <p>Incorporating support from a therapist may help address the problem with engagement in DMHIs, but doing so may also reduce some perceived benefits of these interventions. For instance, people completing a DMHI for depression incorporating synchronous support by way of telephone coaching reported that scheduling these phone calls was difficult due to time constraints [<span class="ref-lnk lazy-ref"><a data-rid="cit0043" data-reflink="_i29 _i30 _i31 _i32" href="#"><span class="off-screen">Citation</span>43</a></span>]. Asynchronous support may reduce issues with scheduling, but research suggests few patients involved in a DMHI with an option to send messages to a therapist and receive asynchronous feedback took advantage of this feature; most patients sent fewer than two messages over the course of 14&nbsp;weeks [<span class="ref-lnk lazy-ref"><a data-rid="cit0047" data-reflink="_i29 _i30 _i31 _i32" href="#"><span class="off-screen">Citation</span>47</a></span>]. Furthermore, incorporating support from a live person within DMHIs diminishes the scalability of those interventions, particularly when support is offered by a therapist, given the shortage of mental health professionals.</p>
</div>
<div id="s0003" class="NLM_sec NLM_sec_level_1">
 <h2 id="_i8" class="section-heading-2">3. Overview of artificial intelligence in digital interventions</h2>
 <p>The digital mental health care industry has recently begun to incorporate AI into existing platforms and create AI-guided products [<span class="ref-lnk lazy-ref"><a data-rid="cit0048" data-reflink="_i29 _i30 _i31 _i32" href="#"><span class="off-screen">Citation</span>48</a></span>]. This is being done in many ways, such as health communication, virtual reality, symptom and biomarker monitoring, mental health triage, digital phenotyping to predict outcomes, and personalization of content [<span class="ref-lnk lazy-ref"><a data-rid="cit0049 cit0050 cit0051" data-reflink="_i29 _i30 _i31 _i32" href="#"><span class="off-screen">Citation</span>49–51</a></span>].</p>
 <p>Another popular utilization of AI in DMHIs is artificially intelligent <i>chatbots</i>, also referred to as <i>conversational agents</i> or <i>relational agents</i>; these are computer programs integrated into DMHIs that are able to hold a conversation with the human user [<span class="ref-lnk lazy-ref"><a data-rid="cit0052" data-reflink="_i29 _i30 _i31 _i32" href="#"><span class="off-screen">Citation</span>52</a></span>]. The first credited chatbot, ELIZA, was developed by Joseph Weizenbaum in 1966 [<span class="ref-lnk lazy-ref"><a data-rid="cit0053" data-reflink="_i29 _i30 _i31 _i32" href="#"><span class="off-screen">Citation</span>53</a></span>]. ELIZA was programmed to respond based on a Rogerian psychotherapeutic approach, searching user input for keywords and then applying a rule based on those keywords to provide a response. Since ELIZA, interest in chatbots has increased considerably, particularly after 2016 [<span class="ref-lnk lazy-ref"><a data-rid="cit0054" data-reflink="_i29 _i30 _i31 _i32" href="#"><span class="off-screen">Citation</span>54</a></span>] and within DMHIs. One review found that 39% of health chatbots focused on mental health issues [<span class="ref-lnk lazy-ref"><a data-rid="cit0055" data-reflink="_i29 _i30 _i31 _i32" href="#"><span class="off-screen">Citation</span>55</a></span>], and another review reported that 41 mental health chatbots were developed in 2019 alone [<span class="ref-lnk lazy-ref"><a data-rid="cit0056" data-reflink="_i29 _i30 _i31 _i32" href="#"><span class="off-screen">Citation</span>56</a></span>]. Currently, most mental health chatbots have been designed for depression or anxiety [<span class="ref-lnk lazy-ref"><a data-rid="cit0057" data-reflink="_i29 _i30 _i31 _i32" href="#"><span class="off-screen">Citation</span>57</a></span>], though chatbots have also been developed to support patients with autism [<span class="ref-lnk lazy-ref"><a data-rid="cit0058" data-reflink="_i29 _i30 _i31 _i32" href="#"><span class="off-screen">Citation</span>58</a></span>], suicide risk [<span class="ref-lnk lazy-ref"><a data-rid="cit0059" data-reflink="_i29 _i30 _i31 _i32" href="#"><span class="off-screen">Citation</span>59</a></span>], substance abuse [<span class="ref-lnk lazy-ref"><a data-rid="cit0060" data-reflink="_i29 _i30 _i31 _i32" href="#"><span class="off-screen">Citation</span>60</a></span>], post-traumatic stress disorder, stress [<span class="ref-lnk lazy-ref"><a data-rid="cit0061" data-reflink="_i29 _i30 _i31 _i32" href="#"><span class="off-screen">Citation</span>61</a></span>], dementia [<span class="ref-lnk lazy-ref"><a data-rid="cit0062" data-reflink="_i29 _i30 _i31 _i32" href="#"><span class="off-screen">Citation</span>62</a></span>], and acrophobia [<span class="ref-lnk lazy-ref"><a data-rid="cit0063" data-reflink="_i29 _i30 _i31 _i32" href="#"><span class="off-screen">Citation</span>63</a></span>], and to enhance positive psychological constructs, such as psychological well-being [<span class="ref-lnk lazy-ref"><a data-rid="cit0061" data-reflink="_i29 _i30 _i31 _i32" href="#"><span class="off-screen">Citation</span>61</a></span>], self-compassion [<span class="ref-lnk lazy-ref"><a data-rid="cit0064" data-reflink="_i29 _i30 _i31 _i32" href="#"><span class="off-screen">Citation</span>64</a></span>], mindfulness, and quality of life [<span class="ref-lnk lazy-ref"><a data-rid="cit0065" data-reflink="_i29 _i30 _i31 _i32" href="#"><span class="off-screen">Citation</span>65</a></span>]. Chatbots also have been developed for various groups including children, adolescents, adults, elders [<span class="ref-lnk lazy-ref"><a data-rid="cit0066" data-reflink="_i29 _i30 _i31 _i32" href="#"><span class="off-screen">Citation</span>66</a></span>], and specific clinical populations [<span class="ref-lnk lazy-ref"><a data-rid="cit0067" data-reflink="_i29 _i30 _i31 _i32" href="#"><span class="off-screen">Citation</span>67</a></span>]. Given this tremendous growth, we focus the remainder of our review on the potential for AI-based chatbots within DHMI rather than other applications of AI.</p>
</div>
<div id="s0004" class="NLM_sec NLM_sec_level_1">
 <h2 id="_i9" class="section-heading-2">4. Functions of chatbots in DMHIs</h2>
 <p>Chatbots can be integrated into different platforms, including mobile applications, websites, SMS texting, smart technologies, and virtual reality. Chatbots also vary in their complexity of interaction; they can rely on systems ranging from straightforward rule-based models, like ELIZA, to more advanced AI models using natural language processing (NLP) and machine learning [<span class="ref-lnk lazy-ref"><a data-rid="cit0048" data-reflink="_i29 _i30 _i31 _i32" href="#"><span class="off-screen">Citation</span>48</a></span>]. Typically, after analyzing user dialogue content, chatbots respond through text-based or voice-enabled conversations [<span class="ref-lnk lazy-ref"><a data-rid="cit0068" data-reflink="_i29 _i30 _i31 _i32" href="#"><span class="off-screen">Citation</span>68</a></span>]. User input is primarily written, via open text or multiple-choice options, while output generated by the chatbot can be written, spoken, or visual [<span class="ref-lnk lazy-ref"><a data-rid="cit0069" data-reflink="_i29 _i30 _i31 _i32" href="#"><span class="off-screen">Citation</span>69</a></span>].</p>
 <p>Chatbots have been incorporated into DMHIss to perform various functions, ranging from assistance, screening, psychoeducation, therapeutic intervention, monitoring behavior changes, and relapse prevention [<span class="ref-lnk lazy-ref"><a data-rid="cit0070" data-reflink="_i29 _i30 _i31 _i32" href="#"><span class="off-screen">Citation</span>70</a></span>]. We briefly review some of the most common functions: diagnosis, content delivery, and symptom management.</p>
 <div id="s0004-s2001" class="NLM_sec NLM_sec_level_2">
  <h3 class="section-heading-3" id="_i10">4.1. Diagnosis</h3>
  <p>One way chatbots can help reduce the burden on health care professionals is to diagnose and triage people with mental health concerns, which may help to prioritize in-person services for those who need it most. Chatbots have been used as a diagnostic or screening tool for dementia, substance abuse, stress, depression and suicide, anxiety disorders, and PTSD. In this context, people interact with the chatbot as they would with a human being. Through a series of queries, the chatbot identifies the user’s symptoms, predicts the disease, and recommends treatment or provides information about the diagnosis to the patient [<span class="ref-lnk lazy-ref"><a data-rid="cit0062" data-reflink="_i29 _i30 _i31 _i32" href="#"><span class="off-screen">Citation</span>62</a></span>,<span class="ref-lnk lazy-ref"><a data-rid="cit0071 cit0072 cit0073 cit0074 cit0075 cit0076 cit0077 cit0078 cit0079" data-reflink="_i29 _i30 _i31 _i32" href="#"><span class="off-screen">Citation</span>71–79</a></span>]. This application of chatbots is controversial; in one survey of mental health professionals, 51% felt using chatbots for diagnostic purposes was problematic [<span class="ref-lnk lazy-ref"><a data-rid="cit0080" data-reflink="_i29 _i30 _i31 _i32" href="#"><span class="off-screen">Citation</span>80</a></span>]. However, using AI for diagnostic purposes can help identify people who are at risk, allowing for earlier intervention and helping to reduce the likelihood of future problems [<span class="ref-lnk lazy-ref"><a data-rid="cit0081" data-reflink="_i29 _i30 _i31 _i32" href="#"><span class="off-screen">Citation</span>81</a></span>].</p>
  <p>Research on the effectiveness of chatbots in diagnostic functions remains limited, but preliminary evidence on one diagnostic chatbot, <i>Ada</i>, suggests moderate agreement between the chatbot’s diagnosis and conditions depicted in a vignette. Importantly, however, agreement was higher when psychotherapists entered symptoms into the app based on the vignette, but rather low when entered by laypersons [<span class="ref-lnk lazy-ref"><a data-rid="cit0082" data-reflink="_i29 _i30 _i31 _i32" href="#"><span class="off-screen">Citation</span>82</a></span>]. Even in cases where chatbots do not perform diagnoses themselves, they may help improve engagement with mental health assessments, increasing the likelihood of identifying people in need of care [<span class="ref-lnk lazy-ref"><a data-rid="cit0083" data-reflink="_i29 _i30 _i31 _i32" href="#"><span class="off-screen">Citation</span>83</a></span>]. In addition, other types of AI may effectively predict the onset of certain mental health conditions by utilizing behavioral and self-report data. For example, machine learning algorithms can predict psychosis onset with 79% accuracy, and Attention-Deficit Hyperactivity Disorder and Autism Spectrum Disorder with 96% accuracy [<span class="ref-lnk lazy-ref"><a data-rid="cit0084" data-reflink="_i29 _i30 _i31 _i32" href="#"><span class="off-screen">Citation</span>84</a></span>].</p>
 </div>
 <div id="s0004-s2002" class="NLM_sec NLM_sec_level_2">
  <h3 class="section-heading-3" id="_i11">4.2. Content delivery</h3>
  <p>The most common application of chatbots within DMHIs is to deliver content. While chatbots cannot simulate traditional psychotherapy, they may be able to administer psychotherapeutic interventions that do not involve a high degree of therapeutic competence [<span class="ref-lnk lazy-ref"><a data-rid="cit0085" data-reflink="_i29 _i30 _i31 _i32" href="#"><span class="off-screen">Citation</span>85</a></span>]. For example, some chatbots using NLP can simulate a therapeutic conversational style that implements and teaches users about various therapeutic techniques (see Fitzpatrick et al., 2017, for a discussion) [<span class="ref-lnk lazy-ref"><a data-rid="cit0051" data-reflink="_i29 _i30 _i31 _i32" href="#"><span class="off-screen">Citation</span>51</a></span>]. Chatbots employing principles of cognitive behavioral therapy (CBT) are the most common and well-studied; one meta-analysis found that 10 out of 17 chatbots primarily used CBT [<span class="ref-lnk lazy-ref"><a data-rid="cit0086" data-reflink="_i29 _i30 _i31 _i32" href="#"><span class="off-screen">Citation</span>86</a></span>]. One such chatbot, <i>Woebot</i> [<span class="ref-lnk lazy-ref"><a data-rid="cit0087" data-reflink="_i29 _i30 _i31 _i32" href="#"><span class="off-screen">Citation</span>87</a></span>], delivers CBT to users via instant messaging employing NLP and mimicking human clinicians and social discourse [<span class="ref-lnk lazy-ref"><a data-rid="cit0085" data-reflink="_i29 _i30 _i31 _i32" href="#"><span class="off-screen">Citation</span>85</a></span>].</p>
  <p>However, other chatbots utilize a variety of therapeutic approaches, such as acceptance and commitment therapy and mindfulness [<span class="ref-lnk lazy-ref"><a data-rid="cit0087" data-reflink="_i29 _i30 _i31 _i32" href="#"><span class="off-screen">Citation</span>87</a></span>,<span class="ref-lnk lazy-ref"><a data-rid="cit0088" data-reflink="_i29 _i30 _i31 _i32" href="#"><span class="off-screen">Citation</span>88</a></span>]. <i>Wysa</i>, a chatbot described as ‘emotionally intelligent,’ draws on a variety of therapeutic approaches including CBT, dialectical behavior therapy, and motivational interviewing [<span class="ref-lnk lazy-ref"><a data-rid="cit0089" data-reflink="_i29 _i30 _i31 _i32" href="#"><span class="off-screen">Citation</span>89</a></span>,<span class="ref-lnk lazy-ref"><a data-rid="cit0090" data-reflink="_i29 _i30 _i31 _i32" href="#"><span class="off-screen">Citation</span>90</a></span>]. Some chatbots have been developed for more specific applications, like <i>Vivibot</i>, which helps young people learn positive psychology skills after cancer treatment to support anxiety reduction [<span class="ref-lnk lazy-ref"><a data-rid="cit0067" data-reflink="_i29 _i30 _i31 _i32" href="#"><span class="off-screen">Citation</span>67</a></span>]; whereas others, like <i>MYLO</i>, employ general self-help strategies when users are in distress and work towards suicide prevention [<span class="ref-lnk lazy-ref"><a data-rid="cit0091" data-reflink="_i29 _i30 _i31 _i32" href="#"><span class="off-screen">Citation</span>91</a></span>] (these are less common as they involve greater risk [<span class="ref-lnk lazy-ref"><a data-rid="cit0052" data-reflink="_i29 _i30 _i31 _i32" href="#"><span class="off-screen">Citation</span>52</a></span>]).</p>
 </div>
 <div id="s0004-s2003" class="NLM_sec NLM_sec_level_2">
  <h3 class="section-heading-3" id="_i12">4.3. Management and screening of symptoms</h3>
  <p>Chatbots may also help monitor a patient’s progress or track symptoms and behaviors (e.g. physical activity, hours of sleep, time spent on social media) [<span class="ref-lnk lazy-ref"><a data-rid="cit0092" data-reflink="_i29 _i30 _i31 _i32" href="#"><span class="off-screen">Citation</span>92</a></span>]. Chatbots are currently being used as personal health assistants to promote well-being and mental health check-ins throughout and after completing an intervention [<span class="ref-lnk lazy-ref"><a data-rid="cit0093" data-reflink="_i29 _i30 _i31 _i32" href="#"><span class="off-screen">Citation</span>93</a></span>]. In this capacity, chatbots can help users facilitate the transfer of therapeutic content into their daily lives, assess progress, and provide personalized support by delivering additional mental health resources [<span class="ref-lnk lazy-ref"><a data-rid="cit0052" data-reflink="_i29 _i30 _i31 _i32" href="#"><span class="off-screen">Citation</span>52</a></span>]. AI can also help improve the personalization of care by facilitating more efficient storage and processing of user information [<span class="ref-lnk lazy-ref"><a data-rid="cit0084" data-reflink="_i29 _i30 _i31 _i32" href="#"><span class="off-screen">Citation</span>84</a></span>], which can subsequently allow users to better understand when symptoms flare up or decline. In turn, this can help improve self-management of symptoms and risk of relapse [<span class="ref-lnk lazy-ref"><a data-rid="cit0052" data-reflink="_i29 _i30 _i31 _i32" href="#"><span class="off-screen">Citation</span>52</a></span>], particularly among people without access to a mental health professional [<span class="ref-lnk lazy-ref"><a data-rid="cit0094 cit0095 cit0096" data-reflink="_i29 _i30 _i31 _i32" href="#"><span class="off-screen">Citation</span>94–96</a></span>]. This type of management and screening can also be used after traditional in-person interventions or in outpatient settings. In this case, chatbots could help maintain benefits from treatment by reminding clients of skills and practices (e.g. medication adherence, check-ups, exercise, etc.) [<span class="ref-lnk lazy-ref"><a data-rid="cit0093" data-reflink="_i29 _i30 _i31 _i32" href="#"><span class="off-screen">Citation</span>93</a></span>,<span class="ref-lnk lazy-ref"><a data-rid="cit0097" data-reflink="_i29 _i30 _i31 _i32" href="#"><span class="off-screen">Citation</span>97</a></span>].</p>
 </div>
</div>
<div id="s0005" class="NLM_sec NLM_sec_level_1">
 <h2 id="_i13" class="section-heading-2">5. The evidence for chatbots in DHMI</h2>
 <p>Although chatbots are becoming increasingly popular in DHMI, few chatbot-based DMHIs describe the evidence supporting their programs, and even fewer in-market chatbots have been tested in rigorous, empirical research [<span class="ref-lnk lazy-ref"><a data-rid="cit0057" data-reflink="_i29 _i30 _i31 _i32" href="#"><span class="off-screen">Citation</span>57</a></span>]. What research exists, albeit limited, does suggest mental health chatbots can be effective, acceptable to users, and promote engagement.</p>
 <div id="s0005-s2001" class="NLM_sec NLM_sec_level_2">
  <h3 class="section-heading-3" id="_i14">5.1. Acceptability of chatbots</h3>
  <p>Installations of publicly available mental health apps with integrated chatbots are high, suggesting people are interested in these programs [<span class="ref-lnk lazy-ref"><a data-rid="cit0057" data-reflink="_i29 _i30 _i31 _i32" href="#"><span class="off-screen">Citation</span>57</a></span>]. Users of mental health chatbots also generally report high satisfaction with chatbot interactions, positive perceptions of chatbots, prefer chatbots to information control groups, and indicate interest in using chatbots in the future [<span class="ref-lnk lazy-ref"><a data-rid="cit0057" data-reflink="_i29 _i30 _i31 _i32" href="#"><span class="off-screen">Citation</span>57</a></span>,<span class="ref-lnk lazy-ref"><a data-rid="cit0098" data-reflink="_i29 _i30 _i31 _i32" href="#"><span class="off-screen">Citation</span>98</a></span>]. In one study, 68% of users found <i>Wysa</i> encouraging and helpful [<span class="ref-lnk lazy-ref"><a data-rid="cit0090" data-reflink="_i29 _i30 _i31 _i32" href="#"><span class="off-screen">Citation</span>90</a></span>]. In particular, people are more satisfied when they perceive conversations as private, reporting learning something new during the interaction when chatbot content is similar to what their therapist recommended previously and perceived to be of high quality, when there is appropriate usage of high-quality technological elements, and when the chatbot’s tone or voice is consistent [<span class="ref-lnk lazy-ref"><a data-rid="cit0057" data-reflink="_i29 _i30 _i31 _i32" href="#"><span class="off-screen">Citation</span>57</a></span>]. Moreover, compared to human beings, chatbots are perceived as less judgmental, which facilitates self-disclosure among users, and allows for more conversational flexibility [<span class="ref-lnk lazy-ref"><a data-rid="cit0067" data-reflink="_i29 _i30 _i31 _i32" href="#"><span class="off-screen">Citation</span>67</a></span>,<span class="ref-lnk lazy-ref"><a data-rid="cit0084" data-reflink="_i29 _i30 _i31 _i32" href="#"><span class="off-screen">Citation</span>84</a></span>,<span class="ref-lnk lazy-ref"><a data-rid="cit0099" data-reflink="_i29 _i30 _i31 _i32" href="#"><span class="off-screen">Citation</span>99</a></span>]. In fact, some people prefer to interact with chatbots over mental health professionals [<span class="ref-lnk lazy-ref"><a data-rid="cit0074" data-reflink="_i29 _i30 _i31 _i32" href="#"><span class="off-screen">Citation</span>74</a></span>], which may encourage people who would not normally seek therapy to receive care.</p>
  <p>However, there is variability in how people perceive chatbots. In one study, 32% of participants reported the chatbot was unhelpful [<span class="ref-lnk lazy-ref"><a data-rid="cit0090" data-reflink="_i29 _i30 _i31 _i32" href="#"><span class="off-screen">Citation</span>90</a></span>]. In some cases, users have even reported that their interactions with the chatbot bothered them [<span class="ref-lnk lazy-ref"><a data-rid="cit0090" data-reflink="_i29 _i30 _i31 _i32" href="#"><span class="off-screen">Citation</span>90</a></span>], or that the chatbot was self-focused [<span class="ref-lnk lazy-ref"><a data-rid="cit0090" data-reflink="_i29 _i30 _i31 _i32" href="#"><span class="off-screen">Citation</span>90</a></span>] or annoying [<span class="ref-lnk lazy-ref"><a data-rid="cit0067" data-reflink="_i29 _i30 _i31 _i32" href="#"><span class="off-screen">Citation</span>67</a></span>]. Indeed, concerns regarding the accuracy, trustworthiness, and privacy of chatbots have all emerged as potential barriers to engagement and adoption [<span class="ref-lnk lazy-ref"><a data-rid="cit0100" data-reflink="_i29 _i30 _i31 _i32" href="#"><span class="off-screen">Citation</span>100</a></span>]. Some of this variability in perceptions and satisfaction may be related to the chatbot’s personality, emotional responsiveness, and empathy [<span class="ref-lnk lazy-ref"><a data-rid="cit0101" data-reflink="_i29 _i30 _i31 _i32" href="#"><span class="off-screen">Citation</span>101</a></span>,<span class="ref-lnk lazy-ref"><a data-rid="cit0102" data-reflink="_i29 _i30 _i31 _i32" href="#"><span class="off-screen">Citation</span>102</a></span>], which we will address in more detail later.</p>
 </div>
 <div id="s0005-s2002" class="NLM_sec NLM_sec_level_2">
  <h3 class="section-heading-3" id="_i15">5.2. Effects on user engagement</h3>
  <p>Given that the lack of accountability in unsupported DMHIs is often cited as a drawback [<span class="ref-lnk lazy-ref"><a data-rid="cit0039" data-reflink="_i29 _i30 _i31 _i32" href="#"><span class="off-screen">Citation</span>39</a></span>], one important benefit to chatbots is increased accountability, which leads to increased user engagement [<span class="ref-lnk lazy-ref"><a data-rid="cit0103" data-reflink="_i29 _i30 _i31 _i32" href="#"><span class="off-screen">Citation</span>103</a></span>,<span class="ref-lnk lazy-ref"><a data-rid="cit0104" data-reflink="_i29 _i30 _i31 _i32" href="#"><span class="off-screen">Citation</span>104</a></span>] and behavioral intentions to use the program [<span class="ref-lnk lazy-ref"><a data-rid="cit0105" data-reflink="_i29 _i30 _i31 _i32" href="#"><span class="off-screen">Citation</span>105</a></span>]. For example, college students who completed web-based depression interventions with guidance from chatbots reported liking the accountability from the daily check-ins [<span class="ref-lnk lazy-ref"><a data-rid="cit0085" data-reflink="_i29 _i30 _i31 _i32" href="#"><span class="off-screen">Citation</span>85</a></span>,<span class="ref-lnk lazy-ref"><a data-rid="cit0098" data-reflink="_i29 _i30 _i31 _i32" href="#"><span class="off-screen">Citation</span>98</a></span>]. In another study, participants using a mobile intervention perceived the guidance and direction offered by the chatbot in the program as beneficial [<span class="ref-lnk lazy-ref"><a data-rid="cit0061" data-reflink="_i29 _i30 _i31 _i32" href="#"><span class="off-screen">Citation</span>61</a></span>]. Thus, in addition to prompting users to engage with the intervention regularly, chatbots also help users engage with the material more deeply, which may be more important in reducing depressive symptoms than simply increasing the number of logins or activities completed [<span class="ref-lnk lazy-ref"><a data-rid="cit0035" data-reflink="_i29 _i30 _i31 _i32" href="#"><span class="off-screen">Citation</span>35</a></span>].</p>
  <p>However, while researchers have argued that chatbots help to increase user engagement and reduce dropout rates within DMHIs, relatively few studies have directly explored the impact of chatbots on attrition or user engagement. One study of a digital smoking cessation program found that the addition of a chatbot improved user engagement by 107% compared to their traditional program [<span class="ref-lnk lazy-ref"><a data-rid="cit0104" data-reflink="_i29 _i30 _i31 _i32" href="#"><span class="off-screen">Citation</span>104</a></span>], but most other studies of chatbots – within DMHIs or otherwise – have tested the effects of a chatbot on engagement without a comparison to a non-chatbot-enabled DMHIs. And, to our knowledge, no studies have explored the long-term impact of chatbots on engagement or dropout rates.</p>
 </div>
 <div id="s0005-s2003" class="NLM_sec NLM_sec_level_2">
  <h3 class="section-heading-3" id="_i16">5.3. Effects on mental health outcomes</h3>
  <p>Research on chatbots in DMHIs is in its infancy, and consequently, most studies have focused on the acceptability and usability of chatbots. Fewer studies have conducted rigorous tests of whether chatbots lead to improvements in mental health outcomes, and most research conducted in this domain consists of single-arm studies with no control groups, or studies with control groups that may not be adequate placebo or attention-controls. This is particularly important given that some researchers have argued that individual features of DMHIs, such as the availability of chatbots, may be associated with a greater digital placebo effect [<span class="ref-lnk lazy-ref"><a data-rid="cit0106" data-reflink="_i29 _i30 _i31 _i32" href="#"><span class="off-screen">Citation</span>106</a></span>]. For example, although one meta-analysis found chatbots were effective for improving depression, distress, stress, and acrophobia, their evidence was considered weak due to the lack of studies, conflicting results across studies, and high estimated risk of bias in the included studies [<span class="ref-lnk lazy-ref"><a data-rid="cit0107" data-reflink="_i29 _i30 _i31 _i32" href="#"><span class="off-screen">Citation</span>107</a></span>]. Thus, the evidence for chatbots in this domain should be considered preliminary.</p>
  <p>In one study, researchers found that <i>Woebot</i> users had significantly greater improvements in depressive symptoms compared to participants in the control group, who read an e-book with psychoeducational content [<span class="ref-lnk lazy-ref"><a data-rid="cit0085" data-reflink="_i29 _i30 _i31 _i32" href="#"><span class="off-screen">Citation</span>85</a></span>]. Other research has shown improvements in psychological well-being and perceived stress after two weeks, relative to a waitlist control [<span class="ref-lnk lazy-ref"><a data-rid="cit0061" data-reflink="_i29 _i30 _i31 _i32" href="#"><span class="off-screen">Citation</span>61</a></span>]. Preliminary results also suggest significant improvements in substance use among <i>Woebot</i> users, though this study did not include a control group [<span class="ref-lnk lazy-ref"><a data-rid="cit0060" data-reflink="_i29 _i30 _i31 _i32" href="#"><span class="off-screen">Citation</span>60</a></span>].</p>
  <p>In regards to anxiety, results are more conflicting [<span class="ref-lnk lazy-ref"><a data-rid="cit0107" data-reflink="_i29 _i30 _i31 _i32" href="#"><span class="off-screen">Citation</span>107</a></span>]. One study of first-year college students engaging with a healthy coping intervention delivered by a chatbot named <i>Atena</i> found significant improvements in anxiety, but only among participants with extreme anxiety scores at baseline [<span class="ref-lnk lazy-ref"><a data-rid="cit0108" data-reflink="_i29 _i30 _i31 _i32" href="#"><span class="off-screen">Citation</span>108</a></span>]. This study also lacked a control group, and so these improvements may be a result of regression to the mean rather than engaging with <i>Atena</i>. Another study found that college students who engaged with their chatbot <i>Tess</i> reported significant improvements in anxiety symptoms, but this improvement was not significantly greater than that reported by participants in the control group, who read a psychoeducational book on depression [<span class="ref-lnk lazy-ref"><a data-rid="cit0109" data-reflink="_i29 _i30 _i31 _i32" href="#"><span class="off-screen">Citation</span>109</a></span>].</p>
  <p>Improvements in mental health outcomes may also depend on engagement. For instance, one study showed that users who had high engagement levels had significantly greater improvement in depressive symptoms relative to those with lower levels of engagement [<span class="ref-lnk lazy-ref"><a data-rid="cit0090" data-reflink="_i29 _i30 _i31 _i32" href="#"><span class="off-screen">Citation</span>90</a></span>]. Research on the chatbot designed for young people treated for cancer, <i>Vivibot</i>, also found trends suggesting that greater usage led to greater improvements in anxiety [<span class="ref-lnk lazy-ref"><a data-rid="cit0067" data-reflink="_i29 _i30 _i31 _i32" href="#"><span class="off-screen">Citation</span>67</a></span>]. Thus, much like non-AI-supported DMHIs, the effectiveness of mental health chatbots depends on the user’s level of engagement, but when used as recommended, they appear to be effective.</p>
 </div>
 <div id="s0005-s2004" class="NLM_sec NLM_sec_level_2">
  <h3 class="section-heading-3" id="_i17">5.4. Weaknesses of AI chatbots</h3>
  <p>Not to be mistaken for a utopian solution to existing problems with DMHIs, research points to several weaknesses in mental health chatbots. For instance, given the complexity of human language, chatbots are vulnerable to misinterpreting meaning in user responses [<span class="ref-lnk lazy-ref"><a data-rid="cit0110" data-reflink="_i29 _i30 _i31 _i32" href="#"><span class="off-screen">Citation</span>110</a></span>]. Chatbots are not yet proficient in interpreting ellipses, metaphors, colloquialisms, and hyperbole, and these challenges will be even greater when chatbots are programmed in various languages [<span class="ref-lnk lazy-ref"><a data-rid="cit0111" data-reflink="_i29 _i30 _i31 _i32" href="#"><span class="off-screen">Citation</span>111</a></span>]. Across several studies, a common complaint from users is that interactions with chatbots become repetitive [<span class="ref-lnk lazy-ref"><a data-rid="cit0061" data-reflink="_i29 _i30 _i31 _i32" href="#"><span class="off-screen">Citation</span>61</a></span>,<span class="ref-lnk lazy-ref"><a data-rid="cit0085" data-reflink="_i29 _i30 _i31 _i32" href="#"><span class="off-screen">Citation</span>85</a></span>,<span class="ref-lnk lazy-ref"><a data-rid="cit0098" data-reflink="_i29 _i30 _i31 _i32" href="#"><span class="off-screen">Citation</span>98</a></span>,<span class="ref-lnk lazy-ref"><a data-rid="cit0102" data-reflink="_i29 _i30 _i31 _i32" href="#"><span class="off-screen">Citation</span>102</a></span>], which makes the chatbot feel less human-like [<span class="ref-lnk lazy-ref"><a data-rid="cit0061" data-reflink="_i29 _i30 _i31 _i32" href="#"><span class="off-screen">Citation</span>61</a></span>] and reduces users’ motivation to continue the program [<span class="ref-lnk lazy-ref"><a data-rid="cit0110" data-reflink="_i29 _i30 _i31 _i32" href="#"><span class="off-screen">Citation</span>110</a></span>]. Users also complain about misunderstandings with chatbots [<span class="ref-lnk lazy-ref"><a data-rid="cit0061" data-reflink="_i29 _i30 _i31 _i32" href="#"><span class="off-screen">Citation</span>61</a></span>,<span class="ref-lnk lazy-ref"><a data-rid="cit0085" data-reflink="_i29 _i30 _i31 _i32" href="#"><span class="off-screen">Citation</span>85</a></span>,<span class="ref-lnk lazy-ref"><a data-rid="cit0090" data-reflink="_i29 _i30 _i31 _i32" href="#"><span class="off-screen">Citation</span>90</a></span>], particularly with long or complex messages that may not be understood by chatbots, leading to irrelevant or inappropriate responses which subsequently undermine the therapeutic alliance [<span class="ref-lnk lazy-ref"><a data-rid="cit0100" data-reflink="_i29 _i30 _i31 _i32" href="#"><span class="off-screen">Citation</span>100</a></span>]. Thus, while chatbots can be beneficial, there are several ways in which they can deter users from engaging with the intervention. The risks associated with misunderstandings are even greater when chatbots or other types of AI are being used for diagnostic purposes.</p>
 </div>
</div>
<div id="s0006" class="NLM_sec NLM_sec_level_1">
 <h2 id="_i18" class="section-heading-2">6. A case study: happify health’s AI chatbot Anna<sup>TM</sup></h2>
 <p>In 2019, Happify Health developed a chatbot to integrate into its commercial digital mental health platform. <i>Anna</i> is an AI-based chatbot that models the role of a therapist and delivers some Happify activities. Whereas some other DMHIs chatbots exist as a standalone app (i.e. the chatbot is the intervention), Anna was designed to complement other features of the Happify program. Specifically, Happify was developed to deliver gamified versions of evidence-based activities drawn from various therapeutic approaches, including CBT, mindfulness-based stress reduction, and positive psychology. These activities are organized into tracks designed to help users focus on a particular area of concern, like reducing stress (see Carpenter et al., 2016, for a discussion) [<span class="ref-lnk lazy-ref"><a data-rid="cit0112" data-reflink="_i29 _i30 _i31 _i32" href="#"><span class="off-screen">Citation</span>112</a></span>].</p>
 <p><i>Anna</i> is incorporated into these tracks. Upon selecting an <i>Anna</i>-enabled track, <i>Anna</i> begins by greeting users with an introduction and explanation of the chatbot’s role within the track (see <a href="#f0001">Figure 1</a>). In addition, <i>Anna</i> may ask users specific questions to gather information that can be used to better personalize the track. <i>Anna</i> also delivers some activities within these tracks, deepening engagement with those activities and increasing the likelihood that users complete activities as intended (see <a href="#f0002">Figure 2</a>). Thus, <i>Anna</i> helps to maximize the benefits associated with that activity. To do this, <i>Anna</i> tracks criteria essential to optimal outcomes for each activity, reviews responses based on these criteria, and then prompts users to provide any expected information that was lacking from their initial response. For example, in a gratitude activity, <i>Anna</i> will coach users to express both positive emotion and meaning if these are not expressed in their initial response. While <i>Anna</i> coaches users to engage more efficiently with the platform, <i>Anna</i> also listens to the user; consequently, the course of the conversation may be steered by users and by the chatbot, addressing potential concerns that conversations with chatbots are not interactive enough [<span class="ref-lnk lazy-ref"><a data-rid="cit0098" data-reflink="_i29 _i30 _i31 _i32" href="#"><span class="off-screen">Citation</span>98</a></span>].</p>
 <div class="figure figureViewer" id="f0001">
  <div class="hidden figureViewerArticleInfo">
   <span class="figViewerTitle">Artificially intelligent chatbots in digital mental health interventions: a review</span>
   <div class="articleAuthors articleInfoSection">
    <div class="authorsHeading">
     All authors
    </div>
    <div class="authors">
     <a class="entryAuthor" href="/action/doSearch?Contrib=Boucher%2C+Eliane+M"><span class="hlFld-ContribAuthor"></span></a><a href="/author/Boucher%2C+Eliane+M"><span class="NLM_given-names">Eliane M.</span> Boucher</a> <a href="https://orcid.org/0000-0002-1384-7177"><img src="/templates/jsp/images/orcid.png"></a>, <a class="entryAuthor" href="/action/doSearch?Contrib=Harake%2C+Nicole+R"><span class="hlFld-ContribAuthor"></span></a><a href="/author/Harake%2C+Nicole+R"><span class="NLM_given-names">Nicole R.</span> Harake</a> <a href="https://orcid.org/0000-0003-3750-8185"><img src="/templates/jsp/images/orcid.png"></a>, <a class="entryAuthor" href="/action/doSearch?Contrib=Ward%2C+Haley+E"><span class="hlFld-ContribAuthor"></span></a><a href="/author/Ward%2C+Haley+E"><span class="NLM_given-names">Haley E.</span> Ward</a> <a href="https://orcid.org/0000-0001-5423-8928"><img src="/templates/jsp/images/orcid.png"></a>, <a class="entryAuthor" href="/action/doSearch?Contrib=Stoeckl%2C+Sarah+Elizabeth"><span class="hlFld-ContribAuthor"></span></a><a href="/author/Stoeckl%2C+Sarah+Elizabeth"><span class="NLM_given-names">Sarah Elizabeth</span> Stoeckl</a> <a href="https://orcid.org/0000-0003-2719-2957"><img src="/templates/jsp/images/orcid.png"></a>, <a class="entryAuthor" href="/action/doSearch?Contrib=Vargas%2C+Junielly"><span class="hlFld-ContribAuthor"></span></a><a href="/author/Vargas%2C+Junielly"><span class="NLM_given-names">Junielly</span> Vargas</a> <a href="https://orcid.org/0000-0002-1806-7428"><img src="/templates/jsp/images/orcid.png"></a>, <a class="entryAuthor" href="/action/doSearch?Contrib=Minkel%2C+Jared"><span class="hlFld-ContribAuthor"></span></a><a href="/author/Minkel%2C+Jared"><span class="NLM_given-names">Jared</span> Minkel</a> <a href="https://orcid.org/0000-0001-7979-3098"><img src="/templates/jsp/images/orcid.png"></a>, <a class="entryAuthor" href="/action/doSearch?Contrib=Parks%2C+Acacia+C"><span class="hlFld-ContribAuthor"></span></a><a href="/author/Parks%2C+Acacia+C"><span class="NLM_given-names">Acacia C.</span> Parks</a> <a href="https://orcid.org/0000-0001-6643-0116"><img src="/templates/jsp/images/orcid.png"></a> &amp; <a class="entryAuthor" href="/action/doSearch?Contrib=Zilca%2C+Ran"><span class="hlFld-ContribAuthor"></span></a><a href="/author/Zilca%2C+Ran"><span class="NLM_given-names">Ran</span> Zilca</a> <a href="https://orcid.org/0000-0001-8521-5438"><img src="/templates/jsp/images/orcid.png"></a>
    </div>
   </div>
   <div class="articleLowerInfo articleInfoSection">
    <div class="articleLowerInfoSection articleInfoDOI">
     <a href="https://doi.org/10.1080/17434440.2021.2013200">https://doi.org/10.1080/17434440.2021.2013200</a>
    </div>
    <div class="articleInfoPublicationDate articleLowerInfoSection border">
     <h6>Published online:</h6>31 December 2021
    </div>
   </div>
  </div>
  <div class="figureThumbnailContainer">
   <div class="figureInfo">
    <div class="short-legend">
     <p class="captionText"><span class="captionLabel">Figure 1. </span> Screenshot of happify health’s AI-based chatbot Anna’s introductory dialog.</p>
    </div>
   </div><a href="#" class="thumbnail" aria-label="thumbnail image"><img id="f0001image" src="/na101/home/literatum/publisher/tandf/journals/content/ierd20/2021/ierd20.v018.sup01/17434440.2021.2013200/20220121/images/medium/ierd_a_2013200_f0001_oc.jpg" loading="lazy" height="500" width="195"></a>
   <div class="figureDownloadOptions">
    <a href="#" class="downloadBtn btn btn-sm" role="button">Display full size</a>
   </div>
  </div>
 </div>
 <div class="hidden rs_skip" id="fig-description-f0001">
  <p class="captionText"><span class="captionLabel">Figure 1. </span> Screenshot of happify health’s AI-based chatbot Anna’s introductory dialog.</p>
 </div>
 <div class="hidden rs_skip" id="figureFootNote-f0001">
  <div class="figureFootNote-f0001"></div>
 </div>
 <div class="figure figureViewer" id="f0002">
  <div class="hidden figureViewerArticleInfo">
   <span class="figViewerTitle">Artificially intelligent chatbots in digital mental health interventions: a review</span>
   <div class="articleAuthors articleInfoSection">
    <div class="authorsHeading">
     All authors
    </div>
    <div class="authors">
     <a class="entryAuthor" href="/action/doSearch?Contrib=Boucher%2C+Eliane+M"><span class="hlFld-ContribAuthor"></span></a><a href="/author/Boucher%2C+Eliane+M"><span class="NLM_given-names">Eliane M.</span> Boucher</a> <a href="https://orcid.org/0000-0002-1384-7177"><img src="/templates/jsp/images/orcid.png"></a>, <a class="entryAuthor" href="/action/doSearch?Contrib=Harake%2C+Nicole+R"><span class="hlFld-ContribAuthor"></span></a><a href="/author/Harake%2C+Nicole+R"><span class="NLM_given-names">Nicole R.</span> Harake</a> <a href="https://orcid.org/0000-0003-3750-8185"><img src="/templates/jsp/images/orcid.png"></a>, <a class="entryAuthor" href="/action/doSearch?Contrib=Ward%2C+Haley+E"><span class="hlFld-ContribAuthor"></span></a><a href="/author/Ward%2C+Haley+E"><span class="NLM_given-names">Haley E.</span> Ward</a> <a href="https://orcid.org/0000-0001-5423-8928"><img src="/templates/jsp/images/orcid.png"></a>, <a class="entryAuthor" href="/action/doSearch?Contrib=Stoeckl%2C+Sarah+Elizabeth"><span class="hlFld-ContribAuthor"></span></a><a href="/author/Stoeckl%2C+Sarah+Elizabeth"><span class="NLM_given-names">Sarah Elizabeth</span> Stoeckl</a> <a href="https://orcid.org/0000-0003-2719-2957"><img src="/templates/jsp/images/orcid.png"></a>, <a class="entryAuthor" href="/action/doSearch?Contrib=Vargas%2C+Junielly"><span class="hlFld-ContribAuthor"></span></a><a href="/author/Vargas%2C+Junielly"><span class="NLM_given-names">Junielly</span> Vargas</a> <a href="https://orcid.org/0000-0002-1806-7428"><img src="/templates/jsp/images/orcid.png"></a>, <a class="entryAuthor" href="/action/doSearch?Contrib=Minkel%2C+Jared"><span class="hlFld-ContribAuthor"></span></a><a href="/author/Minkel%2C+Jared"><span class="NLM_given-names">Jared</span> Minkel</a> <a href="https://orcid.org/0000-0001-7979-3098"><img src="/templates/jsp/images/orcid.png"></a>, <a class="entryAuthor" href="/action/doSearch?Contrib=Parks%2C+Acacia+C"><span class="hlFld-ContribAuthor"></span></a><a href="/author/Parks%2C+Acacia+C"><span class="NLM_given-names">Acacia C.</span> Parks</a> <a href="https://orcid.org/0000-0001-6643-0116"><img src="/templates/jsp/images/orcid.png"></a> &amp; <a class="entryAuthor" href="/action/doSearch?Contrib=Zilca%2C+Ran"><span class="hlFld-ContribAuthor"></span></a><a href="/author/Zilca%2C+Ran"><span class="NLM_given-names">Ran</span> Zilca</a> <a href="https://orcid.org/0000-0001-8521-5438"><img src="/templates/jsp/images/orcid.png"></a>
    </div>
   </div>
   <div class="articleLowerInfo articleInfoSection">
    <div class="articleLowerInfoSection articleInfoDOI">
     <a href="https://doi.org/10.1080/17434440.2021.2013200">https://doi.org/10.1080/17434440.2021.2013200</a>
    </div>
    <div class="articleInfoPublicationDate articleLowerInfoSection border">
     <h6>Published online:</h6>31 December 2021
    </div>
   </div>
  </div>
  <div class="figureThumbnailContainer">
   <div class="figureInfo">
    <div class="short-legend">
     <p class="captionText"><span class="captionLabel">Figure 2. </span> Screenshot of happify activity delivered by Anna.</p>
    </div>
   </div><a href="#" class="thumbnail" aria-label="thumbnail image"><img id="f0002image" src="/na101/home/literatum/publisher/tandf/journals/content/ierd20/2021/ierd20.v018.sup01/17434440.2021.2013200/20220121/images/medium/ierd_a_2013200_f0002_oc.jpg" loading="lazy" height="500" width="178"></a>
   <div class="figureDownloadOptions">
    <a href="#" class="downloadBtn btn btn-sm" role="button">Display full size</a>
   </div>
  </div>
 </div>
 <div class="hidden rs_skip" id="fig-description-f0002">
  <p class="captionText"><span class="captionLabel">Figure 2. </span> Screenshot of happify activity delivered by Anna.</p>
 </div>
 <div class="hidden rs_skip" id="figureFootNote-f0002">
  <div class="figureFootNote-f0002"></div>
 </div>
 <p></p>
 <p>Dialog content was generated by a team of clinicians and writers to ensure Anna’s interaction with users effectively models therapists. Addressing previous concerns with preset responses and repetitiveness in chatbots within DMHIs [<span class="ref-lnk lazy-ref"><a data-rid="cit0100" data-reflink="_i29 _i30 _i31 _i32" href="#"><span class="off-screen">Citation</span>100</a></span>], <i>Anna</i> employs a mix of instruction and feedback, and includes both open-ended and multiple-choice follow-up questions, using NLP models to interpret free responses. Anna’s NLP models are trained by Happify Health data scientists, and labeled by a group of clinicians to ensure the interpretation of each dialog turn within the conversation is conducted from a clinical perspective.</p>
 <p>Given that previous research revealed users view chatbots as less human-like [<span class="ref-lnk lazy-ref"><a data-rid="cit0061" data-reflink="_i29 _i30 _i31 _i32" href="#"><span class="off-screen">Citation</span>61</a></span>], social dialogue features like empathy and meta-relational communication, which improve the working alliance, were integrated into <i>Anna</i>, following recommended best practices for AI in mental health care [<span class="ref-lnk lazy-ref"><a data-rid="cit0102" data-reflink="_i29 _i30 _i31 _i32" href="#"><span class="off-screen">Citation</span>102</a></span>,<span class="ref-lnk lazy-ref"><a data-rid="cit0113" data-reflink="_i29 _i30 _i31 _i32" href="#"><span class="off-screen">Citation</span>113</a></span>]. For instance, to help build rapport, <i>Anna</i> refers to users by name in conversations, and communicates curiosity about users by prompting them for background information and relationships with others when such information is mentioned in the normal course of conversation. Anna further communicates interest and understanding by using clarifying examples and content-mirroring responses to follow up user input in conversations. At the beginning of each activity, Anna also initiates a greeting dialog, which may include surface-level information (e.g. noting how often the user logs on) or reference more personal information acquired in past conversations.</p>
 <p>The information gathered during these conversations is stored confidentially along with general user data (e.g. in-app assessment scores, activity across the platform) to form a model of the individual user. <i>Anna</i> can then reference this user model to create a more personalized experience, both relationally (e.g. celebrating an achievement) and by tailoring activities to users (e.g. recommending users perform a favorite activity with a specific person in their life, rather than suggesting they engage in a generic activity with someone).</p>
 <div id="s0006-s2001" class="NLM_sec NLM_sec_level_2">
  <h3 class="section-heading-3" id="_i21">6.1. Preliminary evidence for Anna</h3>
  <p>Although research on <i>Anna</i> is in the preliminary stages, a pilot test of how users perceived <i>Anna</i> following an interaction showed that 89.6% of the 203 surveyed users rated <i>Anna</i> as helpful. When these users were asked to rate <i>Anna</i> on a series of attributes, 74.9%, 73.3%, and 76.8% of users selected ‘agree’ or ‘strongly agree’ on the statements ‘<i>Anna listens to me</i>,’ ‘<i>Anna is curious about me</i>,’ and ‘<i>Anna gives me insights that I can use</i>,’ respectively. Thus, consistent with other chatbot research, users report positive perceptions of <i>Anna</i>.</p>
  <p>In another pilot study we conducted to explore how the addition of <i>Anna</i> within Happify activities would influence participants’ engagement with those activities, we found that participants who received versions of Happify activities led by <i>Anna</i> provided more elaborate responses than those who received the regular Happify activities. We compared participants’ text responses within activities over the course of four weeks based on whether they were exposed to regular Happify activities (<i>n</i> =&nbsp;252) or those enhanced by the addition of <i>Anna</i> (<i>n</i> =&nbsp;237). To do so, we gathered and cleaned all text data (i.e. converted contractions to separate words, removed punctuation, changed uppercase letters to lowercase, and transformed future and past tense words to present tense), and then compared overall word and character counts in participants’ written responses across conditions. Participants in the Happify+<i>Anna</i> condition used significantly more words (<i>M</i> =&nbsp;27.25, <i>SD&nbsp;</i>=&nbsp;25.63) and more characters (<i>M</i> =&nbsp;156.29, <i>SD</i>&nbsp;=&nbsp;151.23) per entry overall compared to participants in the regular Happify condition (<i>M</i> =&nbsp;14.93, <i>SD</i>&nbsp;=&nbsp;14.18, and <i>M</i> =&nbsp;83.93, <i>SD</i>&nbsp;=&nbsp;82.99, respectively; <i>p</i>s &lt; .001).</p>
  <p>While writing more overall does not necessarily suggest participants were engaging more deeply with the activities, we also ran text analysis to identify what topics participants wrote most about, collapsing across the two conditions. To reduce overlap in topics, we identified the four most common themes (note, however, that there is still overlap in words across themes). The first theme included <i>positive relational</i> words like love, sure, and like. The second theme included <i>achievement-related</i> words like work, think, and feel. The third theme included <i>mindfulness-related</i> words like know, savor, and walk. And the fourth theme included <i>cognition-related</i> words like think, feel, and negative. We then compared the frequency of words from each theme in participants’ responses across conditions. We found no significant difference in frequency of positive relational words, χ<sup>2</sup>&nbsp;=&nbsp;0.06, <i>p</i> =&nbsp;.950. However, participants in the Happify condition used significantly more achievement-related words than those in the Happify+<i>Anna</i> condition, χ<sup>2</sup>&nbsp;=&nbsp;237.18, <i>p</i> &lt;&nbsp;.0001, but significantly fewer mindfulness-related, χ2&nbsp;=&nbsp;13.76, <i>p</i> &lt;&nbsp;.001, and cognition-related, χ2&nbsp;=&nbsp;133.32, <i>p</i> &lt;&nbsp;.0001, words. Thus, participants with the AI-enhanced activities were not only more elaborative, but also used more words directly relevant to the tasks Happify asks them to do, such as mindfulness and cognition. While promising, additional research considering the context of the response, rather than measuring the frequency of words, is necessary to better understand how users’ responses differ when guided by a chatbot.</p>
 </div>
</div>
<div id="s0007" class="NLM_sec NLM_sec_level_1">
 <h2 id="_i22" class="section-heading-2">7. Open questions &amp; important considerations</h2>
 <p>Despite the rapid increase of AI in DMHIs in recent years, chatbot technology remains relatively novel and experimental [<span class="ref-lnk lazy-ref"><a data-rid="cit0052" data-reflink="_i29 _i30 _i31 _i32" href="#"><span class="off-screen">Citation</span>52</a></span>], and research on mental health chatbots is limited. Consequently, there are several important questions in regards to the use of AI-based chatbots in DMHIs, particularly as they expand to perform more functions.</p>
 <div id="s0007-s2001" class="NLM_sec NLM_sec_level_2">
  <h3 class="section-heading-3" id="_i23">7.1. Perceptions of AI in mental health care</h3>
  <p>Although research suggests that users rate chatbots favorably [<span class="ref-lnk lazy-ref"><a data-rid="cit0098" data-reflink="_i29 _i30 _i31 _i32" href="#"><span class="off-screen">Citation</span>98</a></span>], few studies have examined the perceptions of chatbots in the general population. Consequently, it is unclear whether the integration of chatbots may encourage or inhibit uptake of DMHIs. Examining beliefs about chatbots among people who are not already using DMHIs would provide critical insight into the barriers to engaging with chatbots in DMHIs, stigmas associated with the use of AI for mental health care, and perceptions of security and privacy. Similarly, few studies have explored perceptions of mental health chatbots among mental health professionals. While some research suggests that healthcare professionals generally view chatbots favorably [<span class="ref-lnk lazy-ref"><a data-rid="cit0080" data-reflink="_i29 _i30 _i31 _i32" href="#"><span class="off-screen">Citation</span>80</a></span>,<span class="ref-lnk lazy-ref"><a data-rid="cit0114" data-reflink="_i29 _i30 _i31 _i32" href="#"><span class="off-screen">Citation</span>114</a></span>], other research found that 48.7% of surveyed psychiatrists from 22 countries reported that AI would only have a minimal impact on their work in the future [<span class="ref-lnk lazy-ref"><a data-rid="cit0115" data-reflink="_i29 _i30 _i31 _i32" href="#"><span class="off-screen">Citation</span>115</a></span>]. Furthermore, personal experience with chatbots among these professionals is low and predicts whether they recommend these programs to their patients [<span class="ref-lnk lazy-ref"><a data-rid="cit0080" data-reflink="_i29 _i30 _i31 _i32" href="#"><span class="off-screen">Citation</span>80</a></span>]. Thus, acceptance from healthcare practitioners is an important step to greater uptake of chatbot-driven DMHIs. Beyond assessing perceptions of AI, developers of mental health chatbots need to consider this task a collaboration between themselves, mental health professionals, and users/patients in order to develop chatbots that meet patients’ needs, goals and lifestyles, ensure trust in AI, and improve mental health outcomes. Consequently, the development of chatbots should be viewed as an iterative process involving regular feedback from practitioners and users alike.</p>
 </div>
 <div id="s0007-s2002" class="NLM_sec NLM_sec_level_2">
  <h3 class="section-heading-3" id="_i24">7.2. Empathy in chatbots</h3>
  <p>One criticism of incorporating chatbots into mental health care is that, by definition, they cannot feel empathy as human beings do [<span class="ref-lnk lazy-ref"><a data-rid="cit0116 cit0117 cit0118" data-reflink="_i29 _i30 _i31 _i32" href="#"><span class="off-screen">Citation</span>116–118</a></span>]. Indeed, while psychiatrists appear to believe that AI may be capable of performing some psychiatric tasks (e.g. documentation), a global survey of psychiatrists found that 83% of respondents felt AI would never be able to match a psychiatrist in terms of empathetic care [<span class="ref-lnk lazy-ref"><a data-rid="cit0115" data-reflink="_i29 _i30 _i31 _i32" href="#"><span class="off-screen">Citation</span>115</a></span>].</p>
  <p>However, the extent to which chatbots are actually empathetic may be less relevant, and what likely matters most is whether patients <i>perceive</i> chatbots to be less empathetic than humans or clinicians [<span class="ref-lnk lazy-ref"><a data-rid="cit0119" data-reflink="_i29 _i30 _i31 _i32" href="#"><span class="off-screen">Citation</span>119</a></span>]. Qualitative research suggests users perceive chatbots, like <i>Woebot</i>, as communicating empathy and emotional support [<span class="ref-lnk lazy-ref"><a data-rid="cit0120" data-reflink="_i29 _i30 _i31 _i32" href="#"><span class="off-screen">Citation</span>120</a></span>], and other research has shown that interacting with a chatbot following ostracism can buffer against the negative effects of social exclusion [<span class="ref-lnk lazy-ref"><a data-rid="cit0120" data-reflink="_i29 _i30 _i31 _i32" href="#"><span class="off-screen">Citation</span>120</a></span>]. Research also supports that chatbots designed to display empathic reactions are rated more positively (i.e. more enjoyable, understanding, sociable, trustworthy, intelligent) than one that is not programmed to respond empathetically [<span class="ref-lnk lazy-ref"><a data-rid="cit0120" data-reflink="_i29 _i30 _i31 _i32" href="#"><span class="off-screen">Citation</span>120</a></span>]. Thus, users may perceive chatbots as empathetic, though the extent to which perceptions of empathy differ between chatbots and people or clinicians remains unclear. Further, although preliminary research suggests users can form a therapeutic relationship with chatbots [<span class="ref-lnk lazy-ref"><a data-rid="cit0121" data-reflink="_i29 _i30 _i31 _i32" href="#"><span class="off-screen">Citation</span>121</a></span>], literature on how users perceive social attributes, such as personality and credibility, of chatbots is lacking. In addition to understanding general perceptions of chatbots, it is crucial to understand the impressions users form of mental health chatbots during interactions and the extent to which these compare to impressions of human practitioners.</p>
 </div>
 <div id="s0007-s2003" class="NLM_sec NLM_sec_level_2">
  <h3 class="section-heading-3" id="_i25">7.3. Individual differences</h3>
  <p>The use of AI has great potential to expand and improve DMHIs [<span class="ref-lnk lazy-ref"><a data-rid="cit0114" data-reflink="_i29 _i30 _i31 _i32" href="#"><span class="off-screen">Citation</span>114</a></span>,<span class="ref-lnk lazy-ref"><a data-rid="cit0122" data-reflink="_i29 _i30 _i31 _i32" href="#"><span class="off-screen">Citation</span>122</a></span>]. However, individual differences, such as demographic characteristics or medical history (clinical vs. non-clinical populations), may impact how people respond to chatbots, yet this area of research remains underexplored. For example, research suggests that young people are hesitant to access mental health care due to perceived stigma [<span class="ref-lnk lazy-ref"><a data-rid="cit0123" data-reflink="_i29 _i30 _i31 _i32" href="#"><span class="off-screen">Citation</span>123</a></span>]. Other research indicates adolescents prefer online conversations to in-person interactions for managing difficult conversations [<span class="ref-lnk lazy-ref"><a data-rid="cit0124" data-reflink="_i29 _i30 _i31 _i32" href="#"><span class="off-screen">Citation</span>124</a></span>]. Conceivably, then, younger people may respond differently to the use of chatbots in mental health care. There is also a dearth of information regarding how race and ethnicity [<span class="ref-lnk lazy-ref"><a data-rid="cit0007" data-reflink="_i29 _i30 _i31 _i32" href="#"><span class="off-screen">Citation</span>7</a></span>], gender, SES, and other factors may play a role in the responses to and outcomes associated with AI-supported DMHIs.</p>
  <p>In addition to demographic variables, individual differences may also influence how people respond to chatbots in DMHIs. For example, in the pilot study we described earlier exploring the effects of integrating the chatbot <i>Anna</i> to Happify activities, we found that although the proportion of participants who dropped out of the study did not differ based on whether they had access to the chatbot or not (<i>p</i> =&nbsp;.943), the effect of mental health self-efficacy on dropout did differ based on condition. Participants who dropped out in the regular Happify condition had significantly lower baseline levels of mental health self-efficacy than those who completed the four-week study (<i>p</i> &lt;&nbsp;.001), whereas mental health self-efficacy did not predict dropout in the Happify+<i>Anna</i> condition (<i>p</i> =&nbsp;.192). This finding might suggest that, in the absence of any support, people who are less confident in their ability to manage their mental health are at a higher risk of terminating the intervention prematurely. However, chatbots, like <i>Anna</i>, may provide sufficient support to mitigate these effects, allowing people with lower levels of mental health self-efficacy to feel more confident in their ability to manage their mental health through the intervention. This effect is preliminary and should be interpreted with caution, but is notable given that, compared to people with greater self-efficacy, these individuals may benefit more from DMHIs in terms of depression, anxiety, and overall distress [<span class="ref-lnk lazy-ref"><a data-rid="cit0125" data-reflink="_i29 _i30 _i31 _i32" href="#"><span class="off-screen">Citation</span>125</a></span>]. In other words, the addition of an AI coach may allow people who are in greater need of an intervention to follow through with that intervention. This highlights the importance of exploring the role of other individual differences to better understand when, and for whom, chatbots are most beneficial. Without consideration of demographic and individual differences, the use of chatbots in DMHIs remains limited and introduces concerns regarding risks, safety, and effectiveness [<span class="ref-lnk lazy-ref"><a data-rid="cit0126" data-reflink="_i29 _i30 _i31 _i32" href="#"><span class="off-screen">Citation</span>126</a></span>,<span class="ref-lnk lazy-ref"><a data-rid="cit0127" data-reflink="_i29 _i30 _i31 _i32" href="#"><span class="off-screen">Citation</span>127</a></span>].</p>
 </div>
 <div id="s0007-s2004" class="NLM_sec NLM_sec_level_2">
  <h3 class="section-heading-3" id="_i26">7.4. Ethical considerations</h3>
  <p>Considering the importance of trust for therapeutic relationships [<span class="ref-lnk lazy-ref"><a data-rid="cit0128" data-reflink="_i29 _i30 _i31 _i32" href="#"><span class="off-screen">Citation</span>128</a></span>], garnering users’ trust is an important consideration in DMHIs [<span class="ref-lnk lazy-ref"><a data-rid="cit0100" data-reflink="_i29 _i30 _i31 _i32" href="#"><span class="off-screen">Citation</span>100</a></span>,<span class="ref-lnk lazy-ref"><a data-rid="cit0129" data-reflink="_i29 _i30 _i31 _i32" href="#"><span class="off-screen">Citation</span>129</a></span>]. Maintaining trust requires a discussion of ethics, privacy, confidentiality, and safety [<span class="ref-lnk lazy-ref"><a data-rid="cit0130" data-reflink="_i29 _i30 _i31 _i32" href="#"><span class="off-screen">Citation</span>130</a></span>]. The highly personal and sensitive nature of mental health information highlights the need to ensure that sensitive patient data is adequately secured and protected. Further, AI in DMHIs introduces additional risk for potential harms, including racial prejudice due to the potential for algorithmic bias [<span class="ref-lnk lazy-ref"><a data-rid="cit0131" data-reflink="_i29 _i30 _i31 _i32" href="#"><span class="off-screen">Citation</span>131</a></span>], crisis response limitations [<span class="ref-lnk lazy-ref"><a data-rid="cit0132" data-reflink="_i29 _i30 _i31 _i32" href="#"><span class="off-screen">Citation</span>132</a></span>], and safety concerns [<span class="ref-lnk lazy-ref"><a data-rid="cit0132" data-reflink="_i29 _i30 _i31 _i32" href="#"><span class="off-screen">Citation</span>132</a></span>]. In some cases, for example, DMHIs have lacked evidence to support their claims regarding improvements in mental health [<span class="ref-lnk lazy-ref"><a data-rid="cit0132" data-reflink="_i29 _i30 _i31 _i32" href="#"><span class="off-screen">Citation</span>132</a></span>], provided inaccurate health education [<span class="ref-lnk lazy-ref"><a data-rid="cit0134" data-reflink="_i29 _i30 _i31 _i32" href="#"><span class="off-screen">Citation</span>134</a></span>], failed to spot signs of sexual abuse [<span class="ref-lnk lazy-ref"><a data-rid="cit0133" data-reflink="_i29 _i30 _i31 _i32" href="#"><span class="off-screen">Citation</span>133</a></span>, <span class="ref-lnk lazy-ref"><a data-rid="cit0134" data-reflink="_i29 _i30 _i31 _i32" href="#"><span class="off-screen">Citation</span>134</a></span>], and encouraged unsafe behavior [<span class="ref-lnk lazy-ref"><a data-rid="cit0135" data-reflink="_i29 _i30 _i31 _i32" href="#"><span class="off-screen">Citation</span>135</a></span>,<span class="ref-lnk lazy-ref"><a data-rid="cit0136" data-reflink="_i29 _i30 _i31 _i32" href="#"><span class="off-screen">Citation</span>136</a></span>]. Therefore, challenges with respect to regulating AI in DMHIs for safety and effectiveness are a particular concern for the relatively novel field of digital mental health [<span class="ref-lnk lazy-ref"><a data-rid="cit0137" data-reflink="_i29 _i30 _i31 _i32" href="#"><span class="off-screen">Citation</span>137</a></span>].</p>
  <p>Given the dynamic and iterative nature of AI-supported DMHIs, standards for evaluation are especially challenging [<span class="ref-lnk lazy-ref"><a data-rid="cit0138" data-reflink="_i29 _i30 _i31 _i32" href="#"><span class="off-screen">Citation</span>138</a></span>]. The existing literature on mental health chatbots includes many calls for considerations of AI-specific ethics [<span class="ref-lnk lazy-ref"><a data-rid="cit0129" data-reflink="_i29 _i30 _i31 _i32" href="#"><span class="off-screen">Citation</span>129</a></span>], but definitions of ethical guidelines have yet to be established and widely adopted. Furthermore, research suggests that ethical and regulatory concerns associated with AI are not often considered by psychiatrists when evaluating the role of AI in mental health [<span class="ref-lnk lazy-ref"><a data-rid="cit0136" data-reflink="_i29 _i30 _i31 _i32" href="#"><span class="off-screen">Citation</span>136</a></span>]. As the FDA and American Psychiatric Association [<span class="ref-lnk lazy-ref"><a data-rid="cit0139" data-reflink="_i29 _i30 _i31 _i32" href="#"><span class="off-screen">Citation</span>139</a></span>,<span class="ref-lnk lazy-ref"><a data-rid="cit0140" data-reflink="_i29 _i30 _i31 _i32" href="#"><span class="off-screen">Citation</span>140</a></span>] issue guidance on these critical topics, there exists an opportunity for those applying AI in mental health care to incorporate ethical, safety, and efficacy standards and values into DMHIs.</p>
 </div>
</div>
<div id="s0008" class="NLM_sec NLM_sec_level_1">
 <h2 id="_i27" class="section-heading-2">8. Conclusion</h2>
 <p>The availability of effective AI-supported interventions is an important avenue to reduce the longstanding burden on practitioners and improve the increasing shortage of mental health professionals [<span class="ref-lnk lazy-ref"><a data-rid="cit0008" data-reflink="_i29 _i30 _i31 _i32" href="#"><span class="off-screen">Citation</span>8</a></span>]. Although preliminary research suggests chatbots are perceived favorably and may help to improve engagement and mental health outcomes, more rigorous tests of chatbots within DMHIs are needed. In particular, more research on how chatbots may help to improve mental health outcomes compared to other digital interventions without chatbots is an important next step, as is considering how individual and contextual factors might influence the impact of mental health chatbots.</p>
</div>
<div id="s0009" class="NLM_sec NLM_sec_level_1">
 <h2 id="_i28" class="section-heading-2">9. Expert opinion: AI as the future of digital mental health</h2>
 <p>To be truly scalable, while optimizing engagement and adherence, digital health solutions must embrace AI, including chatbots, as a means of offering support. Indeed, in one survey of mental health professionals, the majority of respondents indicated healthcare chatbots would play a more significant role than healthcare providers in the future [<span class="ref-lnk lazy-ref"><a data-rid="cit0080" data-reflink="_i29 _i30 _i31 _i32" href="#"><span class="off-screen">Citation</span>80</a></span>]. While skeptics remain, the integration of AI in DMHIs inevitably expands the possibilities of what can be done within digital health. Recent advances in AI technology, paired with the evolution of digital healthcare systems, are making it possible to take digital healthcare to the next level by making it dynamic, transparent, hyper-personalized, and human-like. This will result in levels of engagement and efficacy that are not yet possible today.</p>
 <p>As technology improves and people become more digitally connected, the streams of data available for AI algorithms to analyze are no longer limited to isolated tests and care sessions. Instead, they feed on ongoing streams of data from multiple sources, particularly as the use of wearables and sensors expands. This data can be integrated in real-time to determine a patient’s current state, detect deviations from usual patterns, and make probabilistic predictions of an individual’s future health state, thereby enabling digital mental health systems to respond to events as they take place (or even before they take place).</p>
 <p>Such integration of various data sources will permit a level of personalization and responsiveness that is impossible from human beings alone. Unlike human practitioners, AI algorithms can quickly analyze large amounts of user data to understand and respond to users. For instance, chatbots can predict personality based on a user’s language and subsequently adapt their own personality to match the user’s. Much like mimicry in face-to-face interactions, matching chatbot personalities to user personalities leads to more productive interactions [<span class="ref-lnk lazy-ref"><a data-rid="cit0141" data-reflink="_i29 _i30 _i31 _i32" href="#"><span class="off-screen">Citation</span>141</a></span>].</p>
 <p>AI algorithms also have access to vast amounts of normative data that can be used to train accurate models that infer a patient’s current state, and these models can be revised as new data becomes available. This could result in fine-tuned, responsive models of care that adapt quickly based on where the patient is on their care journey, what is happening in their lives, and even how their day is going.</p>
 <p>Conceivably, while chatbots may never be truly empathetic, access to this normative data along with recent advances in generative deep learning will allow AI-based chatbots to interface with patients in a way that communicates empathy. As we learn from existing chatbots and user feedback, chatbots will become more effective at delivering interactions that are conversational and natural. More importantly, they will become more sensitive to patient states and able to express the appropriate emotions and actions during these interactions. Chatbots will exhibit empathy, curiosity, understanding, and a sense of working collaboratively with patients rather than being one-sided and authoritative, resulting in deeper intrinsic patient motivation to engage and adhere. As such, true therapeutic alliances between users and chatbots will be possible.</p>
 <p>To reach these goals, we need to learn more about the existing strengths and limitations of AI-based chatbots in DMHIs. That requires more widespread adoption of chatbots within DHMI. However, numerous barriers exist to widespread adoption. One important barrier is that there are many misconceptions about the intended role of chatbots in DMHIs. Although critics argue chatbots cannot replace human interaction/therapy [<span class="ref-lnk lazy-ref"><a data-rid="cit0142" data-reflink="_i29 _i30 _i31 _i32" href="#"><span class="off-screen">Citation</span>142</a></span>], most creators of chatbots in DMHIss never intended for their chatbots to replace human therapists. Rather, chatbots were primarily developed to help increase engagement, support the prevention of mental disorders by delivering more engaging and adaptive interventions [<span class="ref-lnk lazy-ref"><a data-rid="cit0030" data-reflink="_i29 _i30 _i31 _i32" href="#"><span class="off-screen">Citation</span>30</a></span>,<span class="ref-lnk lazy-ref"><a data-rid="cit0143" data-reflink="_i29 _i30 _i31 _i32" href="#"><span class="off-screen">Citation</span>143</a></span>], and to re-engage users they identify at risk of dropout. Research also suggests that people may self-disclose more to a mental health practitioner when it is facilitated via self-disclosing chatbots [<span class="ref-lnk lazy-ref"><a data-rid="cit0144" data-reflink="_i29 _i30 _i31 _i32" href="#"><span class="off-screen">Citation</span>144</a></span>]. Thus, chatbots can be effective adjunctive options and may help make in-person therapy more effective. In other words, in its best form, AI algorithms would not replace human intelligence, but augment it (hence why some use the term ‘augmented intelligence’ over ‘artificial intelligence’). Future research exploring the extent to which a combination of AI chatbots, DMHIs, and clinician support compared to DMHIs or clinician support alone will be an important step to understanding how AI chatbots can augment both DMHIs as well as traditional clinician-led therapy.</p>
 <p>An important step to reducing these misconceptions is to focus more on working with and educating mental health professionals about AI in mental health settings. While practitioners generally agree that mental health chatbots are beneficial and important, they also express trepidation about using AI in domains like diagnostics, counseling, and delivering CBT [<span class="ref-lnk lazy-ref"><a data-rid="cit0080" data-reflink="_i29 _i30 _i31 _i32" href="#"><span class="off-screen">Citation</span>80</a></span>,<span class="ref-lnk lazy-ref"><a data-rid="cit0114" data-reflink="_i29 _i30 _i31 _i32" href="#"><span class="off-screen">Citation</span>114</a></span>]. Personal experience with mental health chatbots also tends to be quite low, although practitioners with personal experience are more likely to recommend chatbot-driven DMHIs to their patients. Consequently, ensuring the digital therapeutic industry is working hand-in-hand with healthcare professionals as we expand the use of AI in digital interventions, while also encouraging healthcare professionals to engage with these interventions themselves, will be crucial to securing support from the broader healthcare community.</p>
 <p>Part of what contributes to the misconceptions is the low levels of adoption of chatbots in large-scale DMHIs. As few as 24% of the top-funded DMHIs companies include a conversational agent [<span class="ref-lnk lazy-ref"><a data-rid="cit0070" data-reflink="_i29 _i30 _i31 _i32" href="#"><span class="off-screen">Citation</span>70</a></span>], so few people (or practitioners) have interacted with chatbots in this particular domain. This is compounded by the fact that research on mental health chatbots is limited. Most studies conducted thus far do not include a control group or compare the effects of interacting with a chatbot to a very different approach, like reading an e-book with psychoeducational content. Few, if any, studies have used an adequate sham condition as the control group and, as a result, the extent to which chatbots enhance the effects of DMHIs is unclear. This data will be important to clarify what AI chatbots can and cannot do in this context, and reduce misconceptions about AI. There is a pressing need for more rigorous research on AI in digital therapeutics, and more transparency and communication among AI developers in this domain. Ultimately, the extent to which the digital therapeutics industry engages in more widespread adoption of AI within their interventions, that they engage in a patient-centered approach to developing and referring chatbots, and that they commit to conducting research using best practices will determine the future of AI in DMHIs.</p>
</div>