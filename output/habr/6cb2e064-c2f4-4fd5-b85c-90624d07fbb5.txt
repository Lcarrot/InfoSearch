<div>
 <div class="article-formatted-body article-formatted-body article-formatted-body_version-2">
  <div xmlns="http://www.w3.org/1999/xhtml">
   <figure class="full-width ">
    <img src="https://habrastorage.org/r/w780q1/getpro/habr/upload_files/fc7/c08/763/fc7c087631101613d9908d70e15be0d7.jpeg" width="1396" height="931" data-src="https://habrastorage.org/getpro/habr/upload_files/fc7/c08/763/fc7c087631101613d9908d70e15be0d7.jpeg" data-blurred="true">
    <figcaption></figcaption>
   </figure>
   <p>В&nbsp;этот раз к&nbsp;нам пришел клиент,&nbsp;желавший запустить собственную SSP (Supply‑Side Platform). Это система, которая позволяет владельцам отдельных сайтов или&nbsp;целых сетей продавать рекламные места и получать доход от&nbsp;размещения объявлений. У&nbsp;клиента уже&nbsp;был реализован MVP системы, но&nbsp;оставалось еще много работы.</p>
   <p>Меня зовут Сергей Дербуш, я архитектор в&nbsp;компании «СмартАп Технолоджи». Расскажу о&nbsp;том, что&nbsp;мы доделывали, чтобы система заработала на&nbsp;полную, и как&nbsp;это бустануло навыки всей команды.</p>
   <h3>С чем пришел клиент</h3>
   <p>Клиент занимается установкой рекламы на&nbsp;сайты. Для&nbsp;установки рекламы используется JavaScript с&nbsp;подключенной библиотекой Prebid.js. Процесс устроен следующим образом:</p>
   <ul>
    <li><p>к&nbsp;нашему клиенту приходит владелец сайта (также он зовется паблишером);</p></li>
    <li><p>наш клиент подключает партнеров, через которых можно запрашивать рекламу. Для&nbsp;этого он регистрируется в&nbsp;системах партнеров, заключает договор и получает идентификатор;</p></li>
    <li><p>клиент подготавливает скрипт для&nbsp;запроса и отображения рекламы от&nbsp;партнеров, для&nbsp;этого в&nbsp;библиотеке Prebid.js необходимо подключить адаптеры партнеров;</p></li>
    <li><p>реклама, предоставляемая партнерами, отображается на&nbsp;сайте владельца и считается стоимость ее показа;</p></li>
    <li><p>партнеры предоставляют отчеты о&nbsp;показах и их стоимости;</p></li>
    <li><p>партнеры выплачивают деньги нашему клиенту;</p></li>
    <li><p>наш клиент берет процент за&nbsp;услуги предоставления рекламы;</p></li>
    <li><p>наш клиент выплачивает деньги владельцу сайта;</p></li>
   </ul>
   <p>Партнеры которые предоставляют рекламу являются SSP системами.</p>
   <p>SSP тесно связанно с&nbsp;DSP. DSP (Demand Side Platform)&nbsp;— это система, которая облегчает процесс покупки и продажи рекламных мест. Рекламодатель подключается к&nbsp;DSP, чтобы покупать и размещать рекламу в&nbsp;интернете, отслеживать результаты, оптимизировать все запущенные кампании&nbsp;— все это на&nbsp;едином сервисе и в&nbsp;одном интерфейсе.</p>
   <p>DSP анализирует поступающие от&nbsp;SSP запросы и предоставляет рекламу со ставкой на&nbsp;покупку рекламных мест. Взаимодействие между Prebid.js, SSP и DSP происходит по&nbsp;открытому протоколу OpenRTB.</p>
   <p>Клиент пришел к&nbsp;нам, чтобы запустить свою SSP, в&nbsp;нее будут интегрированы DSP, с&nbsp;которыми есть контракт. Так как&nbsp;DSP отдает рекламу со ставкой, а&nbsp;SSP уменьшает эту ставку на&nbsp;некоторый процент за&nbsp;предоставление своих услуг. Идея заключалась в&nbsp;том, чтобы использовать свою SSP для&nbsp;показа рекламы и зарабатывать на&nbsp;SSP самим.</p>
   <h3>Как работает Prebid.js</h3>
   <p>Prebid.js это библиотека, которая работает по&nbsp;технологии header bidding.</p>
   <figure class="full-width ">
    <img src="https://habrastorage.org/r/w1560/getpro/habr/upload_files/f22/662/339/f226623396a49c2c56648e05a17c84af.png" width="1592" height="1032" data-src="https://habrastorage.org/getpro/habr/upload_files/f22/662/339/f226623396a49c2c56648e05a17c84af.png">
    <figcaption></figcaption>
   </figure>
   <p>Во&nbsp;время загрузки страницы сайта Prebid.js отправляет запросы на&nbsp;получение рекламы через Prebid.js адаптеры подключенных партнеров (SSPs). В&nbsp;запросе отправляются все места и размеры под&nbsp;баннеры (высота и ширина). В&nbsp;ответ SSPs присылают рекламу и ставки, по&nbsp;которым реклама будет показана.</p>
   <p>Prebid.js собирает все ставки от&nbsp;партнеров на&nbsp;показ баннера. Так как&nbsp;на&nbsp;одно рекламное место может приходить несколько предложений, то Prebid.js проводит, так называемый, аукцион первой цены. Это когда выбирается баннер с&nbsp;максимальной ставкой (стоимостью показа). Выигравшая реклама отображается на&nbsp;сайте.</p>
   <h3>Как работает SSP&nbsp;</h3>
   <figure class="">
    <img src="https://habrastorage.org/r/w1560/getpro/habr/upload_files/203/fbd/b51/203fbdb516d81d66bc58848b25c4567b.png" width="509" height="161" data-src="https://habrastorage.org/getpro/habr/upload_files/203/fbd/b51/203fbdb516d81d66bc58848b25c4567b.png">
    <figcaption></figcaption>
   </figure>
   <p>SSP работает по&nbsp;открытому протоколу OpenRTB. Она получает запрос с&nbsp;местами под&nbsp;баннеры и их размеры. SSP собирает информацию о&nbsp;пользователе (геолокацию, браузер, устройство и&nbsp;т.&nbsp;д.), чтобы иметь возможность идентифицировать пользователя в&nbsp;будущем.</p>
   <p>Далее SSP отправляет запросы с&nbsp;местами под&nbsp;баннеры для&nbsp;получения рекламы от&nbsp;подключенных DSP. Она получает и собирает ставки от&nbsp;всех DSP, от&nbsp;которых пришел ответ.</p>
   <p>SSP может взимать процент за&nbsp;использование с&nbsp;каждой ставки и возвращать уменьшенную ставку. В&nbsp;конце SSP отправляет собранные ставки и возвращает результат запроса.</p>
   <p>SSP хранит идентификатор пользователя, который ему предоставляет DSP. SSP отправляет идентификатор пользователя при&nbsp;каждом запросе, чтобы DSP могла идентифицировать одного и того&nbsp;же пользователя.</p>
   <p>SSP добавляет трекер для&nbsp;отслеживания показа рекламы на&nbsp;клиенте. Также она готовит ежедневные отчеты о&nbsp;показах и их стоимости и сравнивает отчеты с&nbsp;отчетами DSP. Несоответствие между ними не&nbsp;должно превышать 5%.</p>
   <h3>У клиента было реализовано MVP, но предстояло еще много работы</h3>
   <figure class="full-width ">
    <img src="https://habrastorage.org/r/w1560/getpro/habr/upload_files/fe3/128/bb2/fe3128bb25f88e82839e0686f4d4d7a2.png" width="1600" height="1227" data-src="https://habrastorage.org/getpro/habr/upload_files/fe3/128/bb2/fe3128bb25f88e82839e0686f4d4d7a2.png">
    <figcaption></figcaption>
   </figure>
   <p>В&nbsp;начале работы с&nbsp;клиентской системой мы выяснили, что&nbsp;у&nbsp;него есть база паблишеров (владельцев сайтов), на&nbsp;сайтах которых установлены скрипты с&nbsp;библиотекой Prebid.js и отображается реклама от&nbsp;нескольких SSP, подключенных через Prebid.js адаптеры.</p>
   <p>Клиент хотел опубликовать свой Prebid.js адаптер, который запрашивал&nbsp;бы рекламу через свою&nbsp;же SSP. В&nbsp;наличии уже&nbsp;был неопубликованный Prebid.js адаптер с&nbsp;возможностью отправки запроса на&nbsp;получения баннерной рекламы от&nbsp;SSP.</p>
   <p>У&nbsp;клиента также&nbsp;был UI для&nbsp;добавления новых паблишеров, сайтов и рекламных блоков для&nbsp;сайта. В&nbsp;UI можно&nbsp;было настраивать параметры для&nbsp;интегрированных DSP (вкл/выкл, время отклика и&nbsp;т.&nbsp;д.)</p>
   <p>SSP могла определять устройство, с&nbsp;которого пришел пользователь, откуда (геолокацию), а&nbsp;также умела подготавливать OpenRTB запрос для&nbsp;DSP и получать ответ. Однако, изучая возможности системы, мы не&nbsp;смогли получить реальную ставку от&nbsp;DSP&nbsp;— интеграции не&nbsp;работали и возвращали пустой ответ.</p>
   <p>Были предоставлены скрипты Terraform для&nbsp;развертывания инфраструктуры в&nbsp;AWS (Amazon Web Services). В&nbsp;скриптах написан модуль для&nbsp;развертывания Kubernetes кластера через сервис AWS EKS. Развернуто стейджинг окружение. Запуск Terraform скриптов производился с&nbsp;локальной машины.</p>
   <p>Предоставлены YAML файлы для&nbsp;запуска SSP сервисов в&nbsp;Kubernetes. А&nbsp;еще построен CI/CD pipeline для&nbsp;доставки изменений в&nbsp;сервисе SSP. В&nbsp;качестве CI использовался Circle CI.</p>
   <p>Отсутствовали мониторинг и логирование. Зато&nbsp;был PySpark Glue Job для&nbsp;сбора и агрегации данных о&nbsp;показах рекламы, которые складываются на&nbsp;S3&nbsp;через сервис Kinesis Firehose</p>
   <p>У&nbsp;клиента уже&nbsp;был готов UI для&nbsp;генерации отчетов о&nbsp;количестве полученной/показанной рекламы, общей стоимости по:</p>
   <ul>
    <li><p>Паблишеру.</p></li>
    <li><p>Дате.</p></li>
    <li><p>DSP партнеру.</p></li>
    <li><p>Веб‑сайту.</p></li>
   </ul>
   <p>Из технологий использовались:</p>
   <ul>
    <li><p>Github&nbsp;— репозиторий с&nbsp;кодом.</p></li>
    <li><p>CircleCI для&nbsp;сборки докер образа и применения Kubernetes шаблонов.</p></li>
    <li><p>AWS сервисы:<br></p>
     <ul>
      <li><p>EKS&nbsp;— сервис для&nbsp;запуска управляемого кластера Kubernetes;</p></li>
      <li><p>EC2&nbsp;— сервис, предоставляющий виртуальные машины, в&nbsp;том числе и машины для&nbsp;EKS;</p></li>
      <li><p>ECR&nbsp;— это хранилище для&nbsp;Docker образов;</p></li>
      <li><p>Cognito&nbsp;— сервис, используемый для&nbsp;авторизации в&nbsp;SSP;</p></li>
      <li><p>Kinesis Firehose&nbsp;— сервис для&nbsp;получения потоков данных в&nbsp;реальном времени. Нужен для&nbsp;сбора информации о&nbsp;ставках и показах, чтобы на&nbsp;их основе формировать отчеты;</p></li>
      <li><p>Cервис баз данных RDS;</p></li>
      <li><p>OpenSearch&nbsp;— этот сервис используется для&nbsp;поиска и для&nbsp;хранения логов от&nbsp;SSP;</p></li>
      <li><p>Glue&nbsp;— ETL cервис, позволяющий строить процессы для&nbsp;обработки данных;</p></li>
      <li><p>S3&nbsp;— хранилище данных, в&nbsp;том числе тех, что&nbsp;сохранены через Kinesis Firehose;</p></li>
      <li><p>Athena&nbsp;— это сервис интерактивной аналитики, который облегчает анализ данных, хранящихся в&nbsp;S3;</p></li>
     </ul></li>
    <li><p>NATS&nbsp;— брокер сообщений для&nbsp;взаимодействия SSP с&nbsp;сервисами.</p></li>
    <li><p>Terraform&nbsp;— скрипт для&nbsp;управления инфраструктурой.</p></li>
   </ul>
   <figure class="full-width ">
    <img src="https://habrastorage.org/r/w780q1/getpro/habr/upload_files/cfe/411/b9e/cfe411b9e19d34298d698c86b4de0906.jpeg" width="1080" height="1081" data-src="https://habrastorage.org/getpro/habr/upload_files/cfe/411/b9e/cfe411b9e19d34298d698c86b4de0906.jpeg" data-blurred="true">
    <figcaption></figcaption>
   </figure>
   <h3>Нужно было настроить мониторинг и сбор логов, а также справляться с высокой нагрузкой</h3>
   <p>Готовясь начать работу над системой, мы наметили такой список задач:</p>
   <ul>
    <li><p>интегрировать несколько DSP, с&nbsp;которыми есть договор;</p></li>
    <li><p>проверить Prebid.js адаптер на&nbsp;соответствие требованиям для&nbsp;публикации в&nbsp;официальном репозитории. Обновить адаптер для&nbsp;совместимости с&nbsp;новой версией Prebid.js;</p></li>
    <li><p>добавить мониторинг. Так как&nbsp;система работает с&nbsp;множеством сервисов и выход из&nbsp;строя одного из&nbsp;них может привести к&nbsp;неработоспособности всей системы, необходимо отслеживать состояние каждого сервиса;</p></li>
    <li><p>настроить алерты. Необходимо определить для&nbsp;каждого сервиса метрики на&nbsp;основании которых следует уведомить команду о&nbsp;неполадках на&nbsp;сервисе или&nbsp;о&nbsp;необходимости что‑то сделать (например, если заканчивается место в&nbsp;базе данных);</p></li>
    <li><p>настроить сбор логов от&nbsp;приложения. В&nbsp;случае неполадок в&nbsp;работе системы необходимо сохранить информацию о&nbsp;том, где произошла ошибка. Также необходимо проинформировать команду о&nbsp;резком увеличении ошибок в&nbsp;системе;</p></li>
    <li><p>одно из&nbsp;требований клиента к&nbsp;системе, чтобы SSP могла справляться с&nbsp;нагрузкой 5000&nbsp;запросов в&nbsp;секунду (QPS).</p></li>
   </ul>
   <h3>Побороли задержки, оптимизировали затраты&nbsp;</h3>
   <p>В&nbsp;ходе работы над проектом мы:</p>
   <ul>
    <li><p>изучили скрипты Terraform и пересоздали компоненты, которые&nbsp;были созданы руками, чтобы вся инфраструктура поднималась централизованно и применялась практика инфраструктура как&nbsp;код (IaC&nbsp;— Infrastructure as Code). Структурировали репозиторий со скриптами и выделили отдельные модули и вынесли параметры, необходимые для&nbsp;повторного развертывания кластера в&nbsp;производство. Запустили pipeline для&nbsp;развертывания в&nbsp;производство;</p></li>
    <li><p>обновили все компоненты системы (Kubernetes кластер, helm chart‑ы, RDS DB);</p></li>
    <li><p>изучили компоненты SSP и выделили из&nbsp;них те, на&nbsp;работу которых уходило много времени, что&nbsp;в&nbsp;случае высокой нагрузки могло блокировать цикл событий. Для&nbsp;анализа использовали профилирование и инструмент clinic.js. Выяснили, что&nbsp;компонент определения устройств, с&nbsp;которых пришел запрос на&nbsp;рекламу в&nbsp;SSP, занимает основное время жизни запроса и решили вынести компонент в&nbsp;отдельный пул воркеров, чтобы уменьшить время блокировки цикла событий.<br><br>После увеличения трафика начался рост задержек на&nbsp;обработку запроса (Latency). Для&nbsp;исследований задержек в&nbsp;цикле событий воспользовались (perf_hooks.monitorEventLoopDelay). Чтобы не&nbsp;обрабатывать запросы, когда задержки в&nbsp;цикле событий превышают заданное значение, реализовали Circuit Breaker паттерн. Также при&nbsp;большом объеме трафика замечали рост latency вместе с&nbsp;увеличением времени работы сборщика мусора (GC). Поэкспериментировали с&nbsp;параметрами ‑max‑semi‑space‑size=(32|64|128), с&nbsp;параметрами CPU, Memory для&nbsp;одного экземпляра приложения (pod‑а). В&nbsp;итоге, после нескольких итераций подбора значений, удалось добиться приемлемого уровня latency.</p><p>SSP должна хранить соответствие пользователей SSP и DSP, для&nbsp;этого использовалась PostgreSQL. После увеличения трафика PostgreSQL долго обрабатывала запрос, а&nbsp;так как&nbsp;у&nbsp;нас&nbsp;был опыт работы с&nbsp;Aerospike в&nbsp;AdTech проектах, то мы заменили PostgreSQL на&nbsp;Aerospike. После этого время поиска пользователя уменьшилось;</p></li>
   </ul>
   <figure class="full-width ">
    <img src="https://habrastorage.org/r/w780q1/getpro/habr/upload_files/fe7/0f5/160/fe70f5160d1ae996deea77d3ed6fd6e4.jpeg" width="1238" height="931" data-src="https://habrastorage.org/getpro/habr/upload_files/fe7/0f5/160/fe70f5160d1ae996deea77d3ed6fd6e4.jpeg" data-blurred="true">
    <figcaption></figcaption>
   </figure>
   <ul>
    <li><p>получили документацию от&nbsp;DSP, с&nbsp;которыми&nbsp;было необходимо интегрироваться. Подготовили модули для&nbsp;каждой из&nbsp;них.</p><p>Чтобы считать, что&nbsp;интеграция с&nbsp;DSP завершена успешно, необходимо:</p></li>
    <li><p>убедиться что&nbsp;запрос содержит все обязательные поля для&nbsp;этой DSP;</p></li>
    <li><p>убедиться что&nbsp;механизм синхронизации информации о&nbsp;пользователе работает;</p></li>
    <li><p>убедиться в&nbsp;получении ставок. Для&nbsp;этого нужно отправить реальный трафик. У&nbsp;нашего клиента&nbsp;была возможность отправлять 1% трафика с&nbsp;нескольких подконтрольных сайтов;</p></li>
    <li><p>сформировать отчеты о&nbsp;показе рекламы и ее стоимости за&nbsp;несколько дней (по дням) и сравнить с&nbsp;отчетами DSP. Несоответствие количества показов и их стоимости между отчетами не&nbsp;должны превышать 5%;</p></li>
    <li><p>в&nbsp;зависимости от&nbsp;DSP, включить сжатие (gzip) трафика между системами, чтобы снизить его стоимость;</p></li>
    <li><p>если DSP не&nbsp;поддерживает в&nbsp;одном запросе нескольких рекламных блоков, отправить несколько параллельных запросов к&nbsp;DSP и объединить результат;</p></li>
   </ul>
   <figure class="full-width ">
    <img src="https://habrastorage.org/r/w780q1/getpro/habr/upload_files/a17/00d/ec2/a1700dec2adc2ed55ea029586595e292.jpeg" width="1080" height="1081" data-src="https://habrastorage.org/getpro/habr/upload_files/a17/00d/ec2/a1700dec2adc2ed55ea029586595e292.jpeg" data-blurred="true">
    <figcaption></figcaption>
   </figure>
   <ul>
    <li><p>в&nbsp;качестве системы мониторинга в&nbsp;кластер Kubernetes установили Prometheus, а&nbsp;для&nbsp;отображения данных в&nbsp;графическом виде установили Grafana. Добавили дашборды для&nbsp;мониторинга Kubernetes кластера (количество node, pod, CPU, Memory), Node JS приложения. Создали информационные панели для&nbsp;бизнес метрик заказчика: общее количество показов и доход за&nbsp;выбранный интервал, количество показов и доход по&nbsp;DSP, CPM (стоимость 1000&nbsp;показов рекламы) по&nbsp;DSP, процент ошибок от&nbsp;общего количества запросов по&nbsp;DSP, общее количество запросов на&nbsp;рекламу к&nbsp;SSP;</p></li>
    <li><p>проанализировали AWS Glue ETL скрипты для&nbsp;сбора данных. Скрипты написаны на&nbsp;Python (PySpark) и занимаются агрегацией данных за&nbsp;предыдущий час работы SSP. Данные хранятся в&nbsp;S3&nbsp;и записываются туда при&nbsp;помощи Kinesis Firehose. При&nbsp;увеличении трафика выяснилось, что&nbsp;скрипты работают долго и тратят много ресурсов. Перевод ETL скриптов на&nbsp;новую версию Glue 3&nbsp;позволил сократить время на&nbsp;запуск ETL скрипта. Информация о&nbsp;ставках и информация о&nbsp;показах хранились в&nbsp;разной схеме данных. Это приводило к&nbsp;объединению больших объемов агрегированных данных и увеличивало время выполнения скриптов. Когда мы создали общую структуру показов ставок и переработали ETL скрипты, то сократили время работы, это привело к&nbsp;снижению затрат на&nbsp;использование сервиса;</p></li>
    <li><p>так как&nbsp;SSP‑система должна обрабатывать большое количество запросов на&nbsp;рекламу, которое зависит от&nbsp;времени суток, нам предстояло внедрить автомасштабирование (горизонтальное масштабирование) виртуальных машин (Node) и SSP приложения (Pod). Для&nbsp;этого в&nbsp;кластере Kubernetes&nbsp;были созданы Node Group‑ы которые позволили задавать тип виртуальных машин. Для&nbsp;сервисов, которые требуют большого объема ресурсов,&nbsp;были выбраны типы Node помощнее и настроено горизонтальное масштабирование, на&nbsp;основе загрузки CPU. Для&nbsp;оптимизации затрат привлекли Spot‑инстансы (это привлечение свободных в&nbsp;данный момент ресурсов на&nbsp;AWS со скидкой). Для&nbsp;мониторинга масштабирования и количества привлеченных Spot‑инстансов добавили информационные панели в&nbsp;Grafana;</p></li>
    <li><p>для&nbsp;сбора и хранения логов воспользовались сервисом AWS OpenSearch. Клиента устроила стоимость и простота его запуска;</p></li>
    <li><p>так как&nbsp;в&nbsp;проекте Node JS библиотеки периодически обновляются и добавляются, то необходимо проверять установленные пакеты на&nbsp;предмет безопасности. Для&nbsp;сканирования зависимостей на&nbsp;уязвимости использовали Owasp плагин для&nbsp;CircleCI;</p></li>
    <li><p>составили документ о&nbsp;всех параметрах системы, которые нужно контролировать и оповещать команду в&nbsp;случае их выхода из&nbsp;заданных пределов. Настроили оповещение в&nbsp;PagerDuty (это платформа для&nbsp;обработки инцидентов, помогает настраивать порядок дежурств и оповещения дежурных инженеров);</p></li>
    <li><p>для&nbsp;тестирования системы и анализа метрик под&nbsp;нагрузкой внедрили практики нагрузочного тестирования. Для&nbsp;этого воспользовались инструментом jMeter. При&nbsp;помощи Ansible скриптов создается заданное количество виртуальных машин на&nbsp;сервисе (EC2), с&nbsp;них генерируется нагрузка на&nbsp;SSP.<br></p></li>
   </ul>
   <figure class="full-width ">
    <img src="https://habrastorage.org/r/w1560/getpro/habr/upload_files/1e7/5a0/a0b/1e75a0a0bc2ca09e49d528752faa0e5c.png" width="1412" height="1600" data-src="https://habrastorage.org/getpro/habr/upload_files/1e7/5a0/a0b/1e75a0a0bc2ca09e49d528752faa0e5c.png">
    <figcaption></figcaption>
   </figure>
   <h3>Что проект дал нашей команде</h3>
   <p>Проект принес опыт использования Node JS в&nbsp;условиях высоких нагрузок. Дал опыт работы с&nbsp;Kubernetes кластером. Информационные панели в&nbsp;Grafana + Kibana логи позволили&nbsp;быстро отслеживать аномалии в&nbsp;работе системы. В&nbsp;проекте наладили процесс автоматизированной поставки от&nbsp;разработки до&nbsp;эксплуатации. Это написание кода, автоматические тесты, развертывание в&nbsp;промежуточной среде, развертывание в&nbsp;рабочей среде</p>
   <h3>Куда система будет развиваться дальше&nbsp;</h3>
   <p>После того как&nbsp;мы вышли в&nbsp;производство, сосредоточились на&nbsp;добавление новых функций:</p>
   <ul>
    <li><p>поддержка видео‑рекламы (до этого система работала только с&nbsp;баннерной рекламой);</p></li>
    <li><p>возможность интегрироваться с&nbsp;SSP через JSTag (для клиентов, которые не&nbsp;используют Prebid.js, подготавливается специальный javascript код);</p></li>
    <li><p>возможность интегрироваться с&nbsp;SSP как&nbsp;с&nbsp;партнером по&nbsp;OpenRTB протоколу (дать возможность другим SSP использовать SSP в&nbsp;качестве DSP);</p></li>
    <li><p>поддержка PMP deals (это необходимо для&nbsp;поддержки аукционов закрытого типа);</p></li>
    <li><p>добавить механизм ограничение количества входящих запросов (так как&nbsp;некоторые DSP имеют ограничения на&nbsp;количество принимаемых запросов, то следует применить throttling запросов);</p></li>
   </ul>
   <p>Дальнейшие планы подключить клиентов и продолжать интегрировать больше DSP. А&nbsp;также вывести систему на&nbsp;стабильный доход.</p>
   <p>А&nbsp;сейчас пройдемте в&nbsp;комментарии обсуждать наш кейс и делиться опытом, если вы тоже работали с&nbsp;SSP‑системами.</p>
   <p></p>
  </div>
 </div>
</div> <!----> <!---->