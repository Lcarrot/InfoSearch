<div>
 <div class="article-formatted-body article-formatted-body article-formatted-body_version-1">
  <div xmlns="http://www.w3.org/1999/xhtml">
   <h1 id="shablon-backend-servera-na-golang--chast-3-docker-docker-compose-kubernetes-kustomizehttpshabrcomrupost716634"><a href="https://habr.com/ru/post/716634/">Шаблон backend сервера на Golang — часть 3 (Docker, Docker Compose, Kubernetes (kustomize)</a></h1><br>
   <p><img src="https://habrastorage.org/r/w780q1/getpro/habr/post_images/110/b29/b5b/110b29b5bc2d9df9e250549adb05268d.jpg" alt="Схема развертывания в Kubernetes" data-src="https://habrastorage.org/getpro/habr/post_images/110/b29/b5b/110b29b5bc2d9df9e250549adb05268d.jpg" data-blurred="true"></p><br>
   <p><a href="https://habr.com/ru/post/492062/">Первая часть</a> шаблона была посвящена HTTP серверу.</p><br>
   <p><a href="https://habr.com/ru/post/500554/">Вторая часть</a> шаблона была посвящена прототипированию REST API.</p><br>
   <p><a href="https://habr.com/ru/post/716634/">Третья часть</a> посвящена развертыванию шаблона в Docker, Docker Compose, Kubernetes (kustomize).</p><br>
   <p>Четвертая часть будет посвящена развертыванию в Kubernetes с Helm chart и настройке Horizontal Autoscaler.</p><br>
   <p>Для корректного развертывания в Kubernetes, в шаблон пришлось внести изменения:</p><br>
   <ul>
    <li>способа конфигурирования — YAML, ENV, <a href="https://kustomize.io/" rel="nofollow noopener noreferrer">Kustomize</a></li>
    <li>подхода к логированию — переход на <a href="https://github.com/uber-go/zap" rel="nofollow noopener noreferrer">zap</a></li>
    <li>способа развертывания схемы БД — переход на <a href="https://www.liquibase.org/" rel="nofollow noopener noreferrer">liquibase</a></li>
    <li>добавление метрик <a href="https://prometheus.io/" rel="nofollow noopener noreferrer">prometheus</a></li>
   </ul><br>
   <p>Ссылка на новый <a href="https://github.com/romapres2010/goapp" rel="nofollow noopener noreferrer">репозиторий</a>.</p><br>
   <p>Шаблон goapp в репозитории полностью готов к развертыванию в Docker, Docker Compose, Kubernetes (kustomize), Kubernetes (helm).</p><br>
   <p><em>Настоящая статья не содержит детального описание используемых технологий</em></p><br>
   <h2 id="soderzhanie">Содержание</h2><br>
   <ol>
    <li>Изменение подхода к конфигурированию</li>
    <li>Добавление метрик <a href="https://prometheus.io/" rel="nofollow noopener noreferrer">prometheus</a></li>
    <li>Изменение подхода к логированию</li>
    <li>Развертывание схемы БД</li>
    <li>Сборка Docker image</li>
    <li>Сборка Docker-Compose</li>
    <li>Схема развертывания в Kubernetes</li>
    <li>Подготовка YAML для Kubernetes</li>
    <li>Kustomization YAML для Kubernetes</li>
    <li>Тестирование Kubernetes с kustomize</li>
   </ol><a name="habracut"></a><br>
   <h2 id="1-izmenenie-podhoda-k-konfigurirovaniyu">1. Изменение подхода к конфигурированию</h2><br>
   <p>Первоначальный вариант запуска приложения с передачей параметров через командную строку не очень удобен для Docker, так как усложняет работу с ENTRYPOINT.<br> Но для совместимости этот вариант приходится поддерживать.</p><br>
   <p>Идеальным способом конфигурирования для Docker и Kubernetes являются переменные окружения ENV. Их проще всего подменять при переключении между средами DEV-TEST-PROD.<br> В нашем случае количество конфигурационных параметров приложения перевалило за 1000 — поэтому конфиг был разделен на части:</p><br>
   <ul>
    <li>условно постоянная часть, которая меняется редко — вынесена в YAML,</li>
    <li>все настройки, связанные со средами порты, пути, БД — вынесены в ENV,</li>
    <li>чувствительные настройки безопасности (пользователи, пароли) — вынесены в отдельное зашифрованное хранилище и передаются либо через ENV, либо через Secret.</li>
   </ul><br>
   <h3 id="uslovno-postoyannaya-chast-konfiga">Условно постоянная часть конфига</h3><br>
   <p>Пример "базового" YAML конфига <a href="https://github.com/romapres2010/goapp/blob/master/deploy/config/app.global.yaml" rel="nofollow noopener noreferrer">app.global.yaml</a>.</p><br>
   <p>При использовании YAML конфигов в Kubernetes, нужно решить пару задач:</p><br>
   <ul>
    <li>как сформировать YAML конфиг в зависимости от среды на которой будет развернуто приложение (DEV-TEST-PROD)</li>
    <li>как внедрить YAML конфиг в Docker контейнер, развернутый в Pod Kubernetes</li>
   </ul><br>
   <p>Самый простой вариант с формированием YAML конфиг — это держать "базовую" версию и делать от нее "ручные клоны" для каждой из сред развертывания.<br> Самый простой вариант внедрения — это добавить YAML конфиги (всех сред) в сборку Docker образа и переключаться между ними через ENV переменные, в зависимости от среды на которой осуществляется запуск.</p><br>
   <p>Более правильный вариант — при сборке использовать Kustomize и overlays для формирования из базового, YAML конфиг для конкретной среды и деплоить их в репозиторий артефактов, откуда он потом будет выложен в примонтированный каталог к Docker контейнер.</p><br>
   <h3 id="dinamicheskoe-konfigurirovanie-rest-api-entry-point">Динамическое конфигурирование REST API entry point</h3><br>
   <p>Столкнулись с тем, что в разных средах нужно по-разному настраивать REST API entry point.<br> Можно было возложить эту функцию на nginx proxy, но решили задачу через YAML конфиг.</p><br>
   <pre><code class="plaintext">handlers:
    HealthHandler:  # Сервис health - проверка активности HEALTH
        enabled: true                      # Признак включен ли сервис
        application: "app"                 # Приложение к которому относится сервис
        module: "system"                   # Модуль к которому относится сервис
        service: "health"                  # Имя сервиса
        version: "v1.0.0"                  # Версия сервиса
        full_path: "/app/system/health"    # URI сервиса /Application.Module.Service.APIVersion или /Application/APIVersion/Module/Service
        params: ""                         # Параметры сервиса с виде {id:[0-9]+}
        method: "GET"                      # HTTP метод: GET, POST, ...
        handler_name: "HealthHandler"      # Имя функции обработчика
    ReadyHandler:   # Сервис ready - handle to test readinessProbe
        enabled: true                      # Признак включен ли сервис
        application: "app"                 # Приложение к которому относится сервис
        module: "system"                   # Модуль к которому относится сервис
        service: "ready"                   # Имя сервиса
        version: "v1.0.0"                  # Версия сервиса
        full_path: "/app/system/ready"     # URI сервиса /Application.Module.Service.APIVersion или /Application/APIVersion/Module/Service
        params: ""                         # Параметры сервиса с виде {id:[0-9]+}
        method: "GET"                      # HTTP метод: GET, POST, ...
        handler_name: "ReadyHandler"       # Имя функции обработчика</code></pre><br>
   <p>Назначение функции обработчика для REST API entry point выполнено <a href="https://github.com/romapres2010/goapp/blob/master/pkg/common/httpservice/httpservice.go#L193" rel="nofollow noopener noreferrer">с использованием reflect</a>.<br> Весьма удобный побочный эффект такого подхода — возможность переключать альтернативные обработчики без пересборки кода, только меняя конфиг.</p><br>
   <pre><code class="go">var dummyHandlerFunc func(http.ResponseWriter, *http.Request)
for handlerName, handlerCfg := range s.cfg.Handlers {
    if handlerCfg.Enabled { // сервис включен
        handler := Handler{}

        if handlerCfg.Params != "" {
            handler.Path = handlerCfg.FullPath + "/" + handlerCfg.Params
        } else {
            handler.Path = handlerCfg.FullPath
        }
        handler.Method = handlerCfg.Method

        // Определим метод обработчика
        method := reflect.ValueOf(s.httpHandler).MethodByName(handlerCfg.HandlerName)

        // Метод найден
        if method.IsValid() {
            methodInterface := method.Interface()                                         // получил метод в виде интерфейса, для дальнейшего преобразования к нужному типу
            handlerFunc, ok := methodInterface.(func(http.ResponseWriter, *http.Request)) // преобразуем к нужному типу
            if ok {
                handler.HandlerFunc = s.recoverWrap(handlerFunc) // Оборачиваем от паники
                _log.Info("Register HTTP handler: HandlerName, Method, FullPath", handlerCfg.HandlerName, handlerCfg.Method, handlerCfg.FullPath)
            } else {
                return _err.NewTyped(_err.ERR_INCORRECT_TYPE_ERROR, requestID, "New", "func(http.ResponseWriter, *http.Request)", reflect.ValueOf(methodInterface).Type().String(), reflect.ValueOf(dummyHandlerFunc).Type().String()).PrintfError()
            }
        } else {
            return _err.NewTyped(_err.ERR_HTTP_HANDLER_METHOD_NOT_FOUND, requestID, handlerCfg.HandlerName).PrintfError()
        }

        s.Handlers[handlerName] = handler
    }
}</code></pre><br>
   <h2 id="2-dobavlenie-metrik-prometheushttpsprometheusio">2 Добавление метрик <a href="https://prometheus.io/" rel="nofollow noopener noreferrer">prometheus</a></h2><br>
   <p>В шаблон встроена сборка следующих метрик для <a href="https://prometheus.io/" rel="nofollow noopener noreferrer">prometheus</a>:</p><br>
   <ul>
    <li>Метрики DB<br>
     <ul>
      <li><strong>db_total</strong> — The total number of processed DB by sql statement (CounterVec)</li>
      <li><strong>db_duration_ms</strong> — The duration histogram of DB operation in ms by sql statement (HistogramVec)</li>
     </ul></li>
    <li>Метрики HTTP request<br>
     <ul>
      <li><strong>http_requests_total_by_resource</strong> — How many HTTP requests processed, partitioned by resource (CounterVec)</li>
      <li><strong>http_requests_error_total_by_resource</strong> — How many HTTP requests was ERRORED, partitioned by resource (CounterVec)</li>
      <li><strong>http_request_duration_ms_by_resource</strong> — The duration histogram of HTTP requests in ms by resource (HistogramVec)</li>
      <li><strong>http_active_requests_count</strong> — The total number of active HTTP requests (Gauge)</li>
      <li><strong>http_request_duration_ms</strong> — The duration histogram of HTTP requests in ms (Histogram)</li>
     </ul></li>
    <li>Метрики HTTP client call<br>
     <ul>
      <li><strong>http_client_call_total_by_resource</strong> — How many HTTP client call processed, partitioned by resource (CounterVec)</li>
      <li><strong>http_client_call_duration_ms_by_resource</strong> — The duration histogram of HTTP client call in ms by resource (HistogramVec)</li>
     </ul></li>
    <li>Метрики WorkerPool<br>
     <ul>
      <li><strong>wp_task_queue_buffer_len_vec</strong> — The len of the worker pool buffer (GaugeVec)</li>
      <li><strong>wp_add_task_wait_count_vec</strong> — The number of the task waiting to add to worker pool queue (GaugeVec)</li>
      <li><strong>wp_worker_process_count_vec</strong> — The number of the working worker (GaugeVec)</li>
     </ul></li>
   </ul><br>
   <p>Доступ к метрикам настроен по стандартному пути HTTP GET <a href="http://127.0.0.1:3000/metrics" rel="nofollow noopener noreferrer">/metrics</a>.</p><br>
   <p>Включение / выключение метрик настраивается через YAML конфиг</p><br>
   <pre><code class="plaintext"># конфигурация сбора метрик
metrics:
    metrics_namespace: com
    metrics_subsystem: go_app
    collect_db_count_vec: true
    collect_db_duration_vec: false
    collect_http_requests_count_vec: true
    collect_http_error_requests_count_vec: true
    collect_http_requests_duration_vec: true
    collect_http_active_requests_count: true
    collect_http_requests_duration: true
    collect_http_client_call_count_vec: true
    collect_http_client_call_duration_vec: true
    collect_wp_task_queue_buffer_len_vec: true
    collect_wp_add_task_wait_count_vec: true
    collect_wp_worker_process_count_vec: true</code></pre><br>
   <h2 id="3-izmenenie-podhoda-k-logirovaniyu">3. Изменение подхода к логированию</h2><br>
   <p>Логирование приложения в Kubernetes можно реализовать несколькими способами:</p><br>
   <ul>
    <li>вывод в stdout, stderr — этот способ является стандартным, но нужно учитывать, что в Kubernetes stdout, stderr Docker контейнеров перенаправляются в файлы, которые имеют ограниченный размер и могут перезаписываться.</li>
    <li>вывод в файл на примонтированный к Docker контейнеру внешний ресурс.</li>
    <li>вывод в структурном виде во внешний сервис, например, Kafka.</li>
   </ul><br>
   <p>Всем этим требованиям отлично удовлетворяет библиотека <a href="https://github.com/uber-go/zap" rel="nofollow noopener noreferrer">zap</a>. Она также позволяет настроить несколько параллельных логгеров (ZapCore), которые будут писать в различные приемники.</p><br>
   <p>В пакете <a href="https://github.com/romapres2010/goapp/tree/master/pkg/common/logger" rel="nofollow noopener noreferrer">logger</a> реализована возможность настройки нескольких ZapCore через простой YAML конфиг.</p><br>
   <p>Так же в пакете <a href="https://github.com/romapres2010/goapp/tree/master/pkg/common/logger" rel="nofollow noopener noreferrer">logger</a> добавлена интеграция <a href="https://github.com/uber-go/zap" rel="nofollow noopener noreferrer">zap</a> с библиотекой <a href="https://gopkg.in/natefinch/lumberjack.v2" rel="nofollow noopener noreferrer">lumberjack</a> для управления циклом ротации и архивирования лог файлов.</p><br>
   <p>Любой из ZapCore можно динамически включать/выключать или изменять через HTTP POST: <a href="https://github.com/romapres2010/goapp/blob/master/pkg/common/httpservice/handler_logger_set_config.go" rel="nofollow noopener noreferrer">/system/config/logger</a>.</p><br>
   <p>Например, в следующем конфиге объявлены 4 ZapCore.</p><br>
   <ul>
    <li>все сообщения от DEBUG до INFO выводятся в файл app.debug.log<br>
     <ul>
      <li>максимальный размер лог файла в 10 MB</li>
      <li>время хранения истории лог файлов 7 дней</li>
      <li>максимальное количество архивных логов — 10 без архивирования</li>
     </ul></li>
    <li>все сообщения от ERROR до FATAL выводятся в файл app.error.log</li>
    <li>все сообщения выводятся в stdout</li>
    <li>все сообщения от ERROR до FATAL выводятся в stderr</li>
   </ul><br>
   <pre><code class="plaintext"># конфигурация сервиса логирования
logger:
    enable: true                        # состояние логирования 'true', 'false'
    global_level: INFO                  # debug, info, warn, error, dpanic, panic, fatal - все логгеры ниже этого уровня будут отключены
    global_filename: /app/log/app.log   # глобальное имя файл для логирования
    zap:
        enable: true                # состояние логирования 'true', 'false'
        disable_caller: false       # запретить вывод в лог информации о caller
        disable_stacktrace: false   # запретить вывод stacktrace
        development: false          # режим разработки для уровня dpanic
        stacktrace_level: error     # для какого уровня выводить stacktrace debug, info, warn, error, dpanic, panic, fatal
        core:
            -   enable: true        # состояние логирования 'true', 'false'
                min_level: null     # минимальный уровень логирования debug, info, warn, error, dpanic, panic, fatal
                max_level: INFO     # максимальный уровень логирования debug, info, warn, error, dpanic, panic, fatal
                log_to: lumberjack  # логировать в 'file', 'stderr', 'stdout', 'url', 'lumberjack'
                encoding: "console" # формат вывода 'console', 'json'
                file:
                    filename: ".debug.log"       # имя файл для логирования, если не заполнено, то используется глобальное имя
                    max_size: 10                 # максимальный размер лог файла в MB
                    max_age: 7                   # время хранения истории лог файлов в днях
                    max_backups: 10              # максимальное количество архивных логов
                    local_time: true             # использовать локальное время в имени архивных лог файлов
                    compress: false              # сжимать архивные лог файлы в zip архив
            -   enable: true        # состояние логирования 'true', 'false'
                min_level: ERROR    # минимальный уровень логирования debug, info, warn, error, dpanic, panic, fatal
                max_level: null     # максимальный уровень логирования debug, info, warn, error, dpanic, panic, fatal
                log_to: lumberjack  # логировать в 'file', 'stderr', 'stdout', 'url', 'lumberjack
                encoding: "console" # формат вывода 'console', 'json'
                file:
                    filename: ".error.log"       # имя файл для логирования, если не заполнено, то используется глобальное имя
                    max_size: 10                 # максимальный размер лог файла в MB
                    max_age: 7                   # время хранения истории лог файлов в днях
                    max_backups: 10              # максимальное количество архивных логов
                    local_time: true             # использовать локальное время в имени архивных лог файлов
                    compress: false              # сжимать архивные лог файлы в zip архив
            -   enable: true        # состояние логирования 'true', 'false'
                min_level: null     # минимальный уровень логирования debug, info, warn, error, dpanic, panic, fatal
                max_level: null     # максимальный уровень логирования debug, info, warn, error, dpanic, panic, fatal
                log_to: stdout      # логировать в 'file', 'stderr', 'stdout', 'url', 'lumberjack
                encoding: "console" # формат вывода 'console', 'json'
            -   enable: true        # состояние логирования 'true', 'false'
                min_level: ERROR    # минимальный уровень логирования debug, info, warn, error, dpanic, panic, fatal
                max_level: null     # максимальный уровень логирования debug, info, warn, error, dpanic, panic, fatal
                log_to: stderr      # логировать в 'file', 'stderr', 'stdout', 'url', 'lumberjack
                encoding: "console" # формат вывода 'console', 'json'</code></pre><br>
   <h2 id="4-razvertyvanie-shemy-bd">4. Развертывание схемы БД</h2><br>
   <p>Для первоначального развертывания схемы БД в контейнере и управления изменениями DDL скриптов через Git великолепно подходит <a href="https://www.liquibase.org/" rel="nofollow noopener noreferrer">liquibase</a>.</p><br>
   <p><a href="https://www.liquibase.org/" rel="nofollow noopener noreferrer">Liquibase</a> позволяет самостоятельно генерировать DDL скрипты БД. Для простых случаев — этого вполне хватает, но если требуется тонкая настройка БД (например, настройка партиций, Blob storage) то лучше формировать DDL скрипты в отдельном приложении.<br> Для Postgres отлично подходит TOAD DataModeler. Для Oracle — Oracle SQL Developer Data Modeler. Например, TOAD DataModeler умеет корректно генерировать DDL скрипты для добавления not null столбцов в уже заполненную таблицу.</p><br>
   <p>Весьма полезная функция <a href="https://www.liquibase.org/" rel="nofollow noopener noreferrer">Liquibase</a> — возможность задавать скрипты для отката изменений. Если установка патча на БД прошла неуспешно (сбой добавления столбца в таблицу, нарушение PK, FK), то легко откатиться к предыдущему состоянию.</p><br>
   <p>В шаблон включены примеры <a href="https://github.com/romapres2010/goapp/tree/master/db/liquibase/changelog" rel="nofollow noopener noreferrer">liquibase/changelog</a> и <a href="https://github.com/romapres2010/goapp/tree/master/db/sql" rel="nofollow noopener noreferrer">DDL скрипты</a> для таблиц Country и Currency с минимальным тестовым наполнением.</p><br>
   <p>Так же как и с YAML конфигами, необходимо предусмотреть способ внедрения <a href="https://www.liquibase.org/" rel="nofollow noopener noreferrer">Liquibase</a> Changelog в Docker контейнер Liquibase, развернутый в Job Pod Kubernetes:</p><br>
   <ul>
    <li>скрипты можно встраивать в Liquibase контейнер (не очень хороший вариант, как минимум размер Docker с Liquibase более 300 Мбайт),</li>
    <li>скрипты можно выкладывать в примонтированный каталог к Docker контейнер и на уровне ENV переменных указывать Changelog для запуска.</li>
   </ul><br>
   <p>В шаблоне предусмотрены оба эти варианта.</p><br>
   <h2 id="5-sborka-docker-image">5. Сборка Docker image</h2><br>
   <p>Docker файлы для сборки выложены в каталоге <a href="https://github.com/romapres2010/goapp/tree/master/deploy/docker" rel="nofollow noopener noreferrer">deploy/docker</a>.</p><br>
   <p>Для тестирования сборки можно использовать Docker Desktop — в этом случае не нужно делать push Docker image — они все кешируются на локальной машине.</p><br>
   <h3 id="sborka-docker-image-dlya-go-api">Сборка Docker image для Go Api</h3><br>
   <p>Сборка Docker image для Go Api <a href="https://github.com/romapres2010/goapp/blob/master/deploy/docker/app-api.Dockerfile" rel="nofollow noopener noreferrer">app-api.Dockerfile</a> имеет следующие особенности:</p><br>
   <ul>
    <li>git commit, время сборки и базовая версия приложения передаются через аргументы docker</li>
    <li>создаются точки монтирования для внешних и преднастроенных YAML конфигов и log файлов</li>
    <li>сборка ведется только на локальной копии внешних библиотек ./vendor</li>
    <li>преднастроенные YAML конфиги для различных сред DEV-TEST-PROD можно встроить в сборку и переключаться через ENV переменные</li>
    <li>git commit, время сборки и базовая версия приложения встраиваются в пакет main</li>
    <li>для финальной сборки используется минимальный distroless образ</li>
    <li>точка запуска приложения не содержит параметров — все передается через ENV переменные</li>
   </ul><br>
   <pre><code class="plaintext">##
## Build stage
##

FROM golang:1.19-buster AS build

# git commit, время сборки и базовая версия приложения передаются через аргументы docker
ARG APP_COMMIT
ARG APP_BUILD_TIME
ARG APP_VERSION

WORKDIR /app

# создаются точки монтирования для внешних и преднастроенных YAML конфигов и log файлов
RUN mkdir ./run &amp;&amp; mkdir ./run/defcfg &amp;&amp; mkdir ./run/log &amp;&amp; mkdir ./run/cfg

COPY ./go.mod ./go.mod
COPY ./go.sum ./go.sum
COPY ./pkg ./pkg
COPY ./cmd/app/main.go ./cmd/app/main.go

# сборка ведется только на локальной копии внешних библиотек ./vendor
COPY ./vendor ./vendor

# преднастроенные YAML конфиги для различных сред DEV-TEST-PROD можно встроить в сборку и переключаться через ENV переменные
COPY ./deploy/config/. ./run/defcfg/

# git commit, время сборки и базовая версия приложения встраиваются в пакет main
RUN go build -v -mod vendor -ldflags "-X main.commit=${APP_COMMIT} -X main.buildTime=${APP_BUILD_TIME} -X main.version=${APP_VERSION}" -o ./run/main ./cmd/app/main.go

RUN echo "Based on commit: $APP_COMMIT" &amp;&amp; echo "Build Time: $APP_BUILD_TIME" &amp;&amp; echo "Version: $APP_VERSION"

##
## Deploy stage
##
FROM gcr.io/distroless/base-debian10

WORKDIR /app

COPY --from=build /app/run/. .

EXPOSE 8080/tcp

# точка запуска приложения не содержит параметров - все передается через ENV переменные
ENTRYPOINT [ "/app/main"]</code></pre><br>
   <p>Для ручной сборки Docker выложен тестовый shell скрипт <a href="https://github.com/romapres2010/goapp/blob/master/deploy/docker/app-api-build.sh" rel="nofollow noopener noreferrer">app-api-build.sh</a>:</p><br>
   <ul>
    <li>сборка и запуск всех скриптов всегда осуществляется от корня репозитория</li>
    <li>логирование вывода всех скриптов идет в файлы</li>
    <li>управление версией, именем приложения и docker tag представлено в упрощенном виде через локальные файлы в каталоге <a href="https://github.com/romapres2010/goapp/tree/master/deploy" rel="nofollow noopener noreferrer">deploy</a><br>
     <ul>
      <li>базовая версия приложения ведется в текстовом файле <a href="https://github.com/romapres2010/goapp/blob/master/deploy/version" rel="nofollow noopener noreferrer">version</a>. Для CI/CD через github action дополнительно используется сквозная нумерация, которая добавляется к базовой версии.</li>
      <li>имя приложения ведется в текстовом файле <a href="https://github.com/romapres2010/goapp/blob/master/deploy/app_api_app_name" rel="nofollow noopener noreferrer">app_api_app_name</a>ведется в текстовом файле <a href="https://github.com/romapres2010/goapp/blob/master/deploy/app_api_app_name" rel="nofollow noopener noreferrer">app_api_app_name</a></li>
      <li>имя репозитория для публикации ведется в текстовом файле <a href="https://github.com/romapres2010/goapp/blob/master/deploy/default_repository" rel="nofollow noopener noreferrer">default_repository</a></li>
     </ul></li>
   </ul><br>
   <pre><code class="plaintext">echo "Current directory:"
pwd

export LOG_FILE=$(pwd)/app-api-build.log

echo "Go to working directory:"
pushd ./../../

export APP_VERSION=$(cat ./deploy/version)
export APP_API_APP_NAME=$(cat ./deploy/app_api_app_name)
export APP_REPOSITORY=$(cat ./deploy/default_repository)
export APP_BUILD_TIME=$(date -u '+%Y-%m-%d_%H:%M:%S')
export APP_COMMIT=$(git rev-parse --short HEAD)

docker build --build-arg APP_VERSION=$APP_VERSION --build-arg APP_BUILD_TIME=$APP_BUILD_TIME --build-arg APP_COMMIT=$APP_COMMIT -t $APP_REPOSITORY/$APP_API_APP_NAME:$APP_VERSION -f ./deploy/docker/app-api.Dockerfile . 1&gt;$LOG_FILE 2&gt;&amp;1

echo "Go to current directory:"
popd</code></pre><br>
   <h3 id="sborka-docker-image-dlya-liquibase">Сборка Docker image для liquibase</h3><br>
   <p>Сборка Docker image для Liquibase <a href="https://github.com/romapres2010/goapp/blob/master/deploy/docker/app-liquibase.Dockerfile" rel="nofollow noopener noreferrer">liquibase.Dockerfile</a> имеет следующие особенности:</p><br>
   <ul>
    <li>Changelog для различных сред DEV-TEST-PROD можно встроить в сборку и переключаться через ENV переменные</li>
    <li>DDL и DML скрипты можно встроить в сборку либо использовать внешнюю точку монтирования</li>
   </ul><br>
   <pre><code class="plaintext">##
## Deploy stage
##
FROM liquibase/liquibase:4.9.1

# Changelog для различных сред DEV-TEST-PROD можно встроить в сборку и переключаться через ENV переменные
COPY ./db/liquibase/changelog/. /liquibase/changelog/

# DDL и DML скрипты можно встроить в сборку
COPY ./db/sql /sql
</code></pre><br>
   <h2 id="6-sborka-docker-compose">6. Сборка Docker Compose</h2><br>
   <p>Для локальной сборки удобно использовать Docker Compose. Скрипт сборки <a href="https://github.com/romapres2010/goapp/blob/master/deploy/compose/compose-app.yaml" rel="nofollow noopener noreferrer">compose-app.yaml</a>.</p><br>
   <p>В каталоге <a href="https://github.com/romapres2010/goapp/tree/master/deploy/run/local.compose.global.config" rel="nofollow noopener noreferrer">/deploy/run/local.compose.global.config</a> выложены скрипты для локального тестирования с использованием Docker Compose.</p><br>
   <ul>
    <li>все ENV переменные приложений передаются через внешний <a href="https://github.com/romapres2010/goapp/blob/master/deploy/run/local.compose.global.config/.env" rel="nofollow noopener noreferrer">.env файл</a></li>
    <li>каждый компонент системы (API, БД, UI) можно поднимать независимо, например:<br>
     <ul>
      <li><a href="https://github.com/romapres2010/goapp/blob/master/deploy/run/local.compose.global.config/compose-app-api-db-reload-up.sh" rel="nofollow noopener noreferrer">/deploy/run/local.compose.global.config/compose-app-api-db-reload-up.sh</a> — полностью чистит БД, и пересобирает API</li>
      <li><a href="https://github.com/romapres2010/goapp/blob/master/deploy/run/local.compose.global.config/compose-app-ui-up.sh" rel="nofollow noopener noreferrer">/deploy/run/local.compose.global.config/compose-app-ui-up.sh</a> — переустанавливает только UI</li>
     </ul></li>
   </ul><br>
   <h3 id="docker-compose-dlya-go-api">Docker Compose для Go Api</h3><br>
   <p>Особенности:</p><br>
   <ul>
    <li>при каждом запуске выполняется build</li>
    <li>аргументы в Dockerfile (APP_COMMIT, APP_BUILD_TIME, APP_VERSION) пробрасываются через ENV переменные окружения</li>
    <li>ENV переменные приложения берутся из одноименных ENV переменных окружения (дефолтные значения указаны для примера)</li>
    <li>в Docker монтируются volumеs для конфиг и логов (/app/cfg:ro, /app/log:rw)</li>
    <li>зависимости (depends_on) от БД не выставляются — вместо этого приложение написано так, чтобы оно могло перестартовывать без последствий и указано restart_policy: on-failure</li>
    <li>настроен healthcheck на GET: /app/system/health</li>
   </ul><br>
   <pre><code class="plaintext">  app-api:
    build:
      context: ./../../
      dockerfile: ./deploy/docker/app-api.Dockerfile
      tags:
        - $APP_REPOSITORY/$APP_API_APP_NAME:$APP_VERSION
      args:
        - APP_COMMIT=${APP_COMMIT:-unset}
        - APP_BUILD_TIME=${APP_BUILD_TIME:-unset}
        - APP_VERSION=${APP_VERSION:-unset}
    container_name: app-api
    hostname: app-api-host
    networks:
      - app-net
    ports:
      - $APP_HTTP_OUT_PORT:$APP_HTTP_PORT
      - 8001:$APP_HTTP_PORT
    environment:
      - TZ="Europe/Moscow"
      - APP_CONFIG_FILE=${APP_CONFIG_FILE:-/app/defcfg/app.global.yaml}
      - APP_HTTP_LISTEN_SPEC=${APP_HTTP_LISTEN_SPEC:-0.0.0.0:8080}
      - APP_LOG_LEVEL=${APP_LOG_LEVEL:-ERROR}
      - APP_LOG_FILE=${APP_LOG_FILE:-/app/log/app.log}
      - APP_PG_USER=${APP_PG_USER:-postgres}
      - APP_PG_PASS=${APP_PG_PASS:?database password not set}
      - APP_PG_HOST=${APP_PG_HOST:-app-db-host}
      - APP_PG_PORT=${APP_PG_PORT:-5432}
      - APP_PG_DBNAME=${APP_PG_DBNAME:-postgres}
    volumes:
      - "./../../../app_volumes/cfg:/app/cfg:ro"
      - "./../../../app_volumes/log:/app/log:rw"
    deploy:
      restart_policy:
        condition: on-failure
    healthcheck:
      test: ["curl -f 0.0.0.0:8080/app/system/health"]
      interval: 10s
      timeout: 5s
      retries: 5</code></pre><br>
   <h3 id="docker-compose-dlya-liquibase">Docker Compose для liquibase</h3><br>
   <p>Особенности:</p><br>
   <ul>
    <li>при необходимости тестирования сборки можно примонтировать дополнительный Volume для логов (/liquibase/mylog:rw)</li>
    <li>устанавливается зависимость от БД (depends_on condition: service_healthy)</li>
    <li>при необходимости в Docker монтируются volumеs для changelog и sql (/liquibase/sql:rw, /liquibase/changelog:rw)</li>
    <li>changelog для запуска передается через ENV переменные</li>
   </ul><br>
   <pre><code class="plaintext">  app-liquibase:
    build:
      context: ./../../
      dockerfile: ./deploy/docker/app-liquibase.Dockerfile
      tags:
        - $APP_REPOSITORY/$APP_LUQUIBASE_APP_NAME:$APP_VERSION
    container_name: app-liquibase
    depends_on:
      app-db:
        condition: service_healthy
    networks:
      - app-net
#    volumes:
#      - "./../../../app_volumes/log:/liquibase/mylog:rw"
#      - "./../../../app_volumes/sql:/liquibase/sql:rw"
#      - "./../../../app_volumes/log:/liquibase/changelog:rw"
#    command: --changelog-file=./changelog/$APP_PG_CHANGELOG --url="jdbc:postgresql://$APP_PG_HOST:$APP_PG_PORT/$APP_PG_DBNAME" --username=$APP_PG_USER --password=$APP_PG_PASS --logFile="mylog/liquibase.log" --logLevel=info update
    command: --changelog-file=./changelog/$APP_PG_CHANGELOG --url="jdbc:postgresql://$APP_PG_HOST:$APP_PG_PORT/$APP_PG_DBNAME" --username=$APP_PG_USER --password=$APP_PG_PASS --logLevel=info update</code></pre><br>
   <h3 id="docker-compose-dlya-bd">Docker Compose для БД</h3><br>
   <p>Особенности:</p><br>
   <ul>
    <li>в Docker монтируются volumе для БД (/var/lib/postgresql/data:rw)</li>
    <li>настроен healthcheck на "CMD-SHELL", "pg_isready"</li>
   </ul><br>
   <pre><code class="plaintext">  app-db:
    image: postgres:14.5-alpine
    container_name: app-db
    hostname: app-db-host
    environment:
      - POSTGRES_PASSWORD=${APP_PG_PASS:?database password not set}
      - PGUSER=${APP_PG_USER:?database user not set}
    networks:
      - app-net
    ports:
      - $APP_PG_OUT_PORT:$APP_PG_PORT
    volumes:
      - "./../../../app_volumes/db:/var/lib/postgresql/data:rw"
    healthcheck:
      test: ["CMD-SHELL", "pg_isready"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 10s
    restart: unless-stopped</code></pre><br>
   <h2 id="7-shema-razvertyvaniya-v-kubernetes">7. Схема развертывания в Kubernetes</h2><br>
   <p>Схема развертывания в <a href="https://kubernetes.io/ru/docs/tutorials/kubernetes-basics/" rel="nofollow noopener noreferrer">Kubernetes</a> приведена в сокращенном виде, исключены компоненты, связанные с UI (Jmix) и распределенным кэшем (hazelcast).</p><br>
   <p><img src="https://habrastorage.org/r/w780q1/getpro/habr/post_images/110/b29/b5b/110b29b5bc2d9df9e250549adb05268d.jpg" alt="Схема развертывания в Kubernetes" data-src="https://habrastorage.org/getpro/habr/post_images/110/b29/b5b/110b29b5bc2d9df9e250549adb05268d.jpg" data-blurred="true"></p><br>
   <p>Основным элементом развертывания в Kubernetes является <a href="https://kubernetes.io/docs/concepts/workloads/pods/" rel="nofollow noopener noreferrer">Pod</a>.<br> Для простоты — это группа контейнеров с общими ресурсами, которая изолирована от остальных Pod.</p><br>
   <ul>
    <li>Обычно, в Pod размещается один app контейнер, и, при необходимости, init контейнер.</li>
    <li>Остановить app контейнер в Pod нельзя, либо он завершит работу сам, либо нужно удалить Pod (руками или через Deployment уменьшив количество replicas дo 0).</li>
    <li>Pod размещается на определенном узле кластера Kubernetes</li>
    <li>Для Pod задается политика автоматического рестарта в рамках одного Node <a href="https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle/#restart-policy" rel="nofollow noopener noreferrer">restartPolicy</a>: Always, OnFailure, Never.</li>
    <li>Остановкой и запуском Pod управляет Kubernetes. В общем случае, Pod может быть автоматически остановлен если:<br>
     <ul>
      <li>все контейнеры в Pod завершили работу (успешно или ошибочно)</li>
      <li>нарушены liveness probe для контейнера</li>
      <li>нарушены limits.memory для контейнера</li>
      <li>Pod больше не нужен, например, если уменьшилась нагрузка и Horizontal Autoscaler уменьшил количество replicas</li>
     </ul></li>
    <li>При <a href="https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle/#pod-termination" rel="nofollow noopener noreferrer">остановке Pod</a>:<br>
     <ul>
      <li>в контейнеры будет отправлен STOPSIGNAL</li>
      <li>по прошествии terminationGracePeriodSeconds процессы контейнеров будут удалены</li>
      <li>при удалении Pod и следующем создании, он может быть создан на другом узле кластера, с другими IP адресами и портами и к нему могут быть примонтированы новые Volume</li>
     </ul></li>
   </ul><br>
   <p>При разработке stateless приложений для Kubernetes нужно учитывать эту специфику:</p><br>
   <ul>
    <li>приложение может быть остановлено в любой момент<br>
     <ul>
      <li>необходимо предусмотреть возможность "чистой" остановки в течении grace period (закрыть подключения к БД, завершить (или нет) текущие задачи, закрыть HTTP соединения, ...)</li>
      <li>необходимо предусмотреть возможность "жесткой" остановки, если приложение взаимодействие со stateful сервисами, то предусмотреть компенсирующие воздействия при повторном запуске</li>
      <li>желательно контролировать лимит по памяти для приложения</li>
     </ul></li>
    <li>при запуске/перезапуске приложения все данные в локальной файловой системе контейнера потенциально могут быть потеряны</li>
    <li>при запуске/перезапуске приложения данные на примонтированных Volume потенциально могут быть так же потеряны</li>
    <li>при остановке приложения доступ к stdout и stderr получить можно, но есть ограничения — критические логи желательно сразу отправлять во внешний сервис или на Volume, который точно не будет удален</li>
    <li>в Kubernetes нет возможности явно задать последовательность старта различных Pod<br>
     <ul>
      <li>приложение может потенциально запущено раньше других связанных сервисов</li>
      <li>желательно предусмотреть вариант постоянного (или лимитированного по количеству раз) перезапуска приложения в ожидании готовности внешних сервисов</li>
      <li>желательно предусмотреть в связанных приложениях liveness/readiness probe, чтобы понять, когда связанное приложение готово к работе</li>
     </ul></li>
   </ul><br>
   <h2 id="8-podgotovka-yaml-dlya-kubernetes">8. Подготовка YAML для Kubernetes</h2><br>
   <h3 id="konfigurirovanie">Конфигурирование</h3><br>
   <p>В Kubernetes предусмотрено для основных способа конфигурирования <a href="https://kubernetes.io/docs/concepts/configuration/configmap/" rel="nofollow noopener noreferrer">ConfigMaps</a> для основных настроек и <a href="https://kubernetes.io/docs/concepts/configuration/secret/" rel="nofollow noopener noreferrer">Secrets</a> для настроек, чувствительных с точки зрения безопасности.</p><br>
   <p>В простейшем случае <a href="https://github.com/romapres2010/goapp/blob/master/deploy/kubernates/base/app-configmap.yaml" rel="nofollow noopener noreferrer">/deploy/kubernates/base/app-configmap.yaml</a>, содержит переменные со строковыми значениями. В более сложных вариантах, можно считывать и разбирать конфиги из внешнего файла или использовать внешний файл без "разбора".</p><br>
   <p>Все артефакты Kubernetes желательно сразу создавать в отдельном namespace <a href="https://github.com/romapres2010/goapp/blob/master/deploy/kubernates/base/app-namespase.yaml" rel="nofollow noopener noreferrer">/deploy/kubernates/base/app-namespase.yaml</a>.</p><br>
   <p>Для каждого артефакта нужно указывать метки для дальнейшего поиска и фильтрации. В примере приведена одна метка с именем 'app' и значением 'app'.</p><br>
   <pre><code class="plaintext">apiVersion: v1
kind: ConfigMap
metadata:
  name: app-config
  labels:
    app: app
  namespace: go-app
data:
  APP_CONFIG_FILE: /app/defcfg/app.global.yaml
  APP_HTTP_PORT: "8080"
  APP_HTTP_LISTEN_SPEC: 0.0.0.0:8080
  APP_LOG_LEVEL: ERROR
  APP_LOG_FILE: /app/log/app.log
  APP_PG_HOST: dev-app-db
  APP_PG_PORT: "5432"
  APP_PG_DBNAME: postgres
  APP_PG_CHANGELOG: db.changelog-1.0_recreate_testdatamdg.xml   </code></pre><br>
   <p>"Секретные" конфигурационные данные определяются в <a href="https://github.com/romapres2010/goapp/blob/master/deploy/kubernates/base/app-secret.yaml" rel="nofollow noopener noreferrer">/deploy/kubernates/base/app-secret.yaml</a>.<br> Данный вариант максимально упрощенный — правильно использовать внешнее зашифрованное хранилище.</p><br>
   <pre><code class="plaintext">apiVersion: v1
kind: Secret
metadata:
  name: app-secret
  labels:
    app: app
  namespace: go-app
type: Opaque
stringData:
  APP_PG_USER: postgres
  APP_PG_PASS: postgres</code></pre><br>
   <h3 id="bd">БД</h3><br>
   <p>Развертывание БД в шаблоне приведено в упрощенном stateless варианте (может быть использовано только для dev).<br> Правильный вариант для промышленной эксплуатации — развертывание через оператор с поддержкой кластеризации, например, <a href="https://github.com/zalando/postgres-operator" rel="nofollow noopener noreferrer">zalando</a>.</p><br>
   <p>Прежде всего, нужно запросить ресурсы для <a href="https://kubernetes.io/docs/concepts/storage/persistent-volumes/" rel="nofollow noopener noreferrer">Persistent Volumes</a> на котором будет располагаться файлы с БД.</p><br>
   <p>Если PersistentVolumeClaim удаляется, то выделенный Persistent Volumes в зависимости от persistentVolumeReclaimPolicy, будет удален, очищен или сохранен.</p><br>
   <p><em>Хорошая статья с описанием <a href="https://serveradmin.ru/hranilishha-dannyh-persistent-volumes-v-kubernetes/" rel="nofollow noopener noreferrer">хранилищ данных (Persistent Volumes) в Kubernetes</a></em></p><br>
   <pre><code class="plaintext">apiVersion: v1
kind: PersistentVolumeClaim
metadata:
    name: app-db-claim
    labels:
        app: app
    namespace: go-app
spec:
    accessModes:
        - ReadWriteOnce
    resources:
        requests:
            storage: 100Mi</code></pre><br>
   <p>Управлять созданием Pod удобнее через <a href="https://kubernetes.io/docs/concepts/workloads/controllers/deployment/" rel="nofollow noopener noreferrer">Deployments</a>, который задает шаблон по которому будут создаваться Pod.<br> Особенности развертывания <a href="https://github.com/romapres2010/goapp/blob/master/deploy/kubernates/base/app-db-deployment.yaml" rel="nofollow noopener noreferrer">/deploy/kubernates/base/app-db-deployment.yaml</a>:</p><br>
   <ul>
    <li>количество replicas: 1 — создавать несколько экземпляров БД таким образом можно, но физически у каждой БД будут свои файлы.</li>
    <li>template.metadata.labels задана дополнительная метка tier: app-db, чтобы можно было легко найти Pod БД</li>
    <li>ENV переменные заполняются из ранее созданных ConfigMap и Secret</li>
    <li>определены readinessProbe и livenessProbe</li>
    <li>resources.limits для БД не указываются</li>
    <li>к /var/lib/postgresql/data примонтирован volume, полученный через ранее определенный persistentVolumeClaim.claimName: app-db-claim</li>
   </ul><br>
   <pre><code class="plaintext">apiVersion: apps/v1
kind: Deployment
metadata:
    name: app-db
    labels:
        tier: app-db
    namespace: go-app
spec:
    replicas: 1
    selector:
        matchLabels:
            tier: app-db
    strategy:
        type: Recreate
    template:
        metadata:
            labels:
                app_net: "true"
                tier: app-db
        spec:
            containers:
                - name: app-db
                  env:
                    - name: POSTGRES_PASSWORD
                      valueFrom:
                        secretKeyRef:
                          name: app-secret
                          key: APP_PG_PASS
                    - name: POSTGRES_DB
                      valueFrom:
                        configMapKeyRef:
                          name: app-config
                          key: APP_PG_DBNAME
                    - name: POSTGRES_USER
                      valueFrom:
                        secretKeyRef:
                          name: app-secret
                          key: APP_PG_USER
                    - name: PGUSER
                      valueFrom:
                        secretKeyRef:
                          name: app-secret
                          key: APP_PG_USER
                  image: postgres:14.5-alpine
                  imagePullPolicy: IfNotPresent
                  readinessProbe:
                    exec:
                      command:
                        - pg_isready
                    initialDelaySeconds: 30  # Time to create a new DB
                    failureThreshold: 5
                    periodSeconds: 10
                    timeoutSeconds: 5
                  livenessProbe:
                    exec:
                      command:
                        - pg_isready
                    failureThreshold: 5
                    periodSeconds: 10
                    timeoutSeconds: 5
                  ports:
                    - containerPort: 5432
                  volumeMounts:
                    - mountPath: /var/lib/postgresql/data
                      name: app-db-volume
            hostname: app-db-host
            restartPolicy: Always
            volumes:
              - name: app-db-volume
                persistentVolumeClaim:
                  claimName: app-db-claim</code></pre><br>
   <p>Если доступ к БД нужен вне кластера, то можно определить <a href="https://kubernetes.io/docs/concepts/services-networking/service/" rel="nofollow noopener noreferrer">Service</a> с типом NodePort — <a href="https://github.com/romapres2010/goapp/blob/master/deploy/kubernates/base/app-db-service.yaml" rel="nofollow noopener noreferrer">/deploy/kubernates/base/app-db-service.yaml</a>.</p><br>
   <ul>
    <li>в этом случае на каждом узле кластера, где развернут Pod с БД будет открыт "рандомный" порт, на который будет смаплен порт 5432 из Docker контейнера с БД</li>
    <li>по этому "рандомному" порту и IP адресу узла кластера можно получить доступ к БД.</li>
    <li>назначить БД конкретный порт нельзя. При пересоздании Pod через stateless Deployment, порт может быть уже другим.</li>
    <li>для того, чтобы Service был смаплен с созданным через Deployment Pod БД, должны соответствовать Service.spec.selector.tier: app-db и Deployment.spec.template.metadata.labels.tier: app-db</li>
   </ul><br>
   <pre><code class="plaintext">apiVersion: v1
kind: Service
metadata:
    name: app-db
    labels:
        tier: app-db
    namespace: go-app
spec:
    type: NodePort   # A port is opened on each node in your cluster via Kube proxy.
    ports:
        - port: 5432
          targetPort: 5432
    selector:
        tier: app-db</code></pre><br>
   <h3 id="liquibase">Liquibase</h3><br>
   <p>Liquibase со скриптами должен запускаться только один раз, сразу после старта БД:</p><br>
   <ul>
    <li>нужно задержать запуск, пока БД не поднимется</li>
    <li>второй раз Liquibase не должен запускаться</li>
    <li>нужно различать запуски для первичной инсталляции и установки изменений DDL и DML на существующую БД.</li>
   </ul><br>
   <p>В простом случае, без использования Helm, запуск реализован в виде однократно запускаемого Pod — <a href="https://github.com/romapres2010/goapp/blob/master/deploy/kubernates/base/app-liquibase-pod.yaml" rel="nofollow noopener noreferrer">/deploy/kubernates/base/app-liquibase-pod.yaml</a></p><br>
   <ul>
    <li>ENV переменные заполняются из ранее созданных ConfigMap и Secret</li>
    <li>template.metadata.labels задана дополнительная метка tier: app-liquibase, чтобы можно было легко найти Pod Liquibase</li>
    <li>задан отдельный initContainers с image: busybox:1.28, задача которой — бесконечный цикл (лучше его ограничить, иначе придется руками удалять Pod) — ожидания готовности порта 5432 postgres в Pod c БД.<br>
     <ul>
      <li>для прослушивания порта используется команда nc -w</li>
      <li>здесь нам пригодился созданный ранее Service для БД.</li>
      <li>В рамках Kubernetes кластера в качестве краткого DNS имени используется именно Service.metadata.name, которое мы определили как 'app-db'.</li>
      <li>Это же значения мы записали в ConfigMap.data.APP_PG_HOST</li>
      <li>ConfigMap.data.APP_PG_HOST смаплен на ENV переменную initContainers.env.APP_PG_HOST</li>
      <li>ENV переменную initContainers.env.APP_PG_HOST используем в команде nc -w</li>
     </ul></li>
    <li>в command определена собственно строка запуска Liquibase с передачей через ENV переменную initContainers.env.APP_PG_CHANGELOG корневого скрипта для запуска '--changelog-file=./changelog/$(APP_PG_CHANGELOG)'</li>
    <li>чтобы исключить повторный запуск Liquibase Pod указано spec.restartPolicy: Never</li>
    <li>tag для Docker образа будет определен в kustomize через подстановку image: app-liquibase</li>
   </ul><br>
   <pre><code class="plaintext">apiVersion: v1
kind: Pod
metadata:
    name: app-liquibase
    labels:
        tier: app-liquibase
    namespace: go-app
spec:
    initContainers:
        - name: init-app-liquibase
          env:
            - name: APP_PG_HOST
              valueFrom:
                configMapKeyRef:
                  name: app-config
                  key: APP_PG_HOST
            - name: APP_PG_PORT
              valueFrom:
                configMapKeyRef:
                  name: app-config
                  key: APP_PG_PORT
          image: busybox:1.28
          command: ['sh', '-c', "until nc -w 2 $(APP_PG_HOST) $(APP_PG_PORT); do echo Waiting for $(APP_PG_HOST):$(APP_PG_PORT) to be ready; sleep 5; done"]
    containers:
        - name: app-liquibase
          env:
            - name: APP_PG_USER
              valueFrom:
                secretKeyRef:
                  name: app-secret
                  key: APP_PG_USER
            - name: APP_PG_PASS
              valueFrom:
                secretKeyRef:
                  name: app-secret
                  key: APP_PG_PASS
            - name: APP_PG_HOST
              valueFrom:
                configMapKeyRef:
                  name: app-config
                  key: APP_PG_HOST
            - name: APP_PG_PORT
              valueFrom:
                configMapKeyRef:
                  name: app-config
                  key: APP_PG_PORT
            - name: APP_PG_DBNAME
              valueFrom:
                configMapKeyRef:
                  name: app-config
                  key: APP_PG_DBNAME
            - name: APP_PG_CHANGELOG
              valueFrom:
                configMapKeyRef:
                  name: app-config
                  key: APP_PG_CHANGELOG
          image: app-liquibase # image: romapres2010/app-liquibase:2.0.0
          command: ['sh', '-c', "docker-entrypoint.sh --changelog-file=./changelog/$(APP_PG_CHANGELOG) --url=jdbc:postgresql://$(APP_PG_HOST):$(APP_PG_PORT)/$(APP_PG_DBNAME) --username=$(APP_PG_USER) --password=$(APP_PG_PASS) --logLevel=info update"]
          imagePullPolicy: IfNotPresent
    restartPolicy: Never</code></pre><br>
   <h3 id="go-api">Go Api</h3><br>
   <p>Особенности развертывания <a href="https://github.com/romapres2010/goapp/blob/master/deploy/kubernates/base/app-api-deployment.yaml" rel="nofollow noopener noreferrer">/deploy/kubernates/base/app-api-deployment.yaml</a>:</p><br>
   <ul>
    <li>начальное количество replicas: 1, остальные будет автоматически создаваться Horizontal Autoscaler</li>
    <li>template.metadata.labels задана дополнительная метка tier: app-api, чтобы можно было легко найти Pod Go App</li>
    <li>ENV переменные заполняются из ранее созданных ConfigMap и Secret</li>
    <li>по аналогии с Liquibase задан отдельный initContainers для ожидания готовности порта 5432 postgres в Pod c БД<br>
     <ul>
      <li>так как все Pod (БД, Liquibase, Go Api) будут запущены одновременно, то возникнет ситуация, когда БД уже стартовала, но Liquibase еще не применил DDL и DML скрипты. Или применил их только частично.</li>
      <li>в это время контейнер с Go Api (в текущем шаблоне) будет падать с ошибкой и пересоздаваться.</li>
      <li>поэтому нужно правильно настроить readinessProbe для Go Api, которая определяет с какого момента приложение готово к работе</li>
      <li>альтернативный вариант иметь в БД метку или сигнал, об успешной установке патчей на БД по которому Go Api будет готова к работе</li>
     </ul></li>
    <li>определены readinessProbe и livenessProbe на основе httpGet</li>
    <li>определена период мягкой остановки — terminationGracePeriodSeconds: 45</li>
    <li>заданы resources.requests — это определяет минимальные ресурсы, для старта приложения. В зависимости от этого будет выбран узел кластера со свободными ресурсами.</li>
    <li>заданы resources.limits<br>
     <ul>
      <li>если превышен limits.memory, то Pod будет удален и пересоздан</li>
      <li>limits.cpu контролируется собственно кластером — больше процессорного времени не будет выделено.</li>
     </ul></li>
    <li>tag для Docker образа будет определен в kustomize через подстановку image: app-api</li>
   </ul><br>
   <pre><code class="plaintext">apiVersion: apps/v1
kind: Deployment
metadata:
    name: app-api
    labels:
        tier: app-api
    namespace: go-app
spec:
    replicas: 1
    selector:
        matchLabels:
            tier: app-api
    strategy:
        type: Recreate
    template:
        metadata:
            labels:
                app_net: "true"
                tier: app-api
        spec:
            initContainers:
                - name: init-app-api
                  env:
                    - name: APP_PG_HOST
                      valueFrom:
                        configMapKeyRef:
                          name: app-config
                          key: APP_PG_HOST
                    - name: APP_PG_PORT
                      valueFrom:
                        configMapKeyRef:
                          name: app-config
                          key: APP_PG_PORT
                  image: busybox:1.28
                  command: ['sh', '-c', "until nc -w 2 $(APP_PG_HOST) $(APP_PG_PORT); do echo Waiting for $(APP_PG_HOST):$(APP_PG_PORT) to be ready; sleep 5; done"]
            terminationGracePeriodSeconds: 45
            containers:
                - name: app-api
                  env:
                    - name: APP_CONFIG_FILE
                      valueFrom:
                        configMapKeyRef:
                          name: app-config
                          key: APP_CONFIG_FILE
                    - name: APP_HTTP_LISTEN_SPEC
                      valueFrom:
                        configMapKeyRef:
                          name: app-config
                          key: APP_HTTP_LISTEN_SPEC
                    - name: APP_LOG_LEVEL
                      valueFrom:
                        configMapKeyRef:
                          name: app-config
                          key: APP_LOG_LEVEL
                    - name: APP_LOG_FILE
                      valueFrom:
                        configMapKeyRef:
                          name: app-config
                          key: APP_LOG_FILE
                    - name: APP_PG_USER
                      valueFrom:
                        secretKeyRef:
                          name: app-secret
                          key: APP_PG_USER
                    - name: APP_PG_PASS
                      valueFrom:
                        secretKeyRef:
                          name: app-secret
                          key: APP_PG_PASS
                    - name: APP_PG_HOST
                      valueFrom:
                        configMapKeyRef:
                          name: app-config
                          key: APP_PG_HOST
                    - name: APP_PG_PORT
                      valueFrom:
                        configMapKeyRef:
                          name: app-config
                          key: APP_PG_PORT
                    - name: APP_PG_DBNAME
                      valueFrom:
                        configMapKeyRef:
                          name: app-config
                          key: APP_PG_DBNAME
                  image: app-api # image: romapres2010/app-api:2.0.0
                  imagePullPolicy: IfNotPresent
                  readinessProbe:
                    httpGet:
                      path: /app/system/health
                      port: 8080
                      scheme: HTTP
                    initialDelaySeconds: 30  # Time to start
                    failureThreshold: 5
                    periodSeconds: 10
                    timeoutSeconds: 5
                  livenessProbe:
                    httpGet:
                      path: /app/system/health
                      port: 8080
                      scheme: HTTP
                    failureThreshold: 5
                    periodSeconds: 10
                    timeoutSeconds: 5
                  ports:
                    - containerPort: 8080
                  resources:
                    requests:
                      cpu: 500m
                      memory: 256Mi
                    limits:
                      cpu: 2000m
                      memory: 2000Mi
            hostname: app-api-host
            restartPolicy: Always</code></pre><br>
   <p>Доступ к Go Api нужен вне кластера, причем так как Pod может быть несколько, то нужен LoadBalancer — <a href="https://github.com/romapres2010/goapp/blob/master/deploy/kubernates/base/app-api-service.yaml" rel="nofollow noopener noreferrer">/deploy/kubernates/base/app-api-service.yaml</a>.</p><br>
   <ul>
    <li>в нашем случае для кластера настраивается внешний порт 3000 на IP адрес собственно кластера</li>
    <li>для того, чтобы Service был смаплен с созданным через Deployment Pod Go APi (при масштабировании нагрузки их будет несколько), должны соответствовать Service.spec.selector.tier: app-api и Deployment.spec.template.metadata.labels.tier: app-api</li>
   </ul><br>
   <pre><code class="plaintext">apiVersion: v1
kind: Service
metadata:
    name: app-api
    labels:
        tier: app-api
    namespace: go-app
spec:
    type: LoadBalancer
    ports:
        - port: 3000
          targetPort: 8080
    selector:
        tier: app-api</code></pre><br>
   <h2 id="9-kustomization-yaml-dlya-kubernetes">9. Kustomization YAML для Kubernetes</h2><br>
   <p>Kubernetes не предоставляет стандартной возможности использовать внешние ENV переменные — каждый раз нужно менять YAML и заново "накатывать конфигурацию".</p><br>
   <p>Самый простой автоматически вносить изменения в YAML файлы в зависимости от сред развертывания DEV-TEST-PROD — это <a href="https://kustomize.io/" rel="nofollow noopener noreferrer">Kustomize</a>.</p><br>
   <ul>
    <li>создается "базовая версия" (не содержит специфику сред) YAML для Kubernetes — она выложена в каталоге <a href="https://github.com/romapres2010/goapp/tree/master/deploy/kubernates/base" rel="nofollow noopener noreferrer">/deploy/kubernates/base</a></li>
    <li>для каждой из сред (вариантов развертывания) создает отдельный каталог, содержащий изменения в YAML, которые должны быть применены поверх "базовой версии". Например, <a href="https://github.com/romapres2010/goapp/tree/master/deploy/kubernates/overlays/dev" rel="nofollow noopener noreferrer">/deploy/kubernates/overlays/dev</a>.</li>
   </ul><br>
   <p>Состав YAML "базовой версии" определен в <a href="https://github.com/romapres2010/goapp/blob/master/deploy/kubernates/base/kustomization.yaml" rel="nofollow noopener noreferrer">/deploy/kubernates/base/kustomization.yaml</a>.</p><br>
   <pre><code class="plaintext">commonLabels:
  app: app
  variant: base
resources:
- app-namespase.yaml
- app-net-networkpolicy.yaml
- app-configmap.yaml
- app-secret.yaml
- app-db-persistentvolumeclaim.yaml
- app-db-deployment.yaml
- app-db-service.yaml
- app-api-deployment.yaml
- app-api-service.yaml
- app-liquibase-pod.yaml</code></pre><br>
   <p>Состав изменений, которые нужно применить для среды DEV к "базовой версии" определен в <a href="https://github.com/romapres2010/goapp/blob/master/deploy/kubernates/overlays/dev/kustomization.yaml" rel="nofollow noopener noreferrer">/deploy/kubernates/overlays/dev/kustomization.yaml</a>.</p><br>
   <ul>
    <li>в имена всех артефактов Kubernetes добавлен дополнительный префикс namePrefix: dev-</li>
    <li>определена новая метка, для фильтрации всех артефактов среды — commonLabels.variant: dev</li>
    <li>определены подстановки реальных Docker образов для app-api и app-liquibase</li>
    <li>определены патчи, которые нужно применить поверх "базовой версии"</li>
   </ul><br>
   <pre><code class="plaintext">namePrefix: dev-
commonLabels:
  variant: dev
commonAnnotations:
  note: This is development
resources:
- ../../base
images:
- name: app-api
  newName: romapres2010/app-api
  newTag: 2.0.0
- name: app-liquibase
  newName: romapres2010/app-liquibase
  newTag: 2.0.0
patches:
- app-configmap.yaml
- app-secret.yaml
- app-api-deployment.yaml</code></pre><br>
   <h3 id="patchi-poverh-bazovoy-versii">Патчи поверх базовой версии</h3><br>
   <p>Включают изменения, специфичные для сред: IP, порты, имена схем БД, пароли, ресурсы, лимиты.</p><br>
   <p>Изменение Secret</p><br>
   <pre><code class="plaintext">apiVersion: v1
kind: Secret
metadata:
  name: app-secret
  labels:
    app: app
  namespace: go-app
type: Opaque
stringData:
  APP_PG_USER: postgres
  APP_PG_PASS: postgres</code></pre><br>
   <p>Изменение ConfigMap</p><br>
   <pre><code class="plaintext">apiVersion: v1
kind: ConfigMap
metadata:
  name: app-config
  labels:
    app: app
  namespace: go-app
data:
  APP_CONFIG_FILE: /app/defcfg/app.global.yaml
  APP_HTTP_PORT: "8080"
  APP_HTTP_LISTEN_SPEC: 0.0.0.0:8080
  APP_LOG_LEVEL: ERROR
  APP_LOG_FILE: /app/log/app.log
  APP_PG_HOST: dev-app-db
  APP_PG_PORT: "5432"
  APP_PG_DBNAME: postgres
  APP_PG_CHANGELOG: db.changelog-1.0_recreate_testdatamdg.xml   </code></pre><br>
   <p>Изменение Deployment Go Api — заданы другие лимиты, количество реплик и время "чистой" остановки</p><br>
   <pre><code class="plaintext">apiVersion: apps/v1
kind: Deployment
metadata:
    name: app-api
    labels:
        tier: app-api
    namespace: go-app
spec:
    replicas: 2
    template:
        spec:
            terminationGracePeriodSeconds: 60
            containers:
                - name: app-api
                  resources:
                      limits:
                          cpu: 4000m
                          memory: 4000Mi
            restartPolicy: Always</code></pre><br>
   <h2 id="10-testirovanie-kubernetes-s-kustomize">10. Тестирование Kubernetes с kustomize</h2><br>
   <p>Для тестирования подготовлено несколько скриптов в каталоге <a href="https://github.com/romapres2010/goapp/tree/master/deploy/kubernates" rel="nofollow noopener noreferrer">/deploy/kubernates</a>.</p><br>
   <p>Для тестирования под Windows достаточно поставить Docker Desktop и включить в нем опцию Enable Kubernetes.</p><br>
   <p><em>скрипты представлены только для ознакомительных целей</em></p><br>
   <h3 id="sborka-docker-obrazov">Сборка Docker образов</h3><br>
   <p>В файле <a href="https://github.com/romapres2010/goapp/blob/master/deploy/default_repository" rel="nofollow noopener noreferrer">/deploy/default_repository</a> нужно подменить константу на свой Docker репозиторий. Без этого тоже будет работать, но не получится сделать docker push.</p><br>
   <p>Собрать Docker образы:</p><br>
   <ul>
    <li>Go App <a href="https://github.com/romapres2010/goapp/blob/master/deploy/docker/app-api-build.sh" rel="nofollow noopener noreferrer">/deploy/docker/app-api-build.sh</a></li>
    <li>Liquibase <a href="https://github.com/romapres2010/goapp/blob/master/deploy/docker/app-liquibase-build.sh" rel="nofollow noopener noreferrer">/deploy/docker/app-liquibase-build.sh</a></li>
   </ul><br>
   <p>Выполнять docker push не обязательно, так как после сборки Docker образы кешируются на локальной машине.</p><br>
   <h3 id="razvertyvanie-artefaktov-kubernetes">Развертывание артефактов Kubernetes</h3><br>
   <p>Запускаем скрипт <a href="https://github.com/romapres2010/goapp/blob/master/deploy/kubernates/kube-build.sh" rel="nofollow noopener noreferrer">/deploy/kubernates/kube-build.sh</a> и передаем ему в качестве параметров среду dev, которую нужно развернуть.</p><br>
   <pre><code class="plaintext">./kube-build.sh dev</code></pre><br>
   <p>Все результаты логируются в каталог <a href="https://github.com/romapres2010/goapp/tree/master/deploy/kubernates/log" rel="nofollow noopener noreferrer">/deploy/kubernates/log</a></p><br>
   <ul>
    <li>первым шагом выполняется kubectl kustomize и формируется итоговый YAML для среды DEV <a href="https://github.com/romapres2010/goapp/blob/master/deploy/kubernates/log/build-dev-kustomize.yaml" rel="nofollow noopener noreferrer">/kubernates/log/build-dev-kustomize.yaml</a> — можно посмотреть как применилась dev конфигурация</li>
    <li>вторым шагом выполняется kubectl apply — запускается одновременное создание всех ресурсов.</li>
    <li>стартуют сразу все Pod, но dev-app-api и dev-app-liquibase уходит в ожидание через initContainers готовности порта 5432 postgres в Pod c БД</li>
    <li>вставлена задержка 120 сек. и после этого собираются логи всех контейнеров</li>
   </ul><br>
   <h3 id="kratkiy-status-artefaktov-kubernetes">Краткий статус артефактов Kubernetes</h3><br>
   <p>Если выполнить скрипт <a href="https://github.com/romapres2010/goapp/blob/master/deploy/kubernates/kube-descibe.sh" rel="nofollow noopener noreferrer">/master/deploy/kubernates/kube-descibe.sh</a>, то можно посмотреть краткий статус артефактов Kubernetes.</p><br>
   <ul>
    <li>Pod Liquibase успешно отработал и остановился. Его создавали не через Deployment, поэтому у него "нормальное имя" <em>dev-app-liquibase</em>.</li>
    <li>Запущены и работают два Pod Go Api, они создавались через Deployment, поэтому имена имеют "нормальный" префикс и автоматически сгенерированную часть.</li>
    <li>Запущен и работает Pod БД.</li>
    <li>Сервис dev-app-api имеет тип LoadBalancer<br>
     <ul>
      <li>он доступен вне кластера на 3000 порту — например, можно вызвать <a href="http://127.0.0.1:3000/metrics" rel="nofollow noopener noreferrer">/metrics</a>.</li>
      <li>стандартный LoadBalancer работает по алгоритму <a href="https://ru.wikipedia.org/wiki/Round-robin_(%D0%B0%D0%BB%D0%B3%D0%BE%D1%80%D0%B8%D1%82%D0%BC)" rel="nofollow noopener noreferrer">round robin</a>, поэтому без сохранения сессии запросы будут направляться на разные Pod по очереди.</li>
      <li>если клиент запросит HTTP keep-alive, то запросы будут идти на тот же Pod (это имеет значение при использовании Horizontal Autoscaler)</li>
     </ul></li>
    <li>Сервис dev-app-db имеет тип NodePort и указан назначенный ему порт 30906<br>
     <ul>
      <li>что бы получить досут к БД вне кластера, нужно сначала определить на каком узле (Node) кластера поднят этот Pod и по IP адресу узла и порту 30906 можно получить доступ к БД</li>
     </ul></li>
    <li>Между собой Pod могут коммуницировать через краткое DNS имя service.<br>
     <ul>
      <li>Liquibase и Go Api могут обратиться к БД через host name = service: dev-app-db</li>
     </ul></li>
   </ul><br>
   <pre><code class="plaintext">$ ./kube-get.sh dev go-app

Kube namespace: go-app
Kube variant: dev

kubectl get pods
NAME                           READY   STATUS      RESTARTS   AGE   IP           NODE             NOMINATED NODE   READINESS GATES
dev-app-api-59b6ff97b4-2kmfm   1/1     Running     0          13m   10.1.1.182   docker-desktop   &lt;none&gt;           &lt;none&gt;
dev-app-api-59b6ff97b4-plmz6   1/1     Running     0          13m   10.1.1.183   docker-desktop   &lt;none&gt;           &lt;none&gt;
dev-app-db-58bbb867d8-c96bz    1/1     Running     0          13m   10.1.1.184   docker-desktop   &lt;none&gt;           &lt;none&gt;
dev-app-liquibase              0/1     Completed   0          13m   10.1.1.185   docker-desktop   &lt;none&gt;           &lt;none&gt;

kubectl get deployment
NAME          READY   UP-TO-DATE   AVAILABLE   AGE   CONTAINERS   IMAGES                       SELECTOR
dev-app-api   2/2     2            2           13m   app-api      romapres2010/app-api:2.0.0   app=app,tier=app-api,variant=dev
dev-app-db    1/1     1            1           13m   app-db       postgres:14.5-alpine         app=app,tier=app-db,variant=dev

kubectl get service
NAME          TYPE           CLUSTER-IP       EXTERNAL-IP   PORT(S)          AGE   SELECTOR
dev-app-api   LoadBalancer   10.106.114.183   localhost     3000:30894/TCP   13m   app=app,tier=app-api,variant=dev
dev-app-db    NodePort       10.111.201.17    &lt;none&gt;        5432:30906/TCP   13m   app=app,tier=app-db,variant=dev

kubectl get configmap
NAME             DATA   AGE
dev-app-config   9      13m

kubectl get secret
NAME             TYPE     DATA   AGE
dev-app-secret   Opaque   2      13m

kubectl get pvc
NAME               STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS   AGE   VOLUMEMODE
dev-app-db-claim   Bound    pvc-996244d5-c5fd-4496-abfd-b6d9301549af   100Mi      RWO            hostpath       13m   Filesystem

kubectl get hpa
No resources found in go-app namespace.</code></pre><br>
   <h3 id="razvernutyy-status-artefaktov-kubernetes">Развернутый статус артефактов Kubernetes</h3><br>
   <p>Развернутый статус артефактов Kubernetes можно посмотреть, запустив скрипт <a href="https://github.com/romapres2010/goapp/blob/master/deploy/kubernates/kube-descibe.sh" rel="nofollow noopener noreferrer">/master/deploy/kubernates/kube-descibe.sh</a>.</p><br>
   <pre><code class="plaintext">$ ./kube-descibe.sh dev go-app</code></pre><br>
   <p>Пример результатов в файле <a href="https://github.com/romapres2010/goapp/blob/master/deploy/kubernates/log/describe-dev-kube.log" rel="nofollow noopener noreferrer">/deploy/kubernates/log/describe-dev-kube.log</a>.</p><br>
   <h3 id="proverka-logov-docker-konteynerov">Проверка логов Docker контейнеров</h3><br>
   <p>Логи контейнеров Kubernetes можно посмотреть, запустив скрипт <a href="https://github.com/romapres2010/goapp/blob/master/deploy/kubernates/kube-log.sh" rel="nofollow noopener noreferrer">/deploy/kubernates/kube-log.sh</a>.</p><br>
   <pre><code class="plaintext">$ ./kube-log.sh dev go-app</code></pre><br>
   <p>У нас запущено 2 Pod Go Api — в этом скрипте они будут перемешаны и записаны в один файл.</p><br>
   <h3 id="udalenie-artefaktov-kubernetes">Удаление артефактов Kubernetes</h3><br>
   <p>Удалить все артефакты можно скриптом <a href="https://github.com/romapres2010/goapp/blob/master/deploy/kubernates/kube-delete.sh" rel="nofollow noopener noreferrer">/deploy/kubernates/kube-delete.sh</a>.</p><br>
   <pre><code class="plaintext">$ ./kube-deelte.sh dev go-app</code></pre>
  </div>
 </div>
</div> <!----> <!---->