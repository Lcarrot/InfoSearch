<div>
 <div class="article-formatted-body article-formatted-body article-formatted-body_version-2">
  <div xmlns="http://www.w3.org/1999/xhtml">
   <p>Привет. Я Марат Сибгатулин — сетевик в Яндексе, ведущий подкаста linkmeup, автор серии книг «Сети для самых маленьких» и спикер курса Слёрм <a href="https://slurm.io/networking-for-devops"><u>Сети для DevOps</u></a>, который мы сделали совместно с linkmeup.</p>
   <p>Сегодняшний рассказ будет про несколько органических проблем современных сетевых технологий.</p>
   <p>В жизни любого инженера бывают периоды как долгой кропотливой проработки архитектуры, так и долгих кропотливых расследований инцидентов или проблем. Нет, бывают, конечно, и озарения, стремительные лёгкие открытия, но обычно слова «кропотливый» и «методичный» — неизменные спутники нашей работы. И увы — не всегда этот процесс завершается яркой кульминацией и впрыском дофамина.</p>
   <p>Ведь именно ради этого, и только обладая должным терпением и перфекционизмом, мы идём в профессию.</p>
   <p>То, что мы наделали при проектировании, всегда приводит к проблемам при эксплуатации. Отличаются лишь детали: количество таких проблем, сила их влияния, нам их разгребать или кому-то другому, фундаментальные они и требуют масштабных изменений, или декоративные и компенсируются сравнительно небольшими доработками.</p>
   <p>И сфера в общем-то не важна — сетевые технологии, разработка ПО, строительство домов, мостов или самолётов.</p>
   <p>Есть кирпичи, которые научились изготавливать давно и хорошо, есть инструменты проектирования, упрощающие жизнь, есть шаблоны архитектур, которые берёшь и модифицируешь под свои задачи. Конечный результат получается в среднем хороший, в среднем — как у всех.</p>
   <p>Отрасль ИТ от других отличает то, что в ней, обычно, от качества результата не зависит жизнь человека. Во всяком случае, напрямую. Поэтому плохие шаблоны могут вымываться очень долго, а ошибки переходить из одного проекта в другой раз за разом.</p>
   <p>Иногда разбираться приходится с плодами своих же талантов архитектора, но чаще — с чужими. Свои или знаешь заранее, или быстро привыкаешь и вырабатываешь мышечную память к их употреблению. Чужие — каждый раз как в первый: садишься, долго разбираешься, находишь уникальное решение-снежинку, но и тут со временем начинаешь прослеживать те самые ошибки-гастролёры.</p>
   <p>Я работаю с сетями, я их искренне люблю всем сердцем. Потому что они — фундамент, они основа, на которой всё строится и держится. Они с нами навсегда во всех своих проявлениях — медная витая пара, соединяющая сервер с коммутатором, оптический - трансокеанский кабель, WiFi до ноутбука и 4G до телефона.</p>
   <p>Вокруг будут сменяться эпохи и технологии, мир будет бороться за экологию и экологичность, за равенство первых и первенство вторых, мы будем строить базы на Марсе и добывать минералы в поясе Астероидов, но незыблемыми останутся сети.&nbsp;</p>
   <hr>
   <p>&nbsp;<strong>Сколько нужно сетевых инженеров, чтобы остановить шторм?</strong></p>
   <details class="spoiler">
    <summary>Помните тот анекдот про серийного убийцу?</summary>
    <div class="spoiler__content">
     <p>&nbsp;<em>Плохой программист Джон сделал ошибку в коде, из-за которой каждый пользователь программы был вынужден потратить в среднем 15 минут времени на поиск обхода возникшей проблемы. Пользователей было 10 миллионов. Всего впустую потрачено 150 миллионов минут = 2.5 миллиона часов. Если человек спит 8 часов в сутки, то на сознательную деятельность у него остается 16 часов. То есть Джон уничтожил 156250 человеко-дней ≈ 427.8 человеко-лет. Средний мужчина живет 64 года, значит Джон убил примерно 6 целых 68 сотых человека.</em></p>
    </div>
   </details>
   <p>Так вот есть такая технология, как Ethernet. По задумке изобретателей в IEEE (читается как ай-трипл-и) она должна была принести благо и обеспечить работу нескольких узлов внутри одной локальной сети.</p>
   <p>Под локальной сетью понималась сначала общая металлическая шина на коаксиальном кабеле. Это была разделяемая среда, и в конкретный момент времени в ней мог передавать данные только один узел.Даже всякие слова ругательные придумали, вроде CSMA/CD, которые позволяли коллизии разруливать.</p>
   <p>Все узлы получали все данные.&nbsp;</p>
   <figure class="full-width ">
    <img src="https://habrastorage.org/r/w1560/getpro/habr/upload_files/e7b/ef6/62e/e7bef662e00564cc2ac80acad61b38f8.png" width="1402" height="648" data-src="https://habrastorage.org/getpro/habr/upload_files/e7b/ef6/62e/e7bef662e00564cc2ac80acad61b38f8.png">
    <figcaption></figcaption>
   </figure>
   <figure class="full-width ">
    <img src="https://habrastorage.org/r/w1560/getpro/habr/upload_files/a0c/07c/a36/a0c07ca36d7343aadbd6f033619e9b3f.png" width="779" height="618" data-src="https://habrastorage.org/getpro/habr/upload_files/a0c/07c/a36/a0c07ca36d7343aadbd6f033619e9b3f.png">
    <figcaption></figcaption>
   </figure>
   <p>Повторюсь, что технология предполагалась быть ограниченной только небольшим сегментом на несколько узлов. Скорость передачи данных&nbsp; для отдельного узла экспоненциально уменьшается с добавлением каждого нового узла.</p>
   <p>Чтобы адресовать узлы, в такой сети используются MAC-адреса — концепция второго уровня модели OSI (Layer 2 или L2). Всё, что подключено к одной такой шине, называется единым L2-доменом, поскольку все коммуникации в пределах него ограничены именно технологиями L2.</p>
   <p>В скором одножильную шину заменили на многожильную витую пару, для соединения друг с другом узлов стали ставить хабы, а чтобы в один L2-домен можно было включить побольше узлов, стали сегментировать локальную сеть бриджами. Хабы по сути делали физическую коммутацию между кабелями, а бриджи были поумнее и перекидывали между своими портами не просто электрический сигнал, а уже Ethernet-кадры, зная, какой куда нужно передавать.</p>
   <p>&nbsp;</p>
   <figure class="full-width ">
    <img src="https://habrastorage.org/r/w1560/getpro/habr/upload_files/286/297/3f0/2862973f059ff7690c5e1a065ecc1769.png" alt="Хаб" title="Хаб" width="739" height="324" data-src="https://habrastorage.org/getpro/habr/upload_files/286/297/3f0/2862973f059ff7690c5e1a065ecc1769.png">
    <figcaption>
     Хаб
    </figcaption>
   </figure>
   <figure class="full-width ">
    <img src="https://habrastorage.org/r/w1560/getpro/habr/upload_files/cc4/f60/cfc/cc4f60cfc48e110afcb5afbb2c293af9.png" alt="Мост" title="Мост" width="1274" height="452" data-src="https://habrastorage.org/getpro/habr/upload_files/cc4/f60/cfc/cc4f60cfc48e110afcb5afbb2c293af9.png">
    <figcaption>
     Мост
    </figcaption>
   </figure>
   <figure class="full-width ">
    <img src="https://habrastorage.org/r/w1560/getpro/habr/upload_files/8bd/4ce/04b/8bd4ce04b708de9c4946df4d1c52bc11.png" width="1600" height="1018" data-src="https://habrastorage.org/getpro/habr/upload_files/8bd/4ce/04b/8bd4ce04b708de9c4946df4d1c52bc11.png">
    <figcaption></figcaption>
   </figure>
   <p>Домен коллизий уменьшился, и так стало возможным подключать больше узлов, не сильно жертвуя скоростью.</p>
   <p>Ещё чуть позже всё-таки захотелось ещё больше скорости — чтобы каждый узел мог передавать и получать данные с максимальной мощью, предоставляемой сетевой картой - ведь странно, если у тебя 10мегабитный NIC, использовать из него только сотню килобит из-за болтливых соседей?</p>
   <p>Так появились коммутаторы — очень (по тем временам) умные коробки, которые сами умеют определять, где какой узел находится и выстраивать коммуникацию с ними индивидуально на основе изучения MAC-адресов и поддержания их таблицы внутри чипов. Домен коллизий вообще исчез, потому что на одном проводе оказались только два устройства, у которых ещё и Rx с Tx на разных парах — шли сколько хочешь в обоих направлениях.</p>
   <p>С ними мы шагнули в настоящее, где каждый узел получил возможность подключаться на скорости 1Гб/с</p>
   <p>&nbsp;</p>
   <figure class="full-width ">
    <img src="https://habrastorage.org/r/w1560/getpro/habr/upload_files/ca2/800/ca6/ca2800ca6d61218fccb8621494403a83.png" alt="Домен коллизий" title="Домен коллизий" width="1600" height="465" data-src="https://habrastorage.org/getpro/habr/upload_files/ca2/800/ca6/ca2800ca6d61218fccb8621494403a83.png">
    <figcaption>
     Домен коллизий
    </figcaption>
   </figure>
   <figure class="full-width ">
    <img src="https://habrastorage.org/r/w1560/getpro/habr/upload_files/f12/940/616/f129406161a28db3fb5d758d74a437e3.png" alt="Коммутатор, он же свитч" title="Коммутатор, он же свитч" width="1600" height="1200" data-src="https://habrastorage.org/getpro/habr/upload_files/f12/940/616/f129406161a28db3fb5d758d74a437e3.png">
    <figcaption>
     Коммутатор, он же свитч
    </figcaption>
   </figure>
   <p>И так в результате больших и маленьких последовательных оптимизаций мир получил возможность в один L2-домен засовывать практически неограниченное количество узлов и неизлечимую болезнь, подпортившую много жизней. Именно с коммутаторов началась мировая история широковещательных штормов: сложно даже прикинуть, сколько миллионов часов человечество провело в их отладке.</p>
   <p>Прекрасное и отвратительное в том, что Ethernet превосходно решает порученные ему задачи, имея в самом ядре своём механизм auto-discovery. Узел, отправляющий данные, не знает изначально, сколько и каких соседей с ним есть в сети. Не знает он и то, где расположен его шлюз: он просто шлёт данные. А сеть сама сможет понять, куда их доставить.</p>
   <figure class="full-width ">
    <img src="https://habrastorage.org/getpro/habr/upload_files/893/5c0/77a/8935c077a46c7608e29b008a388da8b7.gif" alt="Широковещательный домен" title="Широковещательный домен" width="1000" height="540" data-src="https://habrastorage.org/getpro/habr/upload_files/893/5c0/77a/8935c077a46c7608e29b008a388da8b7.gif">
    <figcaption>
     Широковещательный домен
    </figcaption>
   </figure>
   <p>Механизм этот называется широковещание или broadcasting. Не знаешь, что делать? Рассылай во всех доступных направлениях — там разберутся.</p>
   <p>Ну а поскольку первые топологии локальных сетей были звёздами и не предполагали наличия петель, никто и не думал о том, что жизнь кадра должна быть конечной и скоротечной.</p>
   <p>Однако если петля есть и кадр в неё попал, он будет бегать по ней без конца и края. А сделать петлю — как два байта отослать: просто вставить кабель одновременно в два порта коммутатора. Я вам не говорил, но это распространённая проказа в домовых сетях.</p>
   <p>Хуже того: такой трафик может мультиплицироваться и за считанные минуты забить любой доступный канал и интерфейс. Характерной чертой штормов является 100% утилизация интерфейсов и истеричная гирлянда светодиодов на портах коммутатора. От этого страдает транзитный пользовательский трафик: для него не остаётся места.</p>
   <div class="tm-iframe_temp" data-src="https://embedd.srv.habr.com/iframe/63ef57bca9cdfde04238d021" data-style="" id="63ef57bca9cdfde04238d021" width=""></div>
   <p>А ещё такие кадры в большинстве своём должны быть обработаны на CPU-коммутатора, вдруг это ему предназначается, поэтому к 100% на интерфейсах добавляется 100% утилизации CPU, а это в свою очередь вырубает Control Plane коробки — она становится неотзывчивой, рвёт установленные сессии, перестают работать протоколы, не отвечает на запросы системы мониторинга или даже SSH.</p>
   <p>Но всё ещё нет проблемы, пока мы Ethernet используем в домашней или офисной сети, где упало что-то: да и бог бы с ним,один пользователь пострадал. Мы можем не строить там сети с петлями административно и запрещать их появление по злому умыслу или незнанию технически. А в сетях операторов связи и датацентрах должны существовать свои другие отказоустойчивые топологии и протоколы.</p>
   <p>И такое использование разных технологий для организации локальных сетей возможно, потому что общим объединяющим протоколом является IP, работающий на третьем уровне модели OSI (L3). А на разных сегментах от отправителя до получателя на канальном и физическом уровне может быть абсолютно что угодно.</p>
   <p>Но почему-то возможность была воспринята как необходимость. И Ethernet начали со страшной силой пихать куда ни попадя. Провайдеры стали делать на нём домовые сети и пихать его в ядро. Сети предприятий распробовав дешевизну оборудования Ethernet и стали строить на нём свои кампусные сети. В датацентрах подешевевшие из-за массовости Ethernet-чипы тоже снискали популярность.</p>
   <p>А всем этим ребятам нужна отказоустойчивость, потому что отказ одного линка или устройства — это сотни и тысячи пострадавших пользователей, недополученных денег и испорченная репутация.</p>
   <p>И как только стали появляется кольцевые L2-топологии, начались и чудовищные штормы так называемого <a href="https://habr.com/ru/post/223037/"><u>BUM-трафика</u></a>. Очень звучное и говорящее название, кстати. Broadcast, Unknown Unicast и Multicast — все эти виды трафика однажды попав в петлю, остаются в ней до тех пор, пока её не разорвать. И все они плоть от плоти L2 — неотъемлемая часть Ethernet, и они присутствуют в сети всегда.</p>
   <p>Как решение этой проблемы появились 50 оттенков STP — протокола, отрезающего лишние линки, чтобы не было петель. Вместе с ними плодились и проприетарные реализации протоколов защиты в кольцевых топологиях а-ля ERPS.</p>
   <p>И вот представьте себе: строите вы сеть, покупаете гору оборудования, кабелей между ними, трансиверов, а потом половину чик — и отрезаете, лишая себя купленной пропускной полосы. Обидно.</p>
   <p>И все эти протоколы рано или поздно дают сбои. Я в своей жизни ещё не встречал отказоустойчивых L2-сетей, в которых бы не было шторма. И это всегда болезненно, это всегда критический инцидент с большим влиянием.</p>
   <p>Я видел сети городских провайдеров, ядро которых построено на L2 и отказоустойчивость обеспечивается STP или его аналогом. Их штормило по несколько раз в месяц — весь город оставался без связи от этого провайдера.</p>
   <p>Я видел сети операторов связи, у которых по городам и весям на сотни километров были раскинуты гроздья радиорелеек, собранные в L2-кольцо с STP. И от штормов в них иные<s> </s>&nbsp;деревни совсем оставались без связи.</p>
   <p>Я видел федеральных операторов, чья физическая сеть построена на L3 и не имеет вышеуказанных проблем. Но поверх неё создавалась так называемая оверлейная или наложенная сервисная сеть, действующая по принципам L2. Да-да, и такое бывает. И в одном L2-домене оказывался целый регион, а то и несколько. Раз в несколько месяцев оно вспыхивало синим пламенем и полыхало по несколько часов, вместе с десятком-другим инженеров, пытающихся понять, что происходит и где рвать петлю.</p>
   <p>Я сам запускал шторм в сети датацентра, когда в сеть Клоза ставил недонастроенное оборудование и пара сотен 100гигабитных линков уходили в полку по утилизации.</p>
   <p>&nbsp;</p>
   <p>И вот этот порочный шаблон повторяется из одного проекта в другой многие годы. Раз за разом. А решение очень простое — оставить L2 там, где ему место — в небольших локальных сетях не больше чем на сотню конечных хостов.</p>
   <p>Да, мы никуда не денемся от Ethernet как от нижележащего протокола: он теперь повсеместный. Мы ничего не можем сделать с механизмом широковещания: уже поздно добавлять поле TTL в Ethernet-заголовок, чтобы старые пакеты погибали — никто не сможет с этим работать.</p>
   <p>Но клянусь, в наших силах сохранить широковещательный домен очень маленьким. И каждый раз, когда к сетевому инженеру приходят с предложением натянуть L2, он должен подозрительно сощурить глаза, взглянув на вопрошающего, и дать тому затрещину. Есть только <a href="https://nag.ru/material/23444"><u>одна причина</u></a>, растягивать L2: вам чересчур спокойно живётся и хорошо спится и руководство разделяет вашу тоску.</p>
   <p>Вообще мы тут рекламируем наш курс про сети, поэтому если хочется погрузиться в описанные вещи поглубже, то вам <a href="https://slurm.io/networking-for-devops">туда прямая дорога</a>. Разбираемся с моделью OSI и её уровнями, смотрим, как работает broadcasting, настраиваем бриджинг и STP, ну и вообще всячески делаем так, чтобы эти грозные слова вас не пугали.</p>
   <hr>
   <h2>Я тебя вычислю по IP</h2>
   <p>&nbsp;Мне кажется, в топ-3 причин нёх на сети входит именно описанная ниже ситуация.</p>
   <p>«Тут ходит — тут не ходит», «то болит — то не болит», «у меня постоянно рвутся коннекты», «у меня через раз устанавливаются коннекты», «поретраю — и работает», «трафик пошёл не туда, куда я сказал», «постоянно разваливается OSPF».</p>
   <p>Эти и другие увлекательные истории можно услышать, когда вы (или вам) устроили конфликт IP-адресов, то есть в двух местах настроили один и тот же IP-адрес. Это может быть интерфейс конечного узла, может быть интерфейс маршрутизатора или любой другой сетевой сущности.</p>
   <p>&nbsp;А дело всё в том, что пакетик такой бежит-бежит по сети. Радостно оглядываясь на таблицу маршрутизации, спешит он к своему получателю. И вот он его уже видит, запрыгивает на сетевой стек машинки с адресом назначения, а оттуда netlink в него из револьвера — бах! Вас тут не ждали! А настоящий получатель так и не получил свой пакет.</p>
   <ul>
    <li><p>Так могут не устанавливаться новые TCP-сессии: потому что на порту никто не слушает;</p></li>
    <li><p>Так могут подвисать и рваться старые, если поток трафика перенаправился в другое место, и там, на порту, никто не слушает;</p></li>
    <li><p>Так могут пинги бежать то к одному получателю, то к другому. И с виду всё хорошо, но почему-то задержки в 7 раз различаются и в tcpdump половину запросов не видно.</p></li>
    <li><p>Так может OSPF не сходиться или постоянно перестраиваться из-за совпавших router-id.</p></li>
   </ul>
   <p>В общем, если видимые симптомы вы можете охарактеризовать словом «дичь» (но не потому что вы вообще в сетях не шарите), то первым под подозрение поставьте задвоившиеся IP-адреса — поверьте моему опыта (я в сетях немного шарю).</p>
   <p>Отлаживать это муторно, но не очень сложно: просто шаг за шагом смотрите на таблицы маршрутизации, местонахождение адреса назначения, его MAC-адреса, за каким портом он изучен. И рано или поздно докопаетесь.</p>
   <p>Один из хостов вы, наверняка, знаете, и всё-таки обнаружите проблему. Можно просто выключить его или перенастроить адрес — тогда у вас выровняется ping и по traceroute вы сможете найти путь к хосту-поганцу с таким же IP (а потом найти и инженера-поганца, допустившего это).</p>
   <p>Обычно такие проблемы случаются, когда выделение и назначение адресов происходит вручную. Просто человеческая ошибка — бывает. Поэтому для IPv4 придумали DHCP. Вам всего лишь нужно в каждом L2-домене поднять один сервер, настроить на нём пулы, шлюз, адрес DNS-сервера и следить за тем, что такой сервер действительно один. Ну, возможно, покопаться с настройками DHCP-Relay и всяких разных DHCP Options. Не беда.</p>
   <p>Главное — что централизованное управление IP-адресами — это благо: оно позволяет снизить вероятность IP-конфликта.</p>
   <p>Это всё касалось IPv4. Не в малой степени он склонен к конфликтам из-за ограниченного адресного пространства (всего 32 бита) и отсутствия вообще каких-либо проверок на такие коллизии.</p>
   <p>&nbsp;</p>
   <figure class="full-width ">
    <img src="https://habrastorage.org/r/w1560/getpro/habr/upload_files/0d1/f4b/187/0d1f4b18745e60d40f167d5ec16a6d2a.png" alt="Работа DHCP-сервера" title="Работа DHCP-сервера" width="690" height="587" data-src="https://habrastorage.org/getpro/habr/upload_files/0d1/f4b/187/0d1f4b18745e60d40f167d5ec16a6d2a.png">
    <figcaption>
     Работа DHCP-сервера
    </figcaption>
   </figure>
   <p>В IPv6 проведена масштабная работа в этом направлении (как и любом другом, на самом деле — они там в IETF вообще большие молодцы, ну если не считать, что из-за них когда-то пришлось придумывать NAT).</p>
   <p>Адрес IPv6 — 128-битный. Поэтому, во-первых, выделить одинаковые адреса чуть посложнее, во-вторых, ручное управление адресами вообще проблематично. И как ответ на «во-вторых» для IPv6 придумали автоконфигурацию IP-адреса на основе префикса сети и MAC-адреса хоста. Акцент на вторую часть — MAC-адрес хоста. Именно он делает в 99,9999% случаев IPv6-адрес уникальным.</p>
   <p>А автоконфигурация избавляет инженера от необходимости поддерживать DHCP-инфраструктуру. Оно просто работает и нагенерит вам с высочайшей долей вероятности неконфликтный, воспитанный IP-адрес.</p>
   <p>А кроме того у IPv6 из коробки есть DAD — Duplicate Address Detection. После назначения адреса на интерфейс он не использует его сразу, а сначала убеждается, что в локальной сети он один такой неповторимый и неотразимый. У меня в практике были десятки случаев конфликтов IPv4 адресов и ни одного IPv6.&nbsp;</p>
   <p>IPv6 большее благо, чем IPv4 DHCP. Любите IPv6, переходите на IPv6, рассказывайте всем про IPv6. А если не понимаете за что любить и почему от 4 сразу перешли к 6 — мы вас <a href="https://slurm.io/networking-for-devops">научим</a>. Как настроить их на Linux, как поднять DHCP-сервер, что за автоконфигурация в IPv6 и чем вообще IPv6 отличается от IPv4 (спойлер — не только числом битов). И вместе с вами подиагностируем разные проблемы связности.</p>
   <hr>
   <h2>Кортеж пятерых</h2>
   <p>&nbsp;А вот следующая штука не факт, что входит в какой-то топ среднестатистического сетевого инженера. И каждый день такое ловить не приходится, и коли уж поймали, то непонятно совершенно, что с этим делать.</p>
   <p>Жили вы обычной жизнью сетевого инженера, BGP-шку там настраивали, новые офисы компании запускали, строили трансокеанский канал между офисами для HFT, чтобы сэкономить 5 миллисекунд по сравнению с конкурентом. И вдруг телеграм начал разрываться, а ваш дежурный пинг в консоли начал показывать 7% потерь.</p>
   <p>Допустим, по плану эвакуации уже побегали, пользователям скорейшее выздоровление пообещали, что делаем дальше?</p>
   <p>Тут, конечно, наберётся материала на серию статей с абсолютно произвольной глубиной погружения.</p>
   <p>На самом деле дальнейшие действия зависят от характера потерь и симптомов: достаём из арсенала ping'и такие и сякие, traceroute/mtr, telnet/nc/curl, в конце концов tcpdump, и начинаем анализировать.</p>
   <p>Только ли проблема с ICMP? Есть ли какой-то специфический шаблон в потерях? Большие пакеты проходят? TCP и UDP одинаково себя ведут? Можно ли сказать, что только у конкретных TCP-сессий есть проблема, в то время как у других всё чётко и стабильно? Показывает ли mtr накопительные проблемы по пути? А если mtr -T? Увеличиваются ли&nbsp; задержки? А проблемы наблюдаются до одного конкретного адреса? Или можно локализовать хотя бы направление? Или вообще куда угодно?</p>
   <p>А если рубануть одного провайдера? А другого? А попробовать попинговать из другого места? А из-за пределов собственной сети вообще? А в обратную сторону — в сторону нашей сети извне — как дела? А маршруты нашей сети нормально вообще по миру распространяются?</p>
   <p>В зависимости от наисследованного дальше причины можно классифицировать следующим образом:</p>
   <ul>
    <li><p>Где-то плохое качество линии, а пакеты портятся/теряются на лету — скорее всего, бессистемные потери без увеличения задержек.</p></li>
   </ul>
   <ul>
    <li><p>Где-то перегрузка на линии или в очередях маршрутизатора — скорее всего, бессистемные потери, сопровождающиеся отрастанием задержек.</p></li>
    <li><p>Один из нескольких равнозначных путей испытывает проблемы выше — скорее всего, бессистемные потери с симптомом «то болит, то не болит». Но на трассировке по TCP или UDP будет видно, что только по конкретному пути проблемы.</p></li>
    <li><p>Что-то стряслось с MTU: раньше большие пакеты проходили нормально, а теперь нет — скорее всего, ping маленькими пакетами (1400 байтов) будет ходить нормально, а большими (1500) будет таймаутить. Скорее всего, можно попробовать в TCP-порт тыркнуться через telnet или nc, а вот curl'ануть уже может не получиться — слишком много данных.</p></li>
    <li><p>Ну и моё любимое — что-то где-то случилось так неловко, что конкретные TCP-flow (или UDP-датаграммы) не доходят до получателя. При этом другие доходят, а ICMP вообще проблем не фиксирует.</p></li>
   </ul>
   <p><em>Запомни, инженер! Если ты не видишь потерь в ping %server_address%, это ещё не значит, что у тебя всё в порядке!</em></p>
   <p>Знаете, первые четыре причины имеют под собой очень увлекательную фундаментальную базу и написано про это много книг и снято видосов (даже моими силами), и там, правда интересно, но позвольте мне проявить слабость и рассказать как раз о последнем.</p>
   <p>Это действительно нечастая ситуация. В пинге вы можете увидеть её как более или менее регулярные потери с характерным шаблоном или нерегулярные бессистемные потери, но примерно одного и того же % запросов, а можете не увидеть совсем.</p>
   <p>Если вы будете делать попытки установить TCP-сессии с помощью telnet или nc, то, на первый взгляд, тоже не будет никакой системы — иногда получается, иногда нет (на что пользователи, собственно, вам и жалуются).</p>
   <p>Кстати, в цикле пробовать ставить сессии можно так</p>
   <pre><code class="bash">while :; do nc -zvw 1 linkmeup.ru 443;&nbsp;done
Connection to linkmeup.ru 443 port [tcp/https] succeeded!
Connection to linkmeup.ru 443 port [tcp/https] succeeded!
Connection to linkmeup.ru 443 port [tcp/https] succeeded!</code></pre>
   <p>Немного косметики:</p>
   <pre><code class="bash">while :; do
	nc -zw 1 linkmeup.ru 80 &amp;&gt; /dev/null &amp;&amp; echo -n '!' || echo -n '.';
	sleep 0.1;
done</code></pre>
   <p>Картина становится более наглядной.</p>
   <p>&nbsp;Но стоит зафиксировать src port, как картина начинает проясняться.</p>
   <p>Дело в том, что при установке TCP-сессии у нас зафиксированы четыре параметра: <code>dst IP</code>, <code>src IP</code>, <code>протокол TCP</code>, <code>dst port</code>. И один параметризируемый — <code>src port</code>. Для каждой новой сессии клиенты (браузеры, почтовики, SSH, FTP) выбирают новый эфемерный src port из диапазона <code>1024-65535</code>. Не выбирать совсем нельзя вообще, меньше 1024 нельзя — всё уже занято, а меняется он для того, чтобы в этом так называемом <strong>5-tuple</strong> (<code>src-ip</code>, <code>dst-ip</code>, <code>proto</code>, <code>src-port</code>, <code>dst-port</code>) была энтропия и при наличии нескольких различных путей на основе этого 5-tuple можно считать хэш и раскладывать по разным путям.</p>
   <figure class="full-width ">
    <img src="https://habrastorage.org/r/w1560/getpro/habr/upload_files/fdf/088/2b9/fdf0882b9d0930588a4ee167e2d6c13f.png" alt="Заголовки IP + TCP" title="Заголовки IP + TCP" width="826" height="120" data-src="https://habrastorage.org/getpro/habr/upload_files/fdf/088/2b9/fdf0882b9d0930588a4ee167e2d6c13f.png">
    <figcaption>
     Заголовки IP + TCP
    </figcaption>
   </figure>
   <p>Хэш от одного и того же 5-tuple будет всегда получаться один и тот же, а следовательно, для него всегда будет выбираться один и тот же путь.</p>
   <p>Так вот, если один из этих путей корёжит пакеты, то всё, что в него отправляется, будет теряться, а всё, что в него не попадает, будет работать нормально.</p>
   <figure class="full-width ">
    <img src="https://habrastorage.org/r/w1560/getpro/habr/upload_files/427/850/124/42785012487f4d321583446d2264bf9d.png" alt="ECMP - Equal Cost Multipath" title="ECMP - Equal Cost Multipath" width="1250" height="934" data-src="https://habrastorage.org/getpro/habr/upload_files/427/850/124/42785012487f4d321583446d2264bf9d.png">
    <figcaption>
     ECMP - Equal Cost Multipath
    </figcaption>
   </figure>
   <p>&nbsp;</p>
   <p>Именно поэтому просто установка соединения будет то успешной, то нет, а если мы фиксируем src port, то эта хэш-неопределённость разрешается, и мы получаем стабильную картину — или каждый раз успешен, или каждый раз провален.</p>
   <p>&nbsp;</p>
   <p>Вот как можно зафиксировать порт:</p>
   <pre><code class="bash">nc -zv -p 34437&nbsp; linkmeup.ru 443</code></pre>
   <p>Вот как можно перебирать порты в цикле</p>
   <pre><code class="bash">for i in {10000..50000} ; do
  echo -n "$i:";
  nc -w 1 -p $i -z linkmeup.ru 443 &amp;&gt; /dev/null &amp;&amp; echo " OK" || echo " FAIL" ;
  sleep 0.1;
done</code></pre>
   <p>Теперь mtr вам, скорее всего, поможет понять, на каком хопе проблема (но может и не помочь).</p>
   <p>И осталось разобраться в чём проблема и исправить её - всего-то.&nbsp;</p>
   <pre><code class="bash">&gt; mtr -T linkmeup.ru -P 443

&nbsp;Host&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; Loss% &nbsp; Snt &nbsp; Last &nbsp; Avg&nbsp; Best&nbsp; Wrst StDev

&nbsp;1. (waiting for reply)

&nbsp;2. 100.64.0.101&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 0.0%&nbsp; &nbsp; 15&nbsp; &nbsp; 0.2 &nbsp; 0.2 &nbsp; 0.2 &nbsp; 0.3 &nbsp; 0.0

&nbsp;&nbsp;&nbsp;&nbsp;100.64.0.33

&nbsp;&nbsp;&nbsp;&nbsp;100.64.0.103

&nbsp;&nbsp;&nbsp;&nbsp;100.64.0.34

&nbsp;&nbsp;&nbsp;&nbsp;100.64.0.74

&nbsp;&nbsp;&nbsp;&nbsp;100.64.0.72

&nbsp;&nbsp;&nbsp;&nbsp;100.64.0.75

&nbsp;&nbsp;&nbsp;&nbsp;100.64.0.39

&nbsp;3. (waiting for reply)

&nbsp;4. msk-ix-2.servers.ru &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 0.0%&nbsp; &nbsp; 14&nbsp; &nbsp; 4.1 &nbsp; 5.9 &nbsp; 3.8&nbsp; 18.3 &nbsp; 4.7

&nbsp;5. se-mo01-s1.servers.ru &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 0.0%&nbsp; &nbsp; 14&nbsp; &nbsp; 3.7 &nbsp; 3.9 &nbsp; 3.7 &nbsp; 4.2 &nbsp; 0.2

&nbsp;6. se-mo1-s3.servers.ru&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 0.0%&nbsp; &nbsp; 14&nbsp; &nbsp; 3.9 &nbsp; 3.8 &nbsp; 3.5 &nbsp; 4.0 &nbsp; 0.1

&nbsp;7. se-mo01-l25-r1.servers.ru &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 0.0%&nbsp; &nbsp; 14&nbsp; &nbsp; 4.5 &nbsp; 5.1 &nbsp; 4.1 &nbsp; 7.5 &nbsp; 0.9

&nbsp;8. proxy03.linkmeup.ru &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 0.0%&nbsp; &nbsp; 14&nbsp; &nbsp; 3.4 &nbsp; 3.4 &nbsp; 3.1 &nbsp; 3.5 &nbsp; 0.1</code></pre>
   <p>А тут… ну тоже серия статей с произвольной глубиной погружения.</p>
   <p>Однако это почти всегда история про баги ПО — где-то что-то недопрограммировалось в чипы или структуры памяти, записалось не то, что было нужно или уже записанное было повреждено.</p>
   <p>Основные причины:</p>
   <ul>
    <li><p>Баг операционной системы сетевого устройства — на уровне работы с протоколами и формирования верхнеуровневых структур. Грубо говоря, программа неправильно посчитала таблицу маршрутизации и испортила один 5-tuple.</p></li>
    <li><p>Баг SDK чипа. Чип коммутации предоставляет API для управления его внутренней памятью и правилами обработки пакетов. SDK для доступа к API — та же программа. И она тоже <a href="https://www.youtube.com/watch?v=2L8-jdV8oG8"><u>содержит баги</u></a><u>.</u></p></li>
   </ul>
   <p>И вы в него можете передать всё правильно, а он запрограммирует в чип с ошибкой. И пакеты с конкретными хэшами будут теряться.</p>
   <ul>
    <li><p>Небольшое расширение вышеприведённых сценариев — залипания на фаерволах, когда в iptables спустились неверные правила.</p></li>
    <li><p>Такие же сюрпризы может подкинуть и ECMP — Equal Cost Multipath, когда один из путей сломан так, что он продолжает выглядеть рабочим (например, анонсирует маршруты, через себя), но с точки зрения передачи транзитных данных поломался. Тогда всё, что раскладывается в него согласно 5-tuple будет теряться. Стоит вывести из под нагрузки этот путь — и всё начинает работать.</p></li>
   </ul>
   <figure class="float bordered full-width ">
    <img src="https://habrastorage.org/r/w1560/getpro/habr/upload_files/943/39d/929/94339d9296cb0c8b1d415c85dca04d24.png" width="1116" height="694" data-src="https://habrastorage.org/getpro/habr/upload_files/943/39d/929/94339d9296cb0c8b1d415c85dca04d24.png">
    <figcaption></figcaption>
   </figure>
   <ul>
    <li><p>И очень изощрённый случай — аппаратные повреждения.</p></li>
   </ul>
   <p>Был в моей практике такой случай. Приходит заказчик в техподдержку (ко мне) и говорит, что у него теряется часть трафика — лечится ретраями: ну прям как я выше описал симптомы.</p>
   <p>Дальнейшая диагностика как раз позволила понять, что проблемы с прохождением определённых 5-tuple.</p>
   <p>По трассировке мы увидели, что потери наблюдаются за пределами сети заказчика — в другом провайдере. А он по счастливой случайности тоже оказался нашим заказчиком, поэтому мы уже с ним продолжили диагностику. На сети этого провайдера стоял так называемый холодильник — шассийная коробка размером со шкаф, в которую напиханы дюжина плат и под полсотни разных сложных чипов. И вот мы посмотрели таблицу коммутации на чипе входной карты, посмотрели в какие пины задней шины шасси раскладываются пакеты, проверили живость этих путей и обнаружили, что через часть из них не проходят данные — мы не можем отловить их на выходном чипе.</p>
   <p>​​</p>
   <figure class="full-width ">
    <img src="https://habrastorage.org/r/w1560/getpro/habr/upload_files/d6d/af9/99e/d6daf999e3352eaade3fbfc02c03a25e.png" alt="https://s3.linkmeup.ru/linkmeup/sdsm/14/ijlfq8wvq-mq4aiwww5-rla6rsy.png" title="https://s3.linkmeup.ru/linkmeup/sdsm/14/ijlfq8wvq-mq4aiwww5-rla6rsy.png" width="1600" height="647" data-src="https://habrastorage.org/getpro/habr/upload_files/d6d/af9/99e/d6daf999e3352eaade3fbfc02c03a25e.png">
    <figcaption>
     https://s3.linkmeup.ru/linkmeup/sdsm/14/ijlfq8wvq-mq4aiwww5-rla6rsy.png
    </figcaption>
   </figure>
   <p>Попросили инженера клиента извлечь интерфейсную плату и сфотографировать её. И что бы вы думали? На задней общей шине оказались загнуты контакты — и всё, что по балансировке внутри этого холодильника раскладывалось на эти пины, терялось.</p>
   <p>Как только увели трафик с этого пути,всё заработало.</p>
   <figure class="full-width ">
    <img src="https://habrastorage.org/r/w1560/getpro/habr/upload_files/474/0e4/b67/4740e4b679c6eea26a40b29e193ddc99.png" alt="Замятые контакты на плате" title="Замятые контакты на плате" width="1598" height="1600" data-src="https://habrastorage.org/getpro/habr/upload_files/474/0e4/b67/4740e4b679c6eea26a40b29e193ddc99.png">
    <figcaption>
     Замятые контакты на плате
    </figcaption>
   </figure>
   <p>&nbsp;Последняя проблема тоже органическая часть современных протоколов, не говоря уж о потерях пакетов как таковых. И даже новые протоколы, такие как QUIC, и всеобщее применение QoS (какая небылица) решить их фундаментально не позволят. Есть разной степени изящности решения, позволяющие обходить проблемные места, но всё это уже предмет архитектурных изысканий мощных сетевых инженеров.</p>
   <p>Однако если вам хочется разобраться с маршрутизацией, работой протоколов TCP и UDP, использованием ping, traceroute, mtr, tcpdump, понять как выбираются пути и какие протоколы в этом помогают — <a href="https://slurm.io/networking-for-devops">мы тоже сможем помочь</a>.</p>
   <p>Мир сетевых технологий удивителен — в нём прогрессивные вещи, вроде SDN и прямой программируемостью структур в чипах коммутации сочетаются с чудовищным legacy времён рассвета информационных технологий. И если вы когда-либо страдали от необходимости поддержки обратной совместимости, просто подойдите к своему сетевому инженеру и попросите его рассказать, как трафик с вашего ноутбука добирается до серверов Яндекса или Amazon. У вас точно будет повод порадоваться тому, как хорошо вам живётся с вашими фреймворками и удивиться, как вообще что-то сегодня работает, учитывая снежный ком технологий для организации связи.</p>
   <hr>
   <p>Любите IPv6!</p>
   <p></p>
  </div>
 </div>
</div> <!----> <!---->