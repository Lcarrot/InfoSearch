<div>
 <div class="article-formatted-body article-formatted-body article-formatted-body_version-2">
  <div xmlns="http://www.w3.org/1999/xhtml">
   <h3>Что такое Марк?</h3>
   <p>Сейчас Марк от медиа Маркер — это генеративная языковая модель, которая умеет придумывать новостные статьи без помощи человека. Марк может писать свободно, опираясь на свой опыт и выбирая каждое слово самостоятельно. Либо же мы можем помочь ему с темой новости и дать начало заголовка или заголовок, тогда Марк продолжит нашу мысль.</p>
   <figure class="full-width ">
    <img src="https://habrastorage.org/getpro/habr/upload_files/e69/73a/1a7/e6973a1a7e6975fd98fbea49fe38887a.PNG" width="1080" height="586">
    <figcaption></figcaption>
   </figure>
   <h4>Что такое языковые модели?</h4>
   <p>Говоря просто, языковые модели — это когда мы пытаемся найти закономерности и правильные речевые последовательности в человеческом языке, а потом на основе этого пробуем создать “язык” независимо от самого человека.</p>
   <p><strong>Как это можно сделать?</strong></p>
   <p>Можно оценить вероятность появления какого-то словосочетания в языке. Иными словами, каждый раз мы отвечаем себе на вопрос: корректно ли использовать такую последовательность слов (формулировку) в русском языке? Если это допустимо, то вероятность употребления такого словосочетания достаточно велика, а значит мы используем языковую конструкцию, которую могут понять остальные, знающие русский.</p>
   <p>В противном случае появляются конструкции вроде “лето наступила”, по правилам нашего языка составленные некорректно, а значит, вероятность появления такой конструкции в речи довольно мала.</p>
   <p><strong>Главная идея:</strong> правильные языковые конструкции могут быть созданы не только человеком, но и искусственным интеллектом (или роботом). Соответственно между хорошей языковой моделью (которая хорошо понимает правила нашего языка) и человеком можно наладить продуктивную взаимную коммуникацию. И вот это уже идея на миллион!</p>
   <h4>Ты сам можешь генерировать новости!</h4>
   <p>Начнём с простого: сейчас мы покажем тебе, как генерировать новости. <strong>Целиком код для генерации лежит на</strong> <a href="https://colab.research.google.com/drive/1j0_uwCIDFSpCHChhEbL5U-PdXd3H5vGM?usp=sharing" rel="noopener noreferrer nofollow">COLAB</a> — можно запустить всё и посмотреть, какую новость Марк сгенерирует для тебя!</p>
   <p>Нашу обученную модель мы выложили на <a href="https://huggingface.co/-" rel="noopener noreferrer nofollow">Hugging Face</a> —&nbsp;технологический стартап с активным open source сообществом, который способствовал внедрению моделей на основе трансформеров (к примеру, предшественников и самой ChatGPT) во всем мире.</p>
   <p>Благодаря стартапу к большому количеству тяжелых (и умных) моделей можно подключиться просто по API и самому творить магию, не имея огромных вычислительных машин.</p>
   <p>Например, вот так мы получим доступ к нашей модели:</p>
   <pre><code class="python">from transformers import GPT2LMHeadModel, GPT2Tokenizer

path = 'AnyaSchen/news_gpt-3'
tokenizer = GPT2Tokenizer.from_pretrained(path)
model = GPT2LMHeadModel.from_pretrained(path).to(DEVICE)</code></pre>
   <p>Возможно, нужно будет установить библиотеку transformers. Например, вот так:</p>
   <pre><code class="python">!pip install transformers</code></pre>
   <p>Ну, а новости ты можешь сгенерировать вот этим кодом:</p>
   <pre><code class="python">inp = input('Введи начало заголовка или просто нажми Enter:')
inp = inp if len(inp) &gt; 0 else tokenizer.bos_token #токен начала предложения

input_ids = tokenizer.encode(inp, return_tensors="pt").to(DEVICE)

out = model.generate(input_ids,
                    do_sample = True,
                    num_beams = 3,
                    temperature = 2.0,
                    top_p = 0.9,
                    max_length = 200,
                    stopping_criteria = StoppingCriteriaList([stop_criteria]),
                    eos_token_id = tokenizer.eos_token_id,
                    bos_token_id = tokenizer.bos_token_id).to(DEVICE)

print(tokenizer.batch_decode(out, skip_special_tokens=False)[0])</code></pre>
   <p>Что тут происходит? Сейчас расскажем.</p>
   <p><strong>Шаг 1:</strong> Мы задаём начало заголовка нашей новости в переменную <code>input</code>. Если вдруг нам нечего дать в начало, и мы нажимаем Enter, то подставляется специальный символ, обозначающий для нашей модели начало предложения (мол, генерируй давай с самого начала).</p>
   <pre><code class="python">inp = input('Введи начало заголовка или просто нажми Enter:')
inp = inp if inp else tokenizer.bos_token #токен начала предложения</code></pre>
   <p><strong>Шаг 2:</strong> Дальше мы берём наш входной текст и говорим (дословно): токенайзер, тебе нужно заэнкодить наше начало.</p>
   <pre><code class="python">input_ids = tokenizer.encode(inp, return_tensors="pt").to(DEVICE)</code></pre>
   <p>По факту мы берем наш кусочек заголовка, пилим его на слова (это делает токенайзер) и преобразуем каждое слово в большие наборы чисел (это и делает энкодер), к которым можно получить доступ по уникальным, назначенным им номерам.</p>
   <p><strong>Зачем так?</strong></p>
   <p>Цель: Мы пытаемся научить компьютер “понимать” наш язык.</p>
   <p>Компьютер хранит наши буковки числами, например, в виде <a href="https://www.ibm.com/docs/ru/sdse/6.4.0?topic=configuration-ascii-characters-from-33-126" rel="noopener noreferrer nofollow">ASCII-кода</a>, где каждому символу присваивается уникальный номер (1, 2, 3…), а потом оно конвертируется в нолики и единички. Так вот <strong>слова</strong> в такой системе просто <strong>набор чисел</strong>, например:</p>
   <p>котик → 1000011101010000111110100010000101000011100010000111010 (<a href="https://planetcalc.ru/6604/" rel="noopener noreferrer nofollow">попробуй сам</a>)</p>
   <p>Но в нашем языке у каждого слова есть смысловой окрас и значение: для нас грусть и радость имеют противоположные смыслы, а в целом — это чувства. Компьютер должен ощущать эту разницу между семантикой слов, чтобы хорошо “говорить” на нашем языке, но для него это всё равно набор ноликов и единичек, не имеющих смысла. Например, вот тебе послание:</p>
   <p><code>10000011010100010000001000011000010001000001100001100001000011001010001000111100001110001000011101010110010000010001000010100010010111000001000100000110001000011100001111111000011010110001000000100001</code></p>
   <p>Понятно, что тут ничего не понятно: ни сколько тут слов, ни что они собой значат. Ты можешь прочитать эти 1 и 0, но в чем смысл, это просто цифры, да?) Ещё одно послание (как у тебя дела с китайским?) — <strong>你會成功的!</strong> Но ты не поймешь, пока кто-то не переведёт <em>это</em> на понятный тебе язык.</p>
   <blockquote>
    <p>Энкодер делает именно это: переводит с человеческого на понятный компьютеру.</p>
   </blockquote>
   <p>Проблема: как объяснить компьютеру, что это за слова и что они значат?</p>
   <p>Умные ученые нашли решение:</p>
   <ul>
    <li><p>разбиваем предложения на кусочки, из которых могут быть составлены родственные слова (именно это и делает <strong>токенайзер</strong> в нашей модели)</p></li>
    <li><p>превращаем каждый кусочек в вектор из чисел, который будет отвечать за семантику.</p>
     <ul>
      <li><p>Почему кусочки, а не слова? Потому что кот и котик — это про одно и тоже, но настроение меняется)</p></li>
     </ul><p>Можно сравнить с настройкой робота из фильма Интерстеллар: 70% честности, 80% позитивности и т.д. Мы задаем какие-то параметры, по которым можно “почувствовать” каждое слово, но не нам, а компьютеру.</p><p>В реальности, конечно, все сильно сложнее…🥲 Но идея понятна, да?</p></li>
    <li><p>Каждому кусочку присваивается уникальный номер, и энкодер нам возвращает именно его. <strong>Почему так?</strong> Чтобы общаться, мы используем слова в определенной последовательности и не сообщаем собеседнику все трактовки каждого сказанного нами слова, надеясь, что наше понимание слов идентично. Так и с компьютером: наборы уникальных номеров — это его слова.</p><p>Например, слово “нейросеть”, переведенное на язык нашей модели, выглядит так:</p><p><code>[[ 845, 648, 1380 ]]</code></p></li>
   </ul>
   <p><strong>Шаг 3: Генерация на понятном компьютеру языке</strong></p>
   <pre><code class="python">out = model.generate(input_ids,
                     do_sample = True,
                     num_beams = 3,
                     temperature = 2.0,
                     top_p = 0.9,
                     max_length = 200,
                     stopping_criteria = StoppingCriteriaList([stop_criteria]),
                     eos_token_id = tokenizer.eos_token_id,
                     bos_token_id = tokenizer.bos_token_id).to(DEVICE)</code></pre>
   <p>Мы научили языковую модель генерировать новости (как? Потом расскажем), но создаёт она их, подбирая семантически каждый кусочек по своим выученным закономерностям и правилам. Чтобы новости получались разнообразнее и умнее, чем тексты у T9, мы использовали самые разные параметры генерации: температура, количество звеньев и т.д. Если будет интересно — расскажем, как они влияют на процесс (можешь поменять значения у себя в коде — увидишь, как всё ломается).</p>
   <p>Из интересного тут — <strong>критерий остановки (код ниже тоже тебе нужен).</strong></p>
   <pre><code class="python">class KeywordsStoppingCriteria(StoppingCriteria):
    def __init__(self, keywords_ids:list):
      self.keywords = keywords_ids
    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor, **kwargs) -&gt; bool:
      if input_ids[0][-1] in self.keywords:
        print(input_ids)
        return True
      return False

stop_criteria = KeywordsStoppingCriteria(tokenizer.encode(tokenizer.eos_token, return_tensors="pt").to(DEVICE))</code></pre>
   <p><strong>Что на выходе генерации?</strong> Чтобы модель не выдавала бесконечные потоки слов, мы ограничили длину предложения в 200 слов (смотри параметры генерации). Но это не всё, ей нужно помочь с окончанием процесса в момент, когда она сгенерировала свою логическую мысль и напечатала специальный токен конца предложения <code>&lt;eos&gt;</code>. Для этого нужен код выше: остановить генерацию, когда увидели ключевое слово (токен) для остановки.</p>
   <p>После выполнения этого шага генерации в переменной <code>out</code> будут лежать числа — уникальные значения кусочков, которые наша языковая модель нагенерировал (aka новости, только на языке, понятном компьютеру).</p>
   <p><strong>Шаг 4: Перевести это обратно на человеческий</strong></p>
   <pre><code class="python">print(tokenizer.batch_decode(out, skip_special_tokens=False)[0])</code></pre>
   <p>Мы перевели человеческий в машинный, машина нагененерировала нам новости на своем языке, а теперь что? Правильно, машинный на человеческий, пожалуйста, чтобы нам понять. → Это и делает декодер.</p>
   <p>И вот нам выводится новость, которая поделена на специальные токены: начала, отделения заголовка от описания и конца предложения. Чтобы эти токены нормально сработали, нужно вначале их показать токенайзеру (смотри, у нас и такие специальные “слова” есть).</p>
   <pre><code class="python">SPECIAL_TOKENS = {'bos_token':'&lt;bos&gt;','eos_token' :'&lt;eos&gt;', 'pad_token':'&lt;pad&gt;', 'sep_token': '&lt;sep&gt;'}
tokenizer.add_special_tokens(SPECIAL_TOKENS)</code></pre>
   <p><strong>Напоминаем, что целиком код для генерации можно найти тут:</strong> <a href="https://colab.research.google.com/drive/1j0_uwCIDFSpCHChhEbL5U-PdXd3H5vGM?usp=sharing" rel="noopener noreferrer nofollow">COLAB</a></p>
   <p>Что же ты сегодня узнал:</p>
   <ul>
    <li><p>Что такое языковая модель</p></li>
    <li><p>Как происходит генерация кода в языковых моделях</p></li>
    <li><p>Энкодер переводит с человеческого на машинный, а декодер — наоборот</p></li>
    <li><p>Токенайзер пилит слова на кусочки, а модель — обрабатывает семантику этих кусочков</p></li>
   </ul>
   <p>Автор статьи: <a class="mention" href="/users/anyaschenikova">@anyaschenikova</a></p>
   <p></p>
  </div>
 </div>
</div> <!----> <!---->